{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LEGO Bricks Identification Project: A Journey of Learning and Adaptation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction and Motivation 🎯\n",
    "\n",
    "Este proyecto surge como una oportunidad para aplicar y demostrar mis habilidades técnicas en el contexto de un cambio profesional hacia la ciencia de datos e inteligencia artificial. Durante esta transición, decidí combinar mi pasión por los LEGO con mi interés por la visión por computadora, dando vida a una idea que conecta creatividad y tecnología.\n",
    "\n",
    "### 1.1 La Inspiración del Proyecto 🧱\n",
    "\n",
    "LEGO ha sido una de mis pasiones desde la infancia. Me fascinaba cómo piezas aparentemente desordenadas podían transformarse en estructuras organizadas y funcionales. Este proyecto busca replicar esa habilidad humana, enseñando a una máquina a identificar, clasificar y organizar piezas individuales en imágenes desordenadas. Más allá de ser un simple experimento técnico, esta iniciativa refleja mi interés por explorar cómo la inteligencia artificial puede emular procesos humanos de reconocimiento visual.\n",
    "\n",
    "### 1.2 Propósito y Objetivos 🎯\n",
    "\n",
    "El propósito principal del proyecto es desarrollar una solución práctica y escalable que demuestre mis habilidades en ciencia de datos y visión por computadora. Los objetivos específicos incluyen:\n",
    "\n",
    "1. Diseñar un pipeline eficiente para la detección y clasificación de piezas de LEGO.\n",
    "2. Documentar el proceso de desarrollo, desde la creación del dataset hasta la implementación del modelo.\n",
    "3. Generar resultados replicables y visualizaciones claras que destaquen el impacto de mi enfoque técnico.\n",
    "\n",
    "Este proyecto también busca ser una base para aplicaciones futuras, como sistemas de clasificación robótica o asistentes visuales en tiempo real, mostrando la capacidad de escalar esta solución inicial hacia un impacto más amplio.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creación del Dataset 🗂️\n",
    "\n",
    "El éxito de este proyecto dependió en gran medida de un dataset bien estructurado y diverso que capturara la naturaleza de las piezas de LEGO. El proceso de creación del dataset fue dividido en dos fases principales: una inicial centrada en los ladrillos básicos y otra posterior dedicada a los studs.\n",
    "\n",
    "### 2.1 Recopilación de Datos 📸\n",
    "\n",
    "Para la primera fase, seleccioné ladrillos básicos de mi colección, excluyendo tiles y piezas transparentes, y los clasifiqué por dimensiones. En un espacio bien iluminado, dispuse los ladrillos con suficiente separación para que fueran fácilmente distinguibles. Usando la cámara de mi móvil configurada en modo de baja calidad y captura automática, generé más de 2000 imágenes mientras caminaba lentamente por la habitación, enfocándome en ángulos típicos desde los que un usuario de LEGO observaría las piezas.\n",
    "\n",
    "En la segunda fase, monté un set fotográfico casero utilizando una plataforma giratoria hecha con piezas de LEGO. Coloqué nuevos conjuntos de ladrillos sobre esta plataforma y capturé imágenes desde un trípode con el móvil, utilizando captura automatizada nuevamente.\n",
    "\n",
    "### 2.2 Anotación de los Datos 📝\n",
    "\n",
    "En la primera fase, utilicé LabelMe para anotar las piezas con bounding boxes en las más de 2000 imágenes, eliminando aquellas que estaban borrosas. Posteriormente, desarrollé scripts para convertir estas anotaciones al formato compatible con YOLO y organizar las carpetas de acuerdo con los requerimientos del modelo.\n",
    "\n",
    "En la segunda fase, utilicé un modelo entrenado inicialmente para detectar ladrillos y generé anotaciones de puntos (point annotations) dentro de las bounding boxes recortadas. Un script adicional transformó estos puntos en nuevas bounding boxes dinámicas, adaptándose a las dimensiones y posiciones de los studs detectados.\n",
    "\n",
    "### 2.3 Limpieza y Preparación del Dataset 🧹\n",
    "\n",
    "Las imágenes borrosas o redundantes fueron eliminadas. Para mitigar la falta de calidad en algunas capturas iniciales, implementé técnicas de aumento de datos (data augmentation) directamente en el alimentador del modelo durante las sesiones de entrenamiento. Esto incluyó rotaciones, variaciones de iluminación y espejado para incrementar la diversidad de las muestras.\n",
    "\n",
    "### 2.4 Insights para Iteraciones Futuras 🔍\n",
    "\n",
    "- **Automatización en la anotación inicial**: Considera herramientas como Supervisely o plataformas que integren anotación semiautomática para acelerar procesos iniciales de bounding boxes.\n",
    "- **Mejorar la calidad inicial**: Aunque la baja calidad fue útil para la primera fase, para futuros proyectos se recomienda una resolución estándar para maximizar el detalle en las imágenes.\n",
    "- **Balance de clases**: Implementar análisis de distribución más temprano en el proceso puede garantizar un balance adecuado en el dataset y evitar clases subrepresentadas.\n",
    "- **Data augmentation externo**: Herramientas como Albumentations o Fastai podrían ofrecer técnicas más avanzadas de aumento, como distorsiones geométricas y filtros, fuera del alimentador del modelo.\n",
    "- **Pruebas preliminares del modelo**: Entrenar modelos pequeños desde el inicio puede revelar rápidamente problemas en el dataset, como errores de anotación o distribuciones desbalanceadas, y ahorrar tiempo en iteraciones posteriores.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Selección del Modelo 🤖\n",
    "\n",
    "### 3.1 Elección del Modelo 🏆\n",
    "\n",
    "El desafío central del proyecto era diseñar un modelo que pudiera identificar piezas individuales de LEGO en imágenes desordenadas y con condiciones variables de iluminación y fondo. Para abordar este problema, era crucial elegir un modelo que equilibrara precisión, rapidez y facilidad de implementación. Tras investigar varias opciones, seleccioné YOLO (You Only Look Once), específicamente la versión YOLOv8n, por las siguientes razones:\n",
    "\n",
    "- **Simplicidad y Configuración Inicial**: YOLO ofrecía una configuración amigable para mi hardware con capacidades de GPU limitadas. Su implementación directa me permitió centrarme más en la preparación del dataset y en los ajustes iterativos.\n",
    "- **Velocidad en Detección en Tiempo Real**: YOLO es conocido por su capacidad de realizar detecciones en una sola pasada de red neuronal, optimizando tanto el tiempo como el uso de recursos computacionales. Esto resultó ideal para entrenar y validar rápidamente el modelo con un dataset en constante refinamiento.\n",
    "- **Robustez en Ambientes Complejos**: Su arquitectura permite manejar imágenes con múltiples objetos, incluso en escenarios donde las piezas están parcialmente ocluidas, lo que alineaba perfectamente con las condiciones del proyecto.\n",
    "\n",
    "### 3.2 Estrategia de Entrenamiento y Adaptación 🛠️\n",
    "\n",
    "El proceso de entrenamiento incluyó los siguientes pasos:\n",
    "\n",
    "- **Creación y Preparación del Dataset**: Se definió un pipeline para convertir anotaciones en formato LabelMe a un formato compatible con YOLO. Esto incluyó la limpieza de datos, aumento (augmentation) durante el entrenamiento y un split 80-20 para entrenamiento y validación.\n",
    "- **Hiperparámetros**: Ajusté los siguientes parámetros clave para optimizar el rendimiento:\n",
    "    - Tamaño de imágenes: Utilicé imágenes redimensionadas para maximizar la compatibilidad con el modelo.\n",
    "    - Épocas: Comencé con 50 épocas para pruebas rápidas, aumentando gradualmente según los resultados.\n",
    "    - Tasa de aprendizaje inicial: 0.001, con un decaimiento cíclico (cos_lr).\n",
    "- **Uso de Scripts Personalizados**: Implementé scripts para:\n",
    "    - Recortar piezas detectadas en imágenes, simplificando tareas posteriores de clasificación.\n",
    "    - Transformar anotaciones de puntos clave en bounding boxes adaptativas según los studs detectados y dimensiones de entrada.\n",
    "- **Iteraciones y Validación**: Cada iteración del modelo fue validada visualizando predicciones sobre nuevas imágenes del dataset, ajustando hiperparámetros según las métricas de precisión y recall.\n",
    "\n",
    "### 3.3 Ejemplos y Visualizaciones 📊\n",
    "\n",
    "Para esta sección se incluirán:\n",
    "\n",
    "- Imágenes con Bounding Boxes de ladrillos detectados: Visualizaciones de ejemplos representativos de detecciones exitosas, resaltando piezas con múltiples colores y tamaños.\n",
    "- Comparación antes y después del modelo: Mostrar ejemplos de imágenes con y sin anotaciones generadas automáticamente por el modelo.\n",
    "- Visualización de transformación de keypoints a bounding boxes: Ejemplos ilustrativos de cómo las anotaciones de studs se convirtieron en cajas 2D.\n",
    "\n",
    "## 4. Próximos Pasos 🚀\n",
    "\n",
    "El desarrollo iterativo del modelo ha proporcionado un camino claro para expandir el alcance del proyecto. A medida que se continúe mejorando, estas iteraciones ayudarán a explorar nuevas áreas de aplicación y optimización en el reconocimiento visual de LEGO.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Implementación del Modelo y Validación 🤖\n",
    "\n",
    "### 4.1 Entrenamiento Inicial y Ajustes 🛠️\n",
    "\n",
    "Una vez seleccionado el modelo YOLOv8n, el entrenamiento se realizó iterativamente para ajustar el modelo a las condiciones específicas del dataset:\n",
    "\n",
    "- **Inicio del Entrenamiento**: Se utilizó el dataset procesado y anotado para realizar pruebas iniciales con un modelo preentrenado. Estas pruebas se centraron en establecer una base para los parámetros clave, como el tamaño de las imágenes y el número de épocas.\n",
    "- **Evaluación Continua**: Durante el entrenamiento, se monitorearon métricas como la precisión y el recall para identificar áreas de mejora. Las visualizaciones de las predicciones ayudaron a identificar sesgos o problemas en el dataset.\n",
    "\n",
    "### 4.2 Validación en Escenarios Controlados 🧪\n",
    "\n",
    "Para evaluar el rendimiento del modelo:\n",
    "\n",
    "- **Pruebas en Dataset Anotado**: Se utilizaron imágenes del conjunto de validación para comparar la detección y clasificación con las etiquetas originales. Esto permitió identificar las clases o condiciones donde el modelo tenía mayor dificultad.\n",
    "- **Pruebas con Transformaciones**: Se aplicaron aumentos artificiales (rotaciones, cambios de iluminación, etc.) para evaluar la robustez del modelo ante variaciones comunes en la captura de imágenes.\n",
    "\n",
    "### 4.3 Resultados en Imágenes Reales 📸\n",
    "\n",
    "El modelo fue probado en escenarios más diversos:\n",
    "\n",
    "- **Colección No Anotada**: Se seleccionaron imágenes reales del entorno, no incluidas en el entrenamiento, para validar la capacidad del modelo en condiciones no vistas.\n",
    "- **Evaluación de Robustez**: Se analizaron las predicciones en piezas o configuraciones no consideradas en el diseño inicial del dataset, evaluando su capacidad de generalización.\n",
    "\n",
    "### 4.4 Visualizaciones y Métricas 📊\n",
    "\n",
    "Para esta sección se incluirán:\n",
    "\n",
    "- **Predicciones con Bounding Boxes**: Imágenes que muestran cómo el modelo detecta las piezas, con especial atención a las detecciones erróneas o falsas positivas.\n",
    "- **Estadísticas Generales**: Tablas y gráficos que presentan las métricas clave: precisión, recall, F1-score, y tiempo promedio por predicción.\n",
    "- **Análisis Comparativo**: Comparación de ejemplos con predicciones y etiquetas originales, mostrando casos representativos de éxito y áreas de mejora.\n",
    "- **Visualización Tipo Spread Grid**: Una visualización tipo grid de miniaturas, mostrando imágenes anotadas aleatorias de todo el dataset.\n",
    "\n",
    "## 5. Próximos Pasos 🚀\n",
    "\n",
    "El desarrollo iterativo del modelo ha proporcionado un camino claro para expandir el alcance del proyecto. A medida que se continúe mejorando, estas iteraciones ayudarán a explorar nuevas áreas de aplicación y optimización en el reconocimiento visual de LEGO.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Reflexión y Futuro 🔍\n",
    "\n",
    "### 5.1 Lecciones Aprendidas 📚\n",
    "\n",
    "Este proyecto no solo amplió mis conocimientos técnicos en visión por computadora y machine learning, sino que también reforzó habilidades esenciales como:\n",
    "\n",
    "- **Gestión de Proyectos Técnicos**: Desde la conceptualización hasta la ejecución, aprendí a planificar y priorizar tareas clave para lograr un resultado completo.\n",
    "- **Iteración y Resolución de Problemas**: Los desafíos encontrados, como la selección inicial de MediaPipe y ajustes en el dataset, me enseñaron a ser flexible y adaptarme rápidamente.\n",
    "- **Documentación y Visualización**: Documentar el proceso de manera clara y crear visualizaciones efectivas fue crucial para comunicar el impacto del trabajo.\n",
    "\n",
    "### 5.2 Mejoras Futuras 🔧\n",
    "\n",
    "Si pudiera comenzar de nuevo, consideraría:\n",
    "\n",
    "- **Ampliar el Dataset Inicial**: Incluir más clases y piezas más variadas desde el principio habría mejorado la capacidad del modelo para generalizar.\n",
    "- **Explorar Alternativas a YOLO**: Experimentar con otros modelos para comparar rendimiento y adaptabilidad.\n",
    "- **Automatizar Anotaciones**: Implementar técnicas más avanzadas para reducir el tiempo dedicado a tareas manuales.\n",
    "\n",
    "### 5.3 Próximos Pasos 🚀\n",
    "\n",
    "- **Optimización del Modelo**: Afinar los hiperparámetros y explorar técnicas de fine-tuning más avanzadas.\n",
    "- **Ampliación del Dataset**: Incorporar nuevos tipos de piezas, incluidos tiles y elementos transparentes.\n",
    "- **Desarrollo de una Interfaz**: Crear una aplicación interactiva que permita a los usuarios cargar imágenes y obtener predicciones en tiempo real.\n",
    "- **Exploración de Nuevas Aplicaciones**: Investigar cómo este pipeline podría adaptarse a otros dominios, como la clasificación de piezas en manufactura o reciclaje.\n",
    "\n",
    "Con estos pasos, espero seguir construyendo sobre este proyecto y expandiendo su alcance e impacto potencial.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **Introduction**\n",
    "This notebook narrates the development of a project that started as a personal challenge and became a demonstration of creative problem-solving and a deep dive into computer vision. The project explores how computer vision techniques can be used to identify and analyze LEGO bricks. \n",
    "\n",
    "### **Motivation**\n",
    "As a lifelong learner and a problem-solver, I found this project to be a perfect opportunity to apply my curiosity and drive to a practical challenge. It also reflects a significant moment in my life: transitioning from my work in the hospitality industry to pursuing a career in technology, where my skills and passions can have a greater impact.\n",
    "\n",
    "## **Problem Statement**\n",
    "LEGO bricks come in many shapes and sizes, often with subtle differences. The challenge was to create a system capable of identifying individual bricks and determining their dimensions using computer vision techniques. While the initial scope was limited to 26 distinct brick types, the problem presented several complexities:\n",
    "\n",
    "- Variability in brick dimensions and shapes.\n",
    "- The need for accurate annotations to train detection models.\n",
    "- Limited data availability, requiring adaptive solutions.\n",
    "\n",
    "## **Methodology**\n",
    "\n",
    "### **1. Initial Approach: Direct Brick Classification**\n",
    "The project began with the idea of classifying LEGO bricks directly using object detection models like YOLO. However, early experimentation revealed significant challenges:\n",
    "\n",
    "- **Dataset limitations**: The dataset was small, unbalanced, and contained subtle differences between classes.\n",
    "- **Model performance**: Direct classification yielded low accuracy due to the aforementioned challenges.\n",
    "\n",
    "### **2. Adaptation: Focusing on Stud Detection**\n",
    "To overcome these hurdles, the approach shifted towards detecting studs (the small bumps on LEGO bricks) and using their positions to infer brick dimensions. This adjustment allowed for:\n",
    "\n",
    "- Simplified classification logic.\n",
    "- Better utilization of the dataset by focusing on a common feature across all bricks.\n",
    "\n",
    "The process involved:\n",
    "- Annotating images with stud positions using LabelMe.\n",
    "- Training a YOLOv8 model to detect studs.\n",
    "- Developing algorithms to compute brick dimensions from the detected stud positions.\n",
    "\n",
    "### **3. Experimentation and Iteration**\n",
    "The development process was highly iterative, involving:\n",
    "- Multiple rounds of dataset refinement and augmentation.\n",
    "- Experimenting with model hyperparameters to improve detection accuracy.\n",
    "- Validating the dimension-calculation algorithms with test images.\n",
    "\n",
    "## **Results**\n",
    "The project achieved:\n",
    "- **Accurate stud detection**: The trained YOLO model demonstrated high precision and recall in detecting studs.\n",
    "- **Dimension calculation**: Algorithms successfully inferred the dimensions of various bricks using detected stud positions.\n",
    "\n",
    "### **Visualizations**\n",
    "- Examples of stud detection with bounding boxes.\n",
    "- Graphs showing model performance metrics (precision, recall, F1 score).\n",
    "\n",
    "## **Reflection and Learnings**\n",
    "This project exemplified the power of curiosity and adaptability in tackling challenges. Key takeaways include:\n",
    "\n",
    "1. The importance of flexibility: Pivoting to a simpler approach yielded better results and deeper insights.\n",
    "2. Learning through experimentation: Iterative testing refined the solution and improved my understanding of the problem.\n",
    "3. A new direction: The project reaffirmed my passion for learning and solving complex problems, motivating me to transition into a technology-focused career.\n",
    "\n",
    "## **Future Directions**\n",
    "This project is a starting point. Potential next steps include:\n",
    "- Expanding the dataset to include more brick types.\n",
    "- Refining the detection and dimension-calculation algorithms for production-level accuracy.\n",
    "- Scaling the solution for real-world applications, such as automated sorting or inventory management.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **Code and Implementation**\n",
    "### **1. Data Preparation**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "```python\n",
    "# Example code for data preprocessing\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "# Load and preprocess images\n",
    "image_dir = 'data/raw'\n",
    "processed_dir = 'data/processed'\n",
    "os.makedirs(processed_dir, exist_ok=True)\n",
    "\n",
    "for image_file in os.listdir(image_dir):\n",
    "    image_path = os.path.join(image_dir, image_file)\n",
    "    image = cv2.imread(image_path)\n",
    "    # Resize, normalize, and save\n",
    "    processed_image = cv2.resize(image, (640, 640))\n",
    "    cv2.imwrite(os.path.join(processed_dir, image_file), processed_image)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **2. Model Training**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "```python\n",
    "# Example code for training the YOLO model\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the YOLO model\n",
    "model = YOLO('yolov8n.pt')\n",
    "\n",
    "# Train the model\n",
    "model.train(data='data.yaml', epochs=50, imgsz=640)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **3. Dimension Calculation**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "```python\n",
    "# Algorithm to calculate brick dimensions from stud positions\n",
    "def calculate_dimensions(stud_positions):\n",
    "    rows = len(set([pos[1] for pos in stud_positions]))  # Unique Y-coordinates\n",
    "    cols = len(set([pos[0] for pos in stud_positions]))  # Unique X-coordinates\n",
    "    return rows, cols\n",
    "\n",
    "# Example usage\n",
    "stud_positions = [(10, 20), (10, 40), (30, 20), (30, 40)]\n",
    "dimensions = calculate_dimensions(stud_positions)\n",
    "print(f\"Brick dimensions: {dimensions[0]} rows x {dimensions[1]} columns\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## **Closing Remarks**\n",
    "This notebook not only showcases a technical solution but also tells the story of how a love for learning and persistence can turn a simple idea into a meaningful project. By sharing this journey, I aim to inspire others and demonstrate my readiness for new challenges in the field of technology.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
