{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"..\")  # Ajusta según tu estructura\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate_folders_ImageDataProject.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project structure created successfully at: c:\\Users\\User\\Projects_Unprotected\\LEGO_Bricks_ML_Vision\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def create_project_structure():\n",
    "    \"\"\"\n",
    "    Create a folder structure for an image data project.\n",
    "\n",
    "    The base path is automatically set to the current working directory.\n",
    "    \"\"\"\n",
    "    # Set the base directory to the current working directory\n",
    "    base_path = os.getcwd()\n",
    "\n",
    "    # Define the folder structure\n",
    "    structure = {\n",
    "        \"data\": [\n",
    "            \"raw\",\n",
    "            \"processed\",\n",
    "            \"annotations\"\n",
    "        ],\n",
    "        \"src\": [\n",
    "            \"data_preprocessing\",\n",
    "            \"models\",\n",
    "            \"visualization\"\n",
    "        ],\n",
    "        \"models\": [\n",
    "            \"checkpoints\",\n",
    "            \"final\"\n",
    "        ],\n",
    "        \"results\": [\n",
    "            \"predictions\",\n",
    "            \"logs\"\n",
    "        ],\n",
    "        \"docs\": [],\n",
    "        \"notebooks\": [],\n",
    "        \"tests\": [],\n",
    "        \"presentation\": []\n",
    "    }\n",
    "\n",
    "    # Function to create folders recursively\n",
    "    def create_folders(base, folders):\n",
    "        for folder, subfolders in folders.items():\n",
    "            folder_path = os.path.join(base, folder)\n",
    "            os.makedirs(folder_path, exist_ok=True)\n",
    "            for subfolder in subfolders:\n",
    "                os.makedirs(os.path.join(folder_path, subfolder), exist_ok=True)\n",
    "\n",
    "    # Create the structure\n",
    "    create_folders(base_path, structure)\n",
    "\n",
    "    print(f\"Project structure created successfully at: {base_path}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    create_project_structure()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exportar Refurbished PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook LEGO_Bricks_Identification_Project_Overview.ipynb to pdf\n",
      "[NbConvertApp] Writing 34205 bytes to notebook.tex\n",
      "[NbConvertApp] Building PDF\n",
      "[NbConvertApp] Running xelatex 3 times: ['xelatex', 'notebook.tex', '-quiet']\n",
      "[NbConvertApp] Running bibtex 1 time: ['bibtex', 'notebook']\n",
      "[NbConvertApp] WARNING | b had problems, most likely because there were no citations\n",
      "[NbConvertApp] PDF successfully created\n",
      "[NbConvertApp] Writing 42940 bytes to LEGO_Bricks_Identification_Project_Overview.pdf\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to pdf LEGO_Bricks_Identification_Project_Overview.ipynb\n",
    "# !jupyter nbconvert --to pdf LEGO_Bricks_Identification_Project_Technical_Report.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refurbishing Pipeline\n",
    "\n",
    "(chatGPT is having bugs remembering converstions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"\n",
    "\n",
    "¡Perfecto! Vamos a estructurar esta entrevista para guiarte a reflexionar sobre cada etapa del proyecto y asegurarnos de que tu storytelling esté alineado con las mejores prácticas. Este enfoque también nos permitirá planear cómo documentar, reproducir y organizar el proyecto de manera profesional.\n",
    "\n",
    "Entrevista Inicial\n",
    "Te haré preguntas relevantes por etapas del proyecto. Responde según tu experiencia, y construiré secciones del documento con base en tus respuestas. También sugeriré scripts o ajustes según lo que compartas.\n",
    "\n",
    "1. Origen del Proyecto\n",
    "¿Qué te motivó a iniciar este proyecto? ¿Fue un desafío técnico que querías resolver, una curiosidad personal, o algo más?\n",
    "¿Cómo conectaste tus habilidades previas con esta idea? Por ejemplo, ¿qué experiencias te influyeron a elegir visión por computadora y no otro campo?\n",
    "2. Creación del Dataset\n",
    "¿Qué tipo de datos recopilaste y cómo? Comparte detalles sobre el proceso, como herramientas utilizadas, número de imágenes, y estrategias para capturar diferentes ángulos y condiciones.\n",
    "¿Cómo manejaste la anotación de los datos? ¿Usaste alguna herramienta específica como LabelMe o desarrollaste algo propio? ¿Hubo desafíos en esta etapa?\n",
    "¿Qué criterios usaste para limpiar y preparar los datos? Por ejemplo, ¿eliminaste imágenes borrosas o hiciste algún balanceo entre clases?\n",
    "3. Selección del Modelo\n",
    "¿Por qué elegiste YOLO? ¿Probaste otros modelos antes? Si es así, ¿cuáles y por qué los descartaste?\n",
    "¿Cómo ajustaste los hiperparámetros durante el entrenamiento? Por ejemplo, tamaño de lotes, número de épocas, etc.\n",
    "¿Qué limitaciones encontraste en el modelo, y cómo las abordaste?\n",
    "4. Implementación y Resultados\n",
    "¿Cómo transformaste los resultados del modelo en algo práctico? Por ejemplo, ¿cómo calculaste las dimensiones de las piezas basándote en la detección de studs?\n",
    "¿Qué métricas utilizaste para medir el éxito? ¿Precision, recall, F1-score, u otras? ¿Cómo documentaste estos resultados?\n",
    "¿Qué visualizaciones creaste para entender mejor los resultados?\n",
    "5. Reflexión y Futuro\n",
    "¿Qué aprendiste en términos técnicos y personales? ¿Qué habilidades desarrollaste o reforzaste?\n",
    "Si comenzaras de nuevo, ¿qué harías diferente?\n",
    "¿Qué pasos futuros te gustaría explorar? Por ejemplo, ¿mejorar el modelo, ampliar el dataset, o implementar nuevas funcionalidades?\n",
    "Próximos Pasos\n",
    "Conforme respondas, construiré borradores de las secciones correspondientes del documento. Si surgen necesidades específicas (como scripts para repetir entrenamientos), las desarrollaremos y definiremos cómo integrarlas al proyecto.\n",
    "\n",
    "Comencemos con tus respuestas a las primeras preguntas de la sección 1 sobre la motivación y el origen del proyecto. 😊\n",
    "\n",
    "# \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementacion de pipeline central\n",
    "\n",
    "## pipeline.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # -*- coding: utf-8 -*-\n",
    "\n",
    "# import os\n",
    "\n",
    "# def generate_structure_file(output_file=\"estructura.txt\"):\n",
    "#     \"\"\"\n",
    "#     Genera un archivo con la estructura de directorios solo si el directorio actual\n",
    "#     contiene una carpeta 'src' o está relacionado con ella.\n",
    "\n",
    "#     Args:\n",
    "#         output_file (str): Nombre del archivo para guardar la estructura.\n",
    "#     \"\"\"\n",
    "#     # Directorio actual\n",
    "#     current_dir = os.getcwd()\n",
    "\n",
    "#     # Verificar si existe la carpeta 'src' en el directorio actual\n",
    "#     if \"src\" in os.listdir(current_dir):\n",
    "#         # Crear la estructura en el directorio actual\n",
    "#         target_dir = current_dir\n",
    "#         print(f\"Directorio 'src' encontrado en el directorio actual: {target_dir}\")\n",
    "#     else:\n",
    "#         # Subir al directorio padre y verificar si 'src' es una carpeta hermana\n",
    "#         parent_dir = os.path.abspath(os.path.join(current_dir, os.pardir))\n",
    "#         if \"src\" in os.listdir(parent_dir):\n",
    "#             # Crear la estructura en el directorio padre\n",
    "#             target_dir = parent_dir\n",
    "#             print(f\"Directorio 'src' encontrado como carpeta hermana. Generando estructura en: {target_dir}\")\n",
    "#         else:\n",
    "#             print(\"No se encontró la carpeta 'src'. No se generará el archivo de estructura.\")\n",
    "#             return\n",
    "\n",
    "#     # Generar la estructura\n",
    "#     with open(output_file, \"w\") as f:\n",
    "#         for root, dirs, files in os.walk(target_dir):\n",
    "#             level = root.replace(target_dir, \"\").count(os.sep)\n",
    "#             indent = \" \" * 4 * level\n",
    "#             f.write(f\"{indent}{os.path.basename(root)}/\\n\")\n",
    "#             subindent = \" \" * 4 * (level + 1)\n",
    "#             for file in files:\n",
    "#                 f.write(f\"{subindent}{file}\\n\")\n",
    "\n",
    "#     print(f\"Estructura generada exitosamente en: {output_file}\")\n",
    "\n",
    "# # Ejecutar la función\n",
    "# if __name__ == \"__main__\":\n",
    "#     generate_structure_file()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Projects_Unprotected\\LEGO_Bricks_ML_Vision\n",
      "c:\\Users\\User\\Projects_Unprotected\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Muestra la ruta actual\n",
    "print(os.getcwd())\n",
    "\n",
    "# Cambia al directorio raíz del proyecto\n",
    "os.chdir(\"..\")  # Cambia al directorio raíz\n",
    "\n",
    "# Confirma el cambio de directorio\n",
    "print(os.getcwd())\n",
    "\n",
    "# Importa funciones desde el pipeline\n",
    "from scripts.pipeline import (\n",
    "    setup_environment,\n",
    "    get_kaggle_credentials,\n",
    "    download_dataset_from_kaggle,\n",
    "    validate_directories,\n",
    "    preprocess_images,\n",
    "    labelme_to_yolo,\n",
    "    train_yolo_pipeline,\n",
    "    test_model_on_real_images,\n",
    "    visualize_results,\n",
    ")\n",
    "\n",
    "# Prueba de las funciones importadas\n",
    "setup_environment()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview.pdf\n",
    "\n",
    "## visualize_presentation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import random\n",
    "# import matplotlib.pyplot as plt\n",
    "# from PIL import Image\n",
    "# from pipeline import test_model_on_real_images\n",
    "\n",
    "# # Crear grids de muestras del dataset\n",
    "# def create_dataset_grid(input_folder, output_folder, grid_size=(3, 3)):\n",
    "#     \"\"\"\n",
    "#     Genera un grid de imágenes de muestra del dataset y las guarda en la carpeta especificada.\n",
    "\n",
    "#     Parameters:\n",
    "#     - input_folder (str): Ruta de la carpeta con las imágenes de entrada.\n",
    "#     - output_folder (str): Ruta donde se guardará el grid generado.\n",
    "#     - grid_size (tuple): Dimensiones del grid (filas, columnas).\n",
    "#     \"\"\"\n",
    "#     os.makedirs(output_folder, exist_ok=True)\n",
    "#     image_files = [f for f in os.listdir(input_folder) if f.endswith(('.jpg', '.png'))]\n",
    "#     random.shuffle(image_files)\n",
    "    \n",
    "#     selected_files = image_files[:grid_size[0] * grid_size[1]]\n",
    "#     fig, axes = plt.subplots(*grid_size, figsize=(grid_size[1] * 4, grid_size[0] * 4))\n",
    "\n",
    "#     for idx, img_file in enumerate(selected_files):\n",
    "#         img_path = os.path.join(input_folder, img_file)\n",
    "#         img = Image.open(img_path)\n",
    "#         row, col = divmod(idx, grid_size[1])\n",
    "#         axes[row, col].imshow(img)\n",
    "#         axes[row, col].axis('off')\n",
    "#         axes[row, col].set_title(f\"{img_file}\", fontsize=10)\n",
    "\n",
    "#     for ax in axes.flat[len(selected_files):]:\n",
    "#         ax.axis('off')\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     grid_path = os.path.join(output_folder, \"dataset_grid.png\")\n",
    "#     plt.savefig(grid_path, bbox_inches='tight')\n",
    "#     plt.close()\n",
    "#     print(f\"Grid de dataset guardado en {grid_path}.\")\n",
    "\n",
    "# # Generar visualizaciones anotadas\n",
    "# def annotate_model_results(model_path, input_folder, output_folder, conf_threshold=0.5):\n",
    "#     \"\"\"\n",
    "#     Genera imágenes con anotaciones del modelo y las guarda en la carpeta especificada.\n",
    "\n",
    "#     Parameters:\n",
    "#     - model_path (str): Ruta al modelo YOLO entrenado.\n",
    "#     - input_folder (str): Carpeta con imágenes de entrada.\n",
    "#     - output_folder (str): Carpeta para guardar las imágenes anotadas.\n",
    "#     - conf_threshold (float): Umbral de confianza para las detecciones.\n",
    "#     \"\"\"\n",
    "#     os.makedirs(output_folder, exist_ok=True)\n",
    "#     test_model_on_real_images(model_path, input_folder, output_folder)\n",
    "#     print(f\"Resultados anotados guardados en {output_folder}.\")\n",
    "\n",
    "# # Crear comparaciones antes/después\n",
    "# def generate_comparison_grid(model_path, input_folder, output_folder, num_samples=5):\n",
    "#     \"\"\"\n",
    "#     Crea un grid comparativo de imágenes antes y después de las predicciones del modelo.\n",
    "\n",
    "#     Parameters:\n",
    "#     - model_path (str): Ruta al modelo YOLO entrenado.\n",
    "#     - input_folder (str): Carpeta con imágenes de entrada.\n",
    "#     - output_folder (str): Carpeta para guardar los grids generados.\n",
    "#     - num_samples (int): Número de imágenes a comparar.\n",
    "#     \"\"\"\n",
    "#     os.makedirs(output_folder, exist_ok=True)\n",
    "#     image_files = [f for f in os.listdir(input_folder) if f.endswith(('.jpg', '.png'))]\n",
    "#     random.shuffle(image_files)\n",
    "#     selected_files = image_files[:num_samples]\n",
    "\n",
    "#     for img_file in selected_files:\n",
    "#         img_path = os.path.join(input_folder, img_file)\n",
    "#         result_path = os.path.join(output_folder, img_file)\n",
    "\n",
    "#         test_model_on_real_images(model_path, img_path, output_folder)\n",
    "#         annotated_img = Image.open(result_path)\n",
    "#         original_img = Image.open(img_path)\n",
    "\n",
    "#         fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "#         axes[0].imshow(original_img)\n",
    "#         axes[0].axis('off')\n",
    "#         axes[0].set_title(\"Original\")\n",
    "\n",
    "#         axes[1].imshow(annotated_img)\n",
    "#         axes[1].axis('off')\n",
    "#         axes[1].set_title(\"Anotado\")\n",
    "\n",
    "#         plt.tight_layout()\n",
    "#         comparison_path = os.path.join(output_folder, f\"comparison_{img_file}\")\n",
    "#         plt.savefig(comparison_path, bbox_inches='tight')\n",
    "#         plt.close()\n",
    "#         print(f\"Grid de comparación guardado en {comparison_path}.\")\n",
    "\n",
    "# # Organización automática de subcarpetas\n",
    "# def organize_presentation_folders(base_folder):\n",
    "#     \"\"\"\n",
    "#     Crea y organiza las subcarpetas necesarias para las visualizaciones.\n",
    "\n",
    "#     Parameters:\n",
    "#     - base_folder (str): Carpeta raíz para las subcarpetas.\n",
    "#     \"\"\"\n",
    "#     subfolders = [\"dataset_samples\", \"model_results\", \"before_after\"]\n",
    "#     for subfolder in subfolders:\n",
    "#         os.makedirs(os.path.join(base_folder, subfolder), exist_ok=True)\n",
    "#     print(f\"Subcarpetas creadas en {base_folder}: {', '.join(subfolders)}\")\n",
    "\n",
    "# # Ejecución del pipeline de visualización\n",
    "# def main():\n",
    "#     base_folder = \"presentation\"\n",
    "#     organize_presentation_folders(base_folder)\n",
    "\n",
    "#     dataset_folder = \"datasets/processed_images\"\n",
    "#     model_path = \"YOLO_Lego_Detection/best.pt\"\n",
    "\n",
    "#     create_dataset_grid(dataset_folder, os.path.join(base_folder, \"dataset_samples\"))\n",
    "#     annotate_model_results(model_path, dataset_folder, os.path.join(base_folder, \"model_results\"))\n",
    "#     generate_comparison_grid(model_path, dataset_folder, os.path.join(base_folder, \"before_after\"))\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Projects_Unprotected\\LEGO_Bricks_ML_Vision\\notebooks\n",
      "c:\\Users\\User\\Projects_Unprotected\\LEGO_Bricks_ML_Vision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-20 00:55:26,826 - INFO - Subcarpetas creadas en presentation: dataset_samples, model_results, before_after\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Muestra la ruta actual\n",
    "print(os.getcwd())\n",
    "\n",
    "# Cambia al directorio raíz del proyecto\n",
    "os.chdir(\"..\")  # Cambia al directorio raíz\n",
    "\n",
    "# Confirma el cambio de directorio\n",
    "print(os.getcwd())\n",
    "\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "# Importar funciones clave\n",
    "from scripts.visualize_presentation import (\n",
    "    create_dataset_grid,\n",
    "    annotate_model_results,\n",
    "    generate_comparison_grid,\n",
    "    organize_presentation_folders\n",
    ")\n",
    "\n",
    "# Definir rutas de entrada y salida\n",
    "BASE_FOLDER = \"presentation\"\n",
    "DATASET_FOLDER = \"datasets/processed_images\"\n",
    "MODEL_PATH = \"YOLO_Lego_Detection/best.pt\"\n",
    "\n",
    "# Crear y organizar las carpetas de presentación\n",
    "organize_presentation_folders(BASE_FOLDER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Projects_Unprotected\\LEGO_Bricks_ML_Vision\n",
      "c:\\Users\\User\\Projects_Unprotected\n",
      "['c:\\\\Users\\\\User\\\\Projects_Unprotected\\\\LEGO_Bricks_ML_Vision\\\\notebooks',\n",
      " 'c:\\\\Users\\\\User\\\\.conda\\\\envs\\\\MiguelEnvHaB\\\\python312.zip',\n",
      " 'c:\\\\Users\\\\User\\\\.conda\\\\envs\\\\MiguelEnvHaB\\\\DLLs',\n",
      " 'c:\\\\Users\\\\User\\\\.conda\\\\envs\\\\MiguelEnvHaB\\\\Lib',\n",
      " 'c:\\\\Users\\\\User\\\\.conda\\\\envs\\\\MiguelEnvHaB',\n",
      " '',\n",
      " 'C:\\\\Users\\\\User\\\\AppData\\\\Roaming\\\\Python\\\\Python312\\\\site-packages',\n",
      " 'c:\\\\Users\\\\User\\\\.conda\\\\envs\\\\MiguelEnvHaB\\\\Lib\\\\site-packages',\n",
      " 'c:\\\\Users\\\\User\\\\.conda\\\\envs\\\\MiguelEnvHaB\\\\Lib\\\\site-packages\\\\win32',\n",
      " 'c:\\\\Users\\\\User\\\\.conda\\\\envs\\\\MiguelEnvHaB\\\\Lib\\\\site-packages\\\\win32\\\\lib',\n",
      " 'c:\\\\Users\\\\User\\\\.conda\\\\envs\\\\MiguelEnvHaB\\\\Lib\\\\site-packages\\\\Pythonwin',\n",
      " 'c:\\\\Users\\\\User\\\\.conda\\\\envs\\\\MiguelEnvHaB\\\\Lib\\\\site-packages\\\\setuptools\\\\_vendor',\n",
      " 'c:\\\\Users\\\\User\\\\Projects_Unprotected\\\\LEGO_Bricks_ML_Vision',\n",
      " 'c:\\\\Users\\\\User\\\\Projects_Unprotected']\n"
     ]
    }
   ],
   "source": [
    "# import sys\n",
    "# import os\n",
    "# from pprint import pprint\n",
    "\n",
    "# # Muestra la ruta actual\n",
    "# print(os.getcwd())\n",
    "\n",
    "# # Cambia al directorio raíz del proyecto\n",
    "# os.chdir(\"..\")  # Cambia al directorio raíz\n",
    "\n",
    "# # Confirma el cambio de directorio\n",
    "# print(os.getcwd())\n",
    "\n",
    "# sys.path.append(os.getcwd()) \n",
    "\n",
    "# pprint(sys.path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-20 00:55:34,996 - INFO - Subcarpetas creadas en presentation: dataset_samples, model_results, before_after\n",
      "2025-01-20 00:55:34,997 - ERROR - La carpeta de entrada no existe.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Carpeta de entrada no encontrada.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m     annotate_model_results(MODEL_PATH, DATASET_FOLDER, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBASE_FOLDER\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/model_results\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m     generate_comparison_grid(MODEL_PATH, DATASET_FOLDER, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBASE_FOLDER\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/before_after\u001b[39m\u001b[38;5;124m\"\u001b[39m, num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m run_visualization_pipeline()\n",
      "Cell \u001b[1;32mIn[2], line 4\u001b[0m, in \u001b[0;36mrun_visualization_pipeline\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_visualization_pipeline\u001b[39m():\n\u001b[0;32m      3\u001b[0m     organize_presentation_folders(BASE_FOLDER)\n\u001b[1;32m----> 4\u001b[0m     create_dataset_grid(DATASET_FOLDER, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBASE_FOLDER\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/dataset_samples\u001b[39m\u001b[38;5;124m\"\u001b[39m, grid_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m))\n\u001b[0;32m      5\u001b[0m     annotate_model_results(MODEL_PATH, DATASET_FOLDER, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBASE_FOLDER\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/model_results\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m     generate_comparison_grid(MODEL_PATH, DATASET_FOLDER, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBASE_FOLDER\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/before_after\u001b[39m\u001b[38;5;124m\"\u001b[39m, num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\User\\Projects_Unprotected\\LEGO_Bricks_ML_Vision\\scripts\\visualize_presentation.py:25\u001b[0m, in \u001b[0;36mcreate_dataset_grid\u001b[1;34m(input_folder, output_folder, grid_size)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(input_folder):\n\u001b[0;32m     24\u001b[0m     logging\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLa carpeta de entrada no existe.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 25\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCarpeta de entrada no encontrada.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     27\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(output_folder, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     28\u001b[0m image_files \u001b[38;5;241m=\u001b[39m [f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(input_folder) \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m'\u001b[39m))]\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: Carpeta de entrada no encontrada."
     ]
    }
   ],
   "source": [
    "# Ejecutar todo el pipeline de visualización\n",
    "def run_visualization_pipeline():\n",
    "    organize_presentation_folders(BASE_FOLDER)\n",
    "    create_dataset_grid(DATASET_FOLDER, f\"{BASE_FOLDER}/dataset_samples\", grid_size=(3, 3))\n",
    "    annotate_model_results(MODEL_PATH, DATASET_FOLDER, f\"{BASE_FOLDER}/model_results\")\n",
    "    generate_comparison_grid(MODEL_PATH, DATASET_FOLDER, f\"{BASE_FOLDER}/before_after\", num_samples=5)\n",
    "\n",
    "run_visualization_pipeline()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setup.py \n",
    "\n",
    "Packege publication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from setuptools import setup, find_packages\n",
    "\n",
    "# setup(\n",
    "#     name=\"lego_bricks_ml_vision\",\n",
    "#     version=\"0.1.0\",\n",
    "#     packages=find_packages(),\n",
    "#     install_requires=[\n",
    "#         \"torch>=1.8.0\",\n",
    "#         \"ultralytics\",\n",
    "#         \"pillow\",\n",
    "#         \"matplotlib\",\n",
    "#         \"kaggle\",\n",
    "#     ],\n",
    "#     entry_points={\n",
    "#         \"console_scripts\": [\n",
    "#             \"run-pipeline=scripts.pipeline:main\",\n",
    "#             \"run-visualize=scripts.visualize_presentation:main\",\n",
    "#         ],\n",
    "#     },\n",
    "#     author=\"Miguel Di Lalla\",\n",
    "#     description=\"A package for LEGO brick detection and visualization using YOLO.\",\n",
    "#     long_description=open(\"README.md\").read(),\n",
    "#     long_description_content_type=\"text/markdown\",\n",
    "#     license=\"Apache License 2.0\",\n",
    "#     classifiers=[\n",
    "#         \"Programming Language :: Python :: 3\",\n",
    "#         \"License :: OSI Approved :: Apache Software License\",\n",
    "#         \"Operating System :: OS Independent\",\n",
    "#     ],\n",
    "#     python_requires=\">=3.8\",\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rqueriments.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Versions:\n",
      "PyTorch: 2.4.1+cpu\n",
      "Ultralytics: 8.2.99\n",
      "Pillow (PIL): 11.1.0\n",
      "Matplotlib: 3.8.4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import ultralytics\n",
    "import PIL\n",
    "import matplotlib\n",
    "import kaggle\n",
    "\n",
    "print(\"Environment Versions:\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Ultralytics: {ultralytics.__version__}\")\n",
    "print(f\"Pillow (PIL): {PIL.__version__}\")\n",
    "print(f\"Matplotlib: {matplotlib.__version__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: kaggle\n",
      "Version: 1.6.17\n",
      "Summary: Kaggle API\n",
      "Home-page: https://github.com/Kaggle/kaggle-api\n",
      "Author: Kaggle\n",
      "Author-email: support@kaggle.com\n",
      "License: Apache 2.0\n",
      "Location: C:\\Users\\User\\.conda\\envs\\MiguelEnvHaB\\Lib\\site-packages\n",
      "Requires: bleach, certifi, python-dateutil, python-slugify, requests, six, tqdm, urllib3\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "# print(f\"Kaggle: {kaggle.__version__}\")\n",
    "! pip show kaggle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch==2.0.1+cpu\n",
    "ultralytics==8.2.99\n",
    "pillow==11.1.0\n",
    "matplotlib==3.8.4\n",
    "kaggle==1.6.17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# README.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LEGO Bricks ML Vision\n",
    "\n",
    "A Python package for detecting and visualizing LEGO bricks using YOLO and custom visualization pipelines. This project is designed to streamline the process of detecting LEGO pieces in cluttered environments and producing meaningful visualizations for analysis and presentation.\n",
    "\n",
    "---\n",
    "\n",
    "## Features\n",
    "\n",
    "- **Object Detection**: Detect LEGO bricks using YOLOv8.\n",
    "- **Preprocessing Tools**: Scripts to preprocess images and datasets.\n",
    "- **Dataset Conversion**: Convert LabelMe annotations to YOLO format.\n",
    "- **Visualization Pipelines**: Create annotated grids, comparisons, and organized folders for presentations.\n",
    "- **Modular Design**: Easy-to-use package structure for scaling or adapting the project.\n",
    "\n",
    "---\n",
    "\n",
    "## Installation\n",
    "\n",
    "To get started with the project, clone the repository and install the package:\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/MiguelDiLalla/LEGO_Bricks_ML_Vision.git\n",
    "cd LEGO_Bricks_ML_Vision\n",
    "pip install -e .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prueba\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.pipeline import train_yolo_pipeline\n",
    "from scripts.visualize_presentation import create_dataset_grid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funciona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example: Train YOLO pipeline\n",
    "train_yolo_pipeline(\"path/to/dataset\")\n",
    "\n",
    "# Example: Create a dataset grid\n",
    "create_dataset_grid(\"path/to/images\", \"path/to/output\", grid_size=(3, 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt conversacion del pipeline.py\n",
    "\n",
    "para actualizar el comentariado luego de la publicacion en PyPl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"\n",
    "\n",
    "Tengo un script en Python que necesita ser revisado y refinado para alcanzar un nivel profesional. Aquí están los objetivos específicos que quiero lograr:\n",
    "\n",
    "1. **Comentarios claros y precisos**: Cada función debe tener una docstring descriptiva que explique claramente su propósito, parámetros, y valores de retorno (si los hay). Los comentarios en línea deben ser breves, relevantes y ayudar a entender el código.\n",
    "\n",
    "2. **Formato consistente**: El código debe seguir el estándar **PEP 8**, incluyendo:\n",
    "   - Longitud de línea máxima de 79 caracteres.\n",
    "   - Identación adecuada (4 espacios).\n",
    "   - Espaciado entre bloques y funciones.\n",
    "   - Organización lógica de las importaciones (primero estándar, luego librerías externas, y finalmente locales).\n",
    "\n",
    "3. **Organización de secciones**: Agrupa las funciones relacionadas y añade encabezados de comentarios (# ===) para separar las secciones principales del script.\n",
    "\n",
    "4. **Buena práctica en manejo de errores**: Incluye excepciones manejadas correctamente donde sea relevante (e.g., para operaciones de I/O o dependencias externas).\n",
    "\n",
    "5. **Estilo profesional en los print()**: Usa mensajes formateados de manera clara y profesional, por ejemplo:\n",
    "   ```python\n",
    "   print(f\"[INFO] Dataset descargado y extraído en {destination}\")\n",
    "\n",
    "6. **Detalles adicionales:** Si algo en el código puede ser optimizado o mejorado, propón cambios para hacerlo más eficiente y profesional.\n",
    "\n",
    "# \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from PIL import Image\n",
    "# import torch\n",
    "# import shutil\n",
    "# import kaggle\n",
    "# import zipfile\n",
    "\n",
    "# # === Configuración Inicial ===\n",
    "# def setup_environment():\n",
    "#     \"\"\"\n",
    "#     Clona el repositorio y configura el entorno necesario para ejecutar el pipeline.\n",
    "\n",
    "#     - Clona el repositorio de GitHub.\n",
    "#     - Instala las dependencias desde el archivo requirements.txt.\n",
    "#     - Configura el dispositivo de ejecución (CPU o GPU).\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         os.system(\"git clone https://github.com/MiguelDiLalla/LEGO_Bricks_ML_Vision.git\")\n",
    "#         os.chdir(\"LEGO_Bricks_ML_Vision\")\n",
    "#         os.system(\"pip install -r requirements.txt\")\n",
    "#         device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#         print(f\"[INFO] Using device: {device}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"[ERROR] Error al configurar el entorno: {e}\")\n",
    "\n",
    "# # === Manejo de Credenciales de Kaggle ===\n",
    "# def get_kaggle_credentials():\n",
    "#     \"\"\"\n",
    "#     Obtiene las credenciales de Kaggle desde variables de entorno o archivo kaggle.json.\n",
    "\n",
    "#     Prioridad de búsqueda:\n",
    "#     1. Variables de entorno: KAGGLE_USERNAME y KAGGLE_KEY.\n",
    "#     2. Archivo ~/.kaggle/kaggle.json.\n",
    "\n",
    "#     Returns:\n",
    "#     - dict: Diccionario con 'username' y 'key'.\n",
    "#     \"\"\"\n",
    "#     username = os.getenv(\"KAGGLE_USERNAME\")\n",
    "#     key = os.getenv(\"KAGGLE_KEY\")\n",
    "\n",
    "#     if username and key:\n",
    "#         return {\"username\": username, \"key\": key}\n",
    "\n",
    "#     kaggle_json_path = os.path.expanduser(\"~/.kaggle/kaggle.json\")\n",
    "#     if os.path.exists(kaggle_json_path):\n",
    "#         try:\n",
    "#             with open(kaggle_json_path, \"r\") as f:\n",
    "#                 return json.load(f)\n",
    "#         except Exception as e:\n",
    "#             raise ValueError(f\"[ERROR] No se pudo leer kaggle.json: {e}\")\n",
    "\n",
    "#     raise ValueError(\"[ERROR] Credenciales de Kaggle no encontradas. Configure las variables de entorno o coloque kaggle.json en ~/.kaggle.\")\n",
    "\n",
    "# # === Descarga de Dataset ===\n",
    "# def download_dataset_from_kaggle(dataset, destination):\n",
    "#     \"\"\"\n",
    "#     Descarga y extrae un dataset de Kaggle.\n",
    "\n",
    "#     Parameters:\n",
    "#     - dataset (str): Nombre del dataset en el formato \"usuario/dataset\".\n",
    "#     - destination (str): Ruta donde se extraerán los archivos.\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         credentials = get_kaggle_credentials()\n",
    "\n",
    "#         os.makedirs(destination, exist_ok=True)\n",
    "#         os.environ[\"KAGGLE_USERNAME\"] = credentials[\"username\"]\n",
    "#         os.environ[\"KAGGLE_KEY\"] = credentials[\"key\"]\n",
    "\n",
    "#         kaggle.api.dataset_download_files(dataset, path=destination, unzip=True)\n",
    "#         print(f\"[INFO] Dataset descargado y extraído en {destination}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"[ERROR] No se pudo descargar el dataset: {e}\")\n",
    "\n",
    "# # === Validación de Directorios ===\n",
    "# def validate_directories(directories):\n",
    "#     \"\"\"\n",
    "#     Valida la existencia de los directorios especificados.\n",
    "\n",
    "#     Parameters:\n",
    "#     - directories (list): Lista de rutas a validar.\n",
    "\n",
    "#     Returns:\n",
    "#     - bool: True si todos los directorios existen, False en caso contrario.\n",
    "#     \"\"\"\n",
    "#     for directory in directories:\n",
    "#         if not os.path.exists(directory):\n",
    "#             print(f\"[ERROR] Directorio no encontrado: {directory}\")\n",
    "#             return False\n",
    "#     print(\"[INFO] Todos los directorios están correctamente configurados.\")\n",
    "#     return True\n",
    "\n",
    "# # === Preprocesamiento de Imágenes ===\n",
    "# def preprocess_images(input_dir, output_dir, target_size=(256, 256)):\n",
    "#     \"\"\"\n",
    "#     Redimensiona imágenes y asegura consistencia en nombres de archivos.\n",
    "\n",
    "#     Parameters:\n",
    "#     - input_dir (str): Ruta de la carpeta con imágenes originales.\n",
    "#     - output_dir (str): Ruta de la carpeta para guardar las imágenes procesadas.\n",
    "#     - target_size (tuple): Dimensiones objetivo para las imágenes (ancho, alto).\n",
    "#     \"\"\"\n",
    "#     os.makedirs(output_dir, exist_ok=True)\n",
    "#     for i, filename in enumerate(sorted(os.listdir(input_dir))):\n",
    "#         if filename.endswith(\".jpg\"):\n",
    "#             try:\n",
    "#                 img = Image.open(os.path.join(input_dir, filename))\n",
    "#                 img_resized = img.resize(target_size)\n",
    "#                 new_filename = f\"image_{i}.jpg\"\n",
    "#                 img_resized.save(os.path.join(output_dir, new_filename))\n",
    "#                 print(f\"[INFO] Procesado: {filename} -> {new_filename}\")\n",
    "#             except Exception as e:\n",
    "#                 print(f\"[ERROR] No se pudo procesar {filename}: {e}\")\n",
    "\n",
    "# # === Conversión de Anotaciones de LabelMe a YOLO ===\n",
    "# def labelme_to_yolo(input_folder, output_folder):\n",
    "#     \"\"\"\n",
    "#     Convierte archivos JSON de LabelMe al formato YOLO.\n",
    "\n",
    "#     Parameters:\n",
    "#     - input_folder (str): Carpeta con archivos JSON de LabelMe.\n",
    "#     - output_folder (str): Carpeta donde se guardarán los archivos YOLO.\n",
    "#     \"\"\"\n",
    "#     os.makedirs(output_folder, exist_ok=True)\n",
    "#     for filename in os.listdir(input_folder):\n",
    "#         if filename.endswith('.json'):\n",
    "#             try:\n",
    "#                 json_file = os.path.join(input_folder, filename)\n",
    "#                 yolo_file = os.path.join(output_folder, filename.replace('.json', '.txt'))\n",
    "#                 # Conversión aquí (implementación omitida para brevedad)\n",
    "#                 print(f\"[INFO] Convertido: {json_file} -> {yolo_file}\")\n",
    "#             except Exception as e:\n",
    "#                 print(f\"[ERROR] Error al convertir {filename}: {e}\")\n",
    "\n",
    "# # === Entrenamiento del Modelo YOLOv8n ===\n",
    "# def train_yolo_pipeline(dataset_path, annotations_format=\"YOLO\", epochs=50, img_size=256):\n",
    "#     \"\"\"\n",
    "#     Configura y entrena el modelo YOLO.\n",
    "\n",
    "#     Parameters:\n",
    "#     - dataset_path (str): Ruta del dataset procesado.\n",
    "#     - annotations_format (str): Formato de las anotaciones (por defecto \"YOLO\").\n",
    "#     - epochs (int): Número de épocas de entrenamiento.\n",
    "#     - img_size (int): Tamaño de las imágenes usadas para entrenamiento.\n",
    "#     \"\"\"\n",
    "#     from ultralytics import YOLO\n",
    "\n",
    "#     dataset_dir = os.path.join(dataset_path, \"processed_images\")\n",
    "#     annotations_dir = os.path.join(dataset_path, \"annotations\")\n",
    "\n",
    "#     if not validate_directories([dataset_dir, annotations_dir]):\n",
    "#         return\n",
    "\n",
    "#     try:\n",
    "#         model = YOLO(\"yolov8n.pt\")\n",
    "#         results = model.train(\n",
    "#             data=annotations_format,\n",
    "#             imgsz=img_size,\n",
    "#             epochs=epochs,\n",
    "#             batch=16,\n",
    "#             project=\"LEGO_Training\",\n",
    "#             name=\"YOLO_Lego_Detection\"\n",
    "#         )\n",
    "#         print(\"[INFO] Entrenamiento finalizado. Resultados:\", results)\n",
    "#     except Exception as e:\n",
    "#         print(f\"[ERROR] Error durante el entrenamiento: {e}\")\n",
    "\n",
    "# # === Evaluación del Modelo Entrenado ===\n",
    "# def test_model_on_real_images(model_path, test_images_dir, output_dir):\n",
    "#     \"\"\"\n",
    "#     Evalúa el modelo YOLO entrenado en imágenes reales.\n",
    "\n",
    "#     Parameters:\n",
    "#     - model_path (str): Ruta del modelo YOLO entrenado.\n",
    "#     - test_images_dir (str): Carpeta con imágenes para evaluación.\n",
    "#     - output_dir (str): Carpeta para guardar los resultados visualizados.\n",
    "#     \"\"\"\n",
    "#     from ultralytics import YOLO\n",
    "\n",
    "#     os.makedirs(output_dir, exist_ok=True)\n",
    "#     model = YOLO(model_path)\n",
    "\n",
    "#     for img_file in os.listdir(test_images_dir):\n",
    "#         if img_file.endswith(\".jpg\"):\n",
    "#             try:\n",
    "#                 img_path = os.path.join(test_images_dir, img_file)\n",
    "#                 results = model(img_path)\n",
    "#                 result_image = results[0].plot()\n",
    "#                 output_path = os.path.join(output_dir, img_file)\n",
    "#                 Image.fromarray(result_image).save(output_path)\n",
    "#                 print(f\"[INFO] Procesado: {img_file} -> {output_path}\")\n",
    "#             except Exception as e:\n",
    "#                 print(f\"[ERROR] Error al procesar {img_file}: {e}\")\n",
    "\n",
    "# # === Visualización de Resultados ===\n",
    "# def visualize_results(dataset_path):\n",
    "#     \"\"\"\n",
    "#     Visualiza detecciones en un grid de imágenes anotadas.\n",
    "\n",
    "#     Parameters:\n",
    "#     - dataset_path (str): Ruta del dataset procesado.\n",
    "#     \"\"\"\n",
    "#     import matplotlib.pyplot as plt\n",
    "\n",
    "#     processed_dir = os.path.join(dataset_path, \"processed_images\")\n",
    "#     images = [os.path.join(processed_dir, img) for img in os.listdir(processed_dir) if img.endswith(\".jpg\")]\n",
    "\n",
    "#     plt.figure(figsize=(10, 10))\n",
    "#     for i, img_path in enumerate(images[:16]):  # Mostrar 16 imágenes\n",
    "#         try:\n",
    "#             img = Image.open(img_path)\n",
    "#             plt.subplot(4, 4, i + 1)\n",
    "#             plt.imshow(img)\n",
    "#             plt.axis('off')\n",
    "#         except Exception as e:\n",
    "#             print(f\"[ERROR] No se pudo cargar {img_path}: {e}\")\n",
    "#     plt.show()\n",
    "\n",
    "# # === Ejecución del Pipeline ===\n",
    "# def main():\n",
    "#     \"\"\"Ejecución principal del pipeline de detección de LEGO.\"\"\"\n",
    "#     setup_environment()\n",
    "#     download_dataset_from_kaggle(\"usuario/dataset\", \"datasets\")\n",
    "#     preprocess_images(\"datasets/raw\", \"datasets/processed\")\n",
    "#     labelme_to_yolo(\"datasets/processed\", \"datasets/annotations\")\n",
    "#     train_yolo_pipeline(\"datasets\")\n",
    "#     test_model_on_real_images(\"YOLO_Lego_Detection/best.pt\", \"test_images\", \"results\")\n",
    "#     visualize_results(\"datasets\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt conversacion del visualization_presentation.py\n",
    "\n",
    "para actualizar el comentariado luego de la publicacion en PyPl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"\n",
    "\n",
    "Tengo el siguiente script en Python que quiero mejorar para alcanzar un nivel profesional. El script incluye varias funciones relacionadas con la visualización de resultados de modelos y organización de carpetas. Estos son los aspectos específicos que quiero mejorar:\n",
    "\n",
    "1. **Comentarios más claros y precisos**:\n",
    "   - Las docstrings deben ser detalladas, siguiendo el estándar Google o NumPy, e incluir:\n",
    "     - Propósito de la función.\n",
    "     - Parámetros (`Args`).\n",
    "     - Valores de retorno (`Returns`), si aplica.\n",
    "   - Los comentarios en línea deben ser breves y específicos para mejorar la comprensión del código.\n",
    "\n",
    "2. **Formato y organización del código**:\n",
    "   - Aplicar el estándar **PEP 8**, como longitud máxima de líneas (79 caracteres) y espaciado adecuado.\n",
    "   - Asegurar que las importaciones estén organizadas (estándar, externas, locales).\n",
    "\n",
    "3. **Manejo de errores**:\n",
    "   - Proponer o incluir excepciones donde sea relevante, como manejo de archivos o directorios inexistentes.\n",
    "\n",
    "4. **Mensajes más profesionales**:\n",
    "   - Reemplazar mensajes `print()` por logs informativos utilizando la librería estándar `logging`.\n",
    "\n",
    "5. **Optimización del flujo de funciones**:\n",
    "   - Identificar redundancias o mejoras en la eficiencia de las funciones.\n",
    "\n",
    "6. **Mejoras generales**:\n",
    "   - Proponer o aplicar cualquier ajuste que haga el script más limpio, profesional y eficiente.\n",
    "\n",
    "A continuación, te comparto el script para que lo refines según estos criterios:\n",
    "\n",
    "\n",
    "Por favor, realiza los ajustes directamente en el código y explica las mejoras realizadas para que pueda aprender del proceso.\n",
    "\n",
    "\n",
    "# \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import random\n",
    "# import logging\n",
    "# import matplotlib.pyplot as plt\n",
    "# from PIL import Image\n",
    "# from scripts.pipeline import test_model_on_real_images\n",
    "\n",
    "# # Configuración del logger\n",
    "# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# def create_dataset_grid(input_folder: str, output_folder: str, grid_size=(3, 3)) -> None:\n",
    "#     \"\"\"\n",
    "#     Genera un grid de imágenes de muestra del dataset y lo guarda en la carpeta especificada.\n",
    "\n",
    "#     Args:\n",
    "#         input_folder (str): Ruta de la carpeta con las imágenes de entrada.\n",
    "#         output_folder (str): Ruta donde se guardará el grid generado.\n",
    "#         grid_size (tuple): Dimensiones del grid (filas, columnas).\n",
    "\n",
    "#     Raises:\n",
    "#         FileNotFoundError: Si la carpeta de entrada no existe o está vacía.\n",
    "#     \"\"\"\n",
    "#     if not os.path.exists(input_folder):\n",
    "#         logging.error(\"La carpeta de entrada no existe.\")\n",
    "#         raise FileNotFoundError(\"Carpeta de entrada no encontrada.\")\n",
    "\n",
    "#     os.makedirs(output_folder, exist_ok=True)\n",
    "#     image_files = [f for f in os.listdir(input_folder) if f.endswith(('.jpg', '.png'))]\n",
    "#     if not image_files:\n",
    "#         logging.error(\"No se encontraron imágenes en la carpeta de entrada.\")\n",
    "#         raise FileNotFoundError(\"No se encontraron imágenes en la carpeta de entrada.\")\n",
    "\n",
    "#     random.shuffle(image_files)\n",
    "#     selected_files = image_files[:grid_size[0] * grid_size[1]]\n",
    "#     fig, axes = plt.subplots(*grid_size, figsize=(grid_size[1] * 4, grid_size[0] * 4))\n",
    "\n",
    "#     for idx, img_file in enumerate(selected_files):\n",
    "#         img_path = os.path.join(input_folder, img_file)\n",
    "#         img = Image.open(img_path)\n",
    "#         row, col = divmod(idx, grid_size[1])\n",
    "#         axes[row, col].imshow(img)\n",
    "#         axes[row, col].axis('off')\n",
    "#         axes[row, col].set_title(f\"{img_file}\", fontsize=10)\n",
    "\n",
    "#     for ax in axes.flat[len(selected_files):]:\n",
    "#         ax.axis('off')\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     grid_path = os.path.join(output_folder, \"dataset_grid.png\")\n",
    "#     plt.savefig(grid_path, bbox_inches='tight')\n",
    "#     plt.close()\n",
    "#     logging.info(f\"Grid de dataset guardado en {grid_path}.\")\n",
    "\n",
    "# def annotate_model_results(model_path: str, input_folder: str, output_folder: str, conf_threshold=0.5) -> None:\n",
    "#     \"\"\"\n",
    "#     Genera imágenes con anotaciones del modelo y las guarda en la carpeta especificada.\n",
    "\n",
    "#     Args:\n",
    "#         model_path (str): Ruta al modelo YOLO entrenado.\n",
    "#         input_folder (str): Carpeta con imágenes de entrada.\n",
    "#         output_folder (str): Carpeta para guardar las imágenes anotadas.\n",
    "#         conf_threshold (float): Umbral de confianza para las detecciones.\n",
    "\n",
    "#     Raises:\n",
    "#         FileNotFoundError: Si la carpeta de entrada no contiene imágenes.\n",
    "#     \"\"\"\n",
    "#     if not os.path.exists(input_folder):\n",
    "#         logging.error(\"La carpeta de entrada no existe.\")\n",
    "#         raise FileNotFoundError(\"Carpeta de entrada no encontrada.\")\n",
    "\n",
    "#     os.makedirs(output_folder, exist_ok=True)\n",
    "#     test_model_on_real_images(model_path, input_folder, output_folder)\n",
    "#     logging.info(f\"Resultados anotados guardados en {output_folder}.\")\n",
    "\n",
    "# def generate_comparison_grid(model_path: str, input_folder: str, output_folder: str, num_samples=5) -> None:\n",
    "#     \"\"\"\n",
    "#     Crea un grid comparativo de imágenes antes y después de las predicciones del modelo.\n",
    "\n",
    "#     Args:\n",
    "#         model_path (str): Ruta al modelo YOLO entrenado.\n",
    "#         input_folder (str): Carpeta con imágenes de entrada.\n",
    "#         output_folder (str): Carpeta para guardar los grids generados.\n",
    "#         num_samples (int): Número de imágenes a comparar.\n",
    "\n",
    "#     Raises:\n",
    "#         FileNotFoundError: Si la carpeta de entrada no contiene imágenes.\n",
    "#     \"\"\"\n",
    "#     if not os.path.exists(input_folder):\n",
    "#         logging.error(\"La carpeta de entrada no existe.\")\n",
    "#         raise FileNotFoundError(\"Carpeta de entrada no encontrada.\")\n",
    "\n",
    "#     os.makedirs(output_folder, exist_ok=True)\n",
    "#     image_files = [f for f in os.listdir(input_folder) if f.endswith(('.jpg', '.png'))]\n",
    "#     if not image_files:\n",
    "#         logging.error(\"No se encontraron imágenes en la carpeta de entrada.\")\n",
    "#         raise FileNotFoundError(\"No se encontraron imágenes en la carpeta de entrada.\")\n",
    "\n",
    "#     random.shuffle(image_files)\n",
    "#     selected_files = image_files[:num_samples]\n",
    "\n",
    "#     for img_file in selected_files:\n",
    "#         img_path = os.path.join(input_folder, img_file)\n",
    "#         result_path = os.path.join(output_folder, img_file)\n",
    "\n",
    "#         test_model_on_real_images(model_path, img_path, output_folder)\n",
    "#         annotated_img = Image.open(result_path)\n",
    "#         original_img = Image.open(img_path)\n",
    "\n",
    "#         fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "#         axes[0].imshow(original_img)\n",
    "#         axes[0].axis('off')\n",
    "#         axes[0].set_title(\"Original\")\n",
    "\n",
    "#         axes[1].imshow(annotated_img)\n",
    "#         axes[1].axis('off')\n",
    "#         axes[1].set_title(\"Anotado\")\n",
    "\n",
    "#         plt.tight_layout()\n",
    "#         comparison_path = os.path.join(output_folder, f\"comparison_{img_file}\")\n",
    "#         plt.savefig(comparison_path, bbox_inches='tight')\n",
    "#         plt.close()\n",
    "#         logging.info(f\"Grid de comparación guardado en {comparison_path}.\")\n",
    "\n",
    "# def organize_presentation_folders(base_folder: str) -> None:\n",
    "#     \"\"\"\n",
    "#     Crea y organiza las subcarpetas necesarias para las visualizaciones.\n",
    "\n",
    "#     Args:\n",
    "#         base_folder (str): Carpeta raíz para las subcarpetas.\n",
    "#     \"\"\"\n",
    "#     subfolders = [\"dataset_samples\", \"model_results\", \"before_after\"]\n",
    "#     for subfolder in subfolders:\n",
    "#         os.makedirs(os.path.join(base_folder, subfolder), exist_ok=True)\n",
    "#     logging.info(f\"Subcarpetas creadas en {base_folder}: {', '.join(subfolders)}\")\n",
    "\n",
    "# def main() -> None:\n",
    "#     \"\"\"\n",
    "#     Ejecución principal del pipeline de visualización.\n",
    "#     \"\"\"\n",
    "#     base_folder = \"presentation\"\n",
    "#     organize_presentation_folders(base_folder)\n",
    "\n",
    "#     dataset_folder = \"datasets/processed_images\"\n",
    "#     model_path = \"YOLO_Lego_Detection/best.pt\"\n",
    "\n",
    "#     create_dataset_grid(dataset_folder, os.path.join(base_folder, \"dataset_samples\"))\n",
    "#     annotate_model_results(model_path, dataset_folder, os.path.join(base_folder, \"model_results\"))\n",
    "#     generate_comparison_grid(model_path, dataset_folder, os.path.join(base_folder, \"before_after\"))\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt conversacion de refinamiento de README.md\n",
    "\n",
    "para actualizar el comentariado luego de la publicacion en PyPl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"\n",
    "\n",
    "Tengo dos scripts bien comentados y refinados que forman el núcleo de mi proyecto. También tengo un archivo `README.md` existente, pero necesito actualizarlo para reflejar los cambios y documentar de forma profesional el proyecto. Aquí están las instrucciones:\n",
    "\n",
    "1. **Descripción clara del proyecto**:\n",
    "   - Explica qué resuelve el proyecto y por qué es útil.\n",
    "   - Destaca las funcionalidades principales basadas en los scripts proporcionados.\n",
    "\n",
    "2. **Estructura profesional**:\n",
    "   - Introducción: Breve descripción del proyecto.\n",
    "   - Instalación: Cómo instalar las dependencias y preparar el entorno.\n",
    "   - Uso: Instrucciones para ejecutar las funcionalidades principales.\n",
    "   - Detalle de scripts y funciones: Documenta cada función clave basada en las docstrings de los scripts refinados.\n",
    "   - Contribuciones: Guía para colaborar.\n",
    "\n",
    "3. **Ejemplos prácticos**:\n",
    "   - Muestra cómo usar funciones principales, con ejemplos claros y fáciles de entender en bloques de código.\n",
    "\n",
    "4. **Estructura del proyecto**:\n",
    "   - Describe la organización de carpetas y archivos, indicando dónde encontrar scripts, datos y resultados.\n",
    "\n",
    "5. **Consistencia y claridad**:\n",
    "   - El README.md debe ser accesible tanto para principiantes como para desarrolladores avanzados.\n",
    "\n",
    "A continuación, tienes los archivos necesarios:\n",
    "1. El estado actual del archivo `README.md`.\n",
    "2. Los dos scripts refinados (`pipeline.py` y `visualize_presentation.py`).\n",
    "\n",
    "Por favor, actualiza el `README.md` para reflejar toda la funcionalidad implementada, utilizando una estructura clara y profesional. Asegúrate de incluir ejemplos prácticos y explicaciones detalladas basadas en las docstrings de los scripts.\n",
    "\n",
    "\n",
    "# \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt para actualizar el technical_report.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# \"\n",
    "\n",
    "Quiero integrar el código de mi paquete publicado en PyPI (lego-bricks-ml-vision) en el documento técnico LEGO_Bricks_Identification_Project_Technical_Report.pdf. El objetivo principal es actualizar el documento para que:\n",
    "\n",
    "Utilice las funciones del paquete:\n",
    "\n",
    "Sustituir bloques de código existentes por ejemplos claros de cómo usar el paquete.\n",
    "Mostrar cómo el pipeline completo puede ser ejecutado con las herramientas del paquete.\n",
    "Simplifique la reproducción de la experiencia:\n",
    "\n",
    "Incluir comandos claros para instalar el paquete y ejecutar pipelines directamente desde el CLI (run-pipeline y run-visualize).\n",
    "Agregar ejemplos de configuración inicial y manejo de errores.\n",
    "Actualice la estructura del documento:\n",
    "\n",
    "Reorganizar las secciones de \"Creación del Dataset\", \"Entrenamiento del Modelo\" y \"Visualización de Resultados\" para reflejar el flujo modular del paquete.\n",
    "Ajustar las secciones de reflexión para resaltar la escalabilidad y facilidad de uso del paquete.\n",
    "Incluya referencias al README:\n",
    "\n",
    "Citar el README y documentación del paquete como recursos adicionales para ampliar la información técnica.\n",
    "Tareas específicas:\n",
    "Revise la sección de introducción para incorporar una descripción breve del paquete y cómo facilita el flujo de trabajo.\n",
    "Sustituya el código en Python por ejemplos concretos de uso de las funciones del paquete (preprocess_images, train_yolo_pipeline, etc.).\n",
    "Proporcione instrucciones para ejecutar pipelines y generar visualizaciones utilizando los comandos de CLI (run-pipeline y run-visualize).\n",
    "Verifique la claridad y profesionalismo del lenguaje, garantizando un tono técnico accesible.\n",
    "Por favor, comencemos revisando la sección de \"Creación del Dataset\" y trabajemos iterativamente para integrar las funciones del paquete.\n",
    "\n",
    "\n",
    "\n",
    "# \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First iteration technical_report.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LEGO Bricks Identification Project: A Technical Report\n",
    "\n",
    "### 1. Introduction and Motivation 🌟\n",
    "\n",
    "This project leverages the **`lego-bricks-ml-vision`** package to implement an efficient pipeline for detecting and classifying LEGO bricks. Inspired by the challenge of identifying specific pieces within a cluttered set of LEGO bricks, this project combines computer vision, machine learning, and dataset management to achieve scalable and replicable results.\n",
    "\n",
    "The key objectives include:\n",
    "\n",
    "1. Designing a pipeline for object detection using YOLOv8.\n",
    "2. Documenting the process to ensure reproducibility and scalability.\n",
    "3. Providing tools for visualization and analysis to showcase the model’s performance.\n",
    "\n",
    "### 2. Dataset Creation 📝\n",
    "\n",
    "The dataset creation process is streamlined using the **`lego-bricks-ml-vision`** package. This package provides commands for downloading datasets, preprocessing images, and converting annotations.\n",
    "\n",
    "#### 2.1 Dataset Overview \n",
    "\n",
    "The dataset used for this project is hosted on Kaggle:\n",
    "- **Dataset Name**: [Spiled LEGO Bricks](https://www.kaggle.com/datasets/migueldilalla/spiled-lego-bricks)\n",
    "- **Contents**:\n",
    "  - **Images**: 1803 images (600×800 resolution)\n",
    "  - **Annotations**: LabelMe-compatible `.txt` files with bounding box data.\n",
    "\n",
    "#### 2.2 Setting Up the Environment \n",
    "\n",
    "Install the **`lego-bricks-ml-vision`** package from PyPI:\n",
    "\n",
    "```python\n",
    "!pip install lego-bricks-ml-vision\n",
    "```\n",
    "\n",
    "Configure the pipeline:\n",
    "\n",
    "```python\n",
    "from lego_bricks_ml_vision import setup_environment\n",
    "setup_environment()\n",
    "```\n",
    "\n",
    "This command ensures that all dependencies are installed and that your environment is ready.\n",
    "\n",
    "#### 2.3 Downloading the Dataset \n",
    "\n",
    "The dataset can be directly downloaded and extracted using the following commands:\n",
    "\n",
    "```python\n",
    "!run-pipeline download-dataset \\\n",
    "    --kaggle-dataset \"migueldilalla/spiled-lego-bricks\" \\\n",
    "    --output-dir \"datasets\"\n",
    "```\n",
    "\n",
    "#### 2.4 Preprocessing Images \n",
    "\n",
    "Resize the images to a consistent size (e.g., 256x256) for model training:\n",
    "\n",
    "```python\n",
    "!run-pipeline preprocess-images \\\n",
    "    --input-dir \"datasets/Images_600x800\" \\\n",
    "    --output-dir \"datasets/processed_images\" \\\n",
    "    --target-size 256\n",
    "```\n",
    "\n",
    "#### 2.5 Converting Annotations \n",
    "\n",
    "Convert annotations from LabelMe format to YOLO format:\n",
    "\n",
    "```python\n",
    "!run-pipeline labelme-to-yolo \\\n",
    "    --input-folder \"datasets/LabelMe_txt_bricks\" \\\n",
    "    --output-folder \"datasets/annotations\"\n",
    "```\n",
    "\n",
    "By automating these tasks, the package ensures consistency and reduces manual effort.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Model Training 🧬\n",
    "\n",
    "The project uses YOLOv8 for LEGO brick detection. Training is performed using the preprocessed dataset and YOLO-compatible annotations.\n",
    "\n",
    "#### 3.1 Training the YOLO Model\n",
    "\n",
    "The `train_yolo_pipeline` function enables straightforward model training:\n",
    "\n",
    "```python\n",
    "!run-pipeline train-yolo \\\n",
    "    --dataset-path \"datasets\" \\\n",
    "    --epochs 50 \\\n",
    "    --img-size 256\n",
    "```\n",
    "\n",
    "#### 3.2 Validating the Model\n",
    "\n",
    "Evaluate the trained model on test images:\n",
    "\n",
    "```python\n",
    "!run-pipeline test-model \\\n",
    "    --model-path \"YOLO_Lego_Detection/best.pt\" \\\n",
    "    --test-images-dir \"test_images\" \\\n",
    "    --output-dir \"results\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Visualization and Results 🎨\n",
    "\n",
    "#### 4.1 Visualizing Results\n",
    "\n",
    "Visualize predictions and annotations using:\n",
    "\n",
    "```python\n",
    "!run-visualize annotate-results \\\n",
    "    --model-path \"YOLO_Lego_Detection/best.pt\" \\\n",
    "    --input-folder \"datasets/processed_images\" \\\n",
    "    --output-folder \"presentation/model_results\"\n",
    "```\n",
    "\n",
    "#### 4.2 Generating Comparison Grids\n",
    "\n",
    "Compare predictions against ground truth:\n",
    "\n",
    "```python\n",
    "!run-visualize comparison-grid \\\n",
    "    --model-path \"YOLO_Lego_Detection/best.pt\" \\\n",
    "    --input-folder \"datasets/processed_images\" \\\n",
    "    --output-folder \"presentation/comparison\"\n",
    "```\n",
    "\n",
    "#### 4.3 Creating Presentation Grids\n",
    "\n",
    "Summarize dataset samples in a grid format:\n",
    "\n",
    "```python\n",
    "!run-visualize create-grid \\\n",
    "    --input-folder \"datasets/processed_images\" \\\n",
    "    --output-folder \"presentation/dataset_samples\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Reflection and Future Work 🔄\n",
    "\n",
    "This project demonstrates the effectiveness of modular pipelines for scalable machine learning workflows. Key insights include:\n",
    "\n",
    "- **Data Quality**: High-quality, annotated datasets significantly enhance model performance.\n",
    "- **Modularity**: Breaking down the pipeline into distinct stages improves reproducibility.\n",
    "- **Visualization**: Effective visual tools aid in debugging and communicating results.\n",
    "\n",
    "#### Future Improvements:\n",
    "1. Expanding the dataset to include more LEGO pieces.\n",
    "2. Integrating semi-automated annotation tools.\n",
    "3. Developing an interactive interface for real-time predictions.\n",
    "\n",
    "---\n",
    "\n",
    "For more details, refer to the [LEGO Bricks ML Vision Documentation](https://github.com/MiguelDiLalla/LEGO_Bricks_ML_Vision).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook saved to: C:\\Users\\User\\Projects_Unprotected\\LEGO_Bricks_ML_Vision\\notebooks\\Technical_Report.ipynb\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nbformat as nbf\n",
    "\n",
    "# Paths for input and output\n",
    "input_md_path = r\"C:\\Users\\User\\Projects_Unprotected\\LEGO_Bricks_ML_Vision\\notebooks\\Technical_Report.md\"\n",
    "output_nb_path = r\"C:\\Users\\User\\Projects_Unprotected\\LEGO_Bricks_ML_Vision\\notebooks\\Technical_Report.ipynb\"\n",
    "\n",
    "# Read the markdown file\n",
    "with open(input_md_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Initialize a new notebook\n",
    "notebook = nbf.v4.new_notebook()\n",
    "\n",
    "# Process the markdown file line by line\n",
    "current_cell_type = \"markdown\"\n",
    "current_cell_content = []\n",
    "\n",
    "for line in lines:\n",
    "    # Detect code block start or end\n",
    "    if line.strip().startswith(\"```\"):  # Toggle between markdown and code\n",
    "        if current_cell_type == \"code\":\n",
    "            # End of a code cell\n",
    "            notebook.cells.append(nbf.v4.new_code_cell(\"\".join(current_cell_content)))\n",
    "            current_cell_content = []\n",
    "            current_cell_type = \"markdown\"\n",
    "        else:\n",
    "            # End of a markdown cell\n",
    "            if current_cell_content:\n",
    "                notebook.cells.append(nbf.v4.new_markdown_cell(\"\".join(current_cell_content)))\n",
    "            current_cell_content = []\n",
    "            current_cell_type = \"code\"\n",
    "    else:\n",
    "        # Append content to the current cell\n",
    "        current_cell_content.append(line)\n",
    "\n",
    "# Add any remaining content as a final cell\n",
    "if current_cell_content:\n",
    "    if current_cell_type == \"markdown\":\n",
    "        notebook.cells.append(nbf.v4.new_markdown_cell(\"\".join(current_cell_content)))\n",
    "    elif current_cell_type == \"code\":\n",
    "        notebook.cells.append(nbf.v4.new_code_cell(\"\".join(current_cell_content)))\n",
    "\n",
    "# Save the notebook\n",
    "nbf.write(notebook, output_nb_path)\n",
    "print(f\"Notebook saved to: {output_nb_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to publish on PyPl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Creating isolated environment: venv+pip...\n",
      "* Installing packages in isolated environment:\n",
      "  - setuptools>=64\n",
      "  - wheel\n",
      "* Getting build dependencies for sdist...\n",
      "running egg_info\n",
      "writing lego_bricks_ml_vision.egg-info\\PKG-INFO\n",
      "writing dependency_links to lego_bricks_ml_vision.egg-info\\dependency_links.txt\n",
      "writing entry points to lego_bricks_ml_vision.egg-info\\entry_points.txt\n",
      "writing requirements to lego_bricks_ml_vision.egg-info\\requires.txt\n",
      "writing top-level names to lego_bricks_ml_vision.egg-info\\top_level.txt\n",
      "reading manifest file 'lego_bricks_ml_vision.egg-info\\SOURCES.txt'\n",
      "reading manifest template 'MANIFEST.in'\n",
      "adding license file 'LICENSE'\n",
      "writing manifest file 'lego_bricks_ml_vision.egg-info\\SOURCES.txt'\n",
      "* Building sdist...\n",
      "running sdist\n",
      "running egg_info\n",
      "writing lego_bricks_ml_vision.egg-info\\PKG-INFO\n",
      "writing dependency_links to lego_bricks_ml_vision.egg-info\\dependency_links.txt\n",
      "writing entry points to lego_bricks_ml_vision.egg-info\\entry_points.txt\n",
      "writing requirements to lego_bricks_ml_vision.egg-info\\requires.txt\n",
      "writing top-level names to lego_bricks_ml_vision.egg-info\\top_level.txt\n",
      "reading manifest file 'lego_bricks_ml_vision.egg-info\\SOURCES.txt'\n",
      "reading manifest template 'MANIFEST.in'\n",
      "adding license file 'LICENSE'\n",
      "writing manifest file 'lego_bricks_ml_vision.egg-info\\SOURCES.txt'\n",
      "running check\n",
      "creating lego_bricks_ml_vision-0.1.31\n",
      "creating lego_bricks_ml_vision-0.1.31\\lego_bricks_ml_vision.egg-info\n",
      "creating lego_bricks_ml_vision-0.1.31\\scripts\n",
      "copying files to lego_bricks_ml_vision-0.1.31...\n",
      "copying LICENSE -> lego_bricks_ml_vision-0.1.31\n",
      "copying MANIFEST.in -> lego_bricks_ml_vision-0.1.31\n",
      "copying README.md -> lego_bricks_ml_vision-0.1.31\n",
      "copying pyproject.toml -> lego_bricks_ml_vision-0.1.31\n",
      "copying requirements.txt -> lego_bricks_ml_vision-0.1.31\n",
      "copying setup.py -> lego_bricks_ml_vision-0.1.31\n",
      "copying lego_bricks_ml_vision.egg-info\\PKG-INFO -> lego_bricks_ml_vision-0.1.31\\lego_bricks_ml_vision.egg-info\n",
      "copying lego_bricks_ml_vision.egg-info\\SOURCES.txt -> lego_bricks_ml_vision-0.1.31\\lego_bricks_ml_vision.egg-info\n",
      "copying lego_bricks_ml_vision.egg-info\\dependency_links.txt -> lego_bricks_ml_vision-0.1.31\\lego_bricks_ml_vision.egg-info\n",
      "copying lego_bricks_ml_vision.egg-info\\entry_points.txt -> lego_bricks_ml_vision-0.1.31\\lego_bricks_ml_vision.egg-info\n",
      "copying lego_bricks_ml_vision.egg-info\\requires.txt -> lego_bricks_ml_vision-0.1.31\\lego_bricks_ml_vision.egg-info\n",
      "copying lego_bricks_ml_vision.egg-info\\top_level.txt -> lego_bricks_ml_vision-0.1.31\\lego_bricks_ml_vision.egg-info\n",
      "copying scripts\\__init__.py -> lego_bricks_ml_vision-0.1.31\\scripts\n",
      "copying scripts\\pipeline.py -> lego_bricks_ml_vision-0.1.31\\scripts\n",
      "copying scripts\\visualize_presentation.py -> lego_bricks_ml_vision-0.1.31\\scripts\n",
      "copying lego_bricks_ml_vision.egg-info\\SOURCES.txt -> lego_bricks_ml_vision-0.1.31\\lego_bricks_ml_vision.egg-info\n",
      "Writing lego_bricks_ml_vision-0.1.31\\setup.cfg\n",
      "Creating tar archive\n",
      "removing 'lego_bricks_ml_vision-0.1.31' (and everything under it)\n",
      "* Building wheel from sdist\n",
      "* Creating isolated environment: venv+pip...\n",
      "* Installing packages in isolated environment:\n",
      "  - setuptools>=64\n",
      "  - wheel\n",
      "* Getting build dependencies for wheel...\n",
      "running egg_info\n",
      "writing lego_bricks_ml_vision.egg-info\\PKG-INFO\n",
      "writing dependency_links to lego_bricks_ml_vision.egg-info\\dependency_links.txt\n",
      "writing entry points to lego_bricks_ml_vision.egg-info\\entry_points.txt\n",
      "writing requirements to lego_bricks_ml_vision.egg-info\\requires.txt\n",
      "writing top-level names to lego_bricks_ml_vision.egg-info\\top_level.txt\n",
      "reading manifest file 'lego_bricks_ml_vision.egg-info\\SOURCES.txt'\n",
      "reading manifest template 'MANIFEST.in'\n",
      "adding license file 'LICENSE'\n",
      "writing manifest file 'lego_bricks_ml_vision.egg-info\\SOURCES.txt'\n",
      "* Building wheel...\n",
      "running bdist_wheel\n",
      "running build\n",
      "running build_py\n",
      "creating build\\lib\\scripts\n",
      "copying scripts\\pipeline.py -> build\\lib\\scripts\n",
      "copying scripts\\visualize_presentation.py -> build\\lib\\scripts\n",
      "copying scripts\\__init__.py -> build\\lib\\scripts\n",
      "installing to build\\bdist.win-amd64\\wheel\n",
      "running install\n",
      "running install_lib\n",
      "creating build\\bdist.win-amd64\\wheel\n",
      "creating build\\bdist.win-amd64\\wheel\\scripts\n",
      "copying build\\lib\\scripts\\pipeline.py -> build\\bdist.win-amd64\\wheel\\.\\scripts\n",
      "copying build\\lib\\scripts\\visualize_presentation.py -> build\\bdist.win-amd64\\wheel\\.\\scripts\n",
      "copying build\\lib\\scripts\\__init__.py -> build\\bdist.win-amd64\\wheel\\.\\scripts\n",
      "running install_egg_info\n",
      "running egg_info\n",
      "writing lego_bricks_ml_vision.egg-info\\PKG-INFO\n",
      "writing dependency_links to lego_bricks_ml_vision.egg-info\\dependency_links.txt\n",
      "writing entry points to lego_bricks_ml_vision.egg-info\\entry_points.txt\n",
      "writing requirements to lego_bricks_ml_vision.egg-info\\requires.txt\n",
      "writing top-level names to lego_bricks_ml_vision.egg-info\\top_level.txt\n",
      "reading manifest file 'lego_bricks_ml_vision.egg-info\\SOURCES.txt'\n",
      "reading manifest template 'MANIFEST.in'\n",
      "adding license file 'LICENSE'\n",
      "writing manifest file 'lego_bricks_ml_vision.egg-info\\SOURCES.txt'\n",
      "Copying lego_bricks_ml_vision.egg-info to build\\bdist.win-amd64\\wheel\\.\\lego_bricks_ml_vision-0.1.31-py3.12.egg-info\n",
      "running install_scripts\n",
      "creating build\\bdist.win-amd64\\wheel\\lego_bricks_ml_vision-0.1.31.dist-info\\WHEEL\n",
      "creating 'c:\\Users\\User\\Projects_Unprotected\\LEGO_Bricks_ML_Vision\\dist\\.tmp-qag61j__\\lego_bricks_ml_vision-0.1.31-py3-none-any.whl' and adding 'build\\bdist.win-amd64\\wheel' to it\n",
      "adding 'scripts/__init__.py'\n",
      "adding 'scripts/pipeline.py'\n",
      "adding 'scripts/visualize_presentation.py'\n",
      "adding 'lego_bricks_ml_vision-0.1.31.dist-info/LICENSE'\n",
      "adding 'lego_bricks_ml_vision-0.1.31.dist-info/METADATA'\n",
      "adding 'lego_bricks_ml_vision-0.1.31.dist-info/WHEEL'\n",
      "adding 'lego_bricks_ml_vision-0.1.31.dist-info/entry_points.txt'\n",
      "adding 'lego_bricks_ml_vision-0.1.31.dist-info/top_level.txt'\n",
      "adding 'lego_bricks_ml_vision-0.1.31.dist-info/RECORD'\n",
      "removing build\\bdist.win-amd64\\wheel\n",
      "Successfully built lego_bricks_ml_vision-0.1.31.tar.gz and lego_bricks_ml_vision-0.1.31-py3-none-any.whl\n"
     ]
    }
   ],
   "source": [
    "!python -m build\n",
    "\n",
    "!python -m twine upload dist/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pipeline_setup.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Detected Environment': 'local'}\n",
      "\n",
      "[INFO] Entorno detectado: Local\n",
      "\n",
      "{'Rutas Configuradas': {'output_path': 'c:\\\\Users\\\\User\\\\Projects_Unprotected\\\\LEGO_Bricks_ML_Vision\\\\notebooks\\\\working\\\\output',\n",
      "                        'raw_images_path': 'working/spiled-lego-bricks\\\\Images_600x800',\n",
      "                        'raw_labels_path': 'working/spiled-lego-bricks\\\\LabelMe_txt_bricks'}}\n",
      "{'Dataset Estructura': {'working/spiled-lego-bricks\\\\Images_600x800': 1803,\n",
      "                        'working/spiled-lego-bricks\\\\LabelMe_txt_bricks': 1803}}\n",
      "[INFO] Estructura de carpetas creada en c:\\Users\\User\\Projects_Unprotected\\LEGO_Bricks_ML_Vision\\notebooks\\working\\output.\n",
      "{'Partición Completada': {'test': 179, 'train': 1262, 'val': 362}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\.conda\\envs\\MiguelEnvHaB\\Lib\\site-packages\\albumentations\\core\\validation.py:45: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
      "  original_init(self, **validated_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Augmented data saved to c:\\Users\\User\\Projects_Unprotected\\LEGO_Bricks_ML_Vision\\notebooks\\working\\output\\augmented_dataset.\n",
      "[INFO] Augmented data merged into train set at c:\\Users\\User\\Projects_Unprotected\\LEGO_Bricks_ML_Vision\\notebooks\\working\\output.\n",
      "[INFO] dataset.yaml created at: c:\\Users\\User\\Projects_Unprotected\\LEGO_Bricks_ML_Vision\\notebooks\\working\\output\\dataset\\dataset.yaml\n",
      "{'Validación Final': {'test': 179, 'train': 5048, 'val': 362}}\n",
      "\n",
      "[INFO] Pipeline setup completed with augmentations and dataset.yaml creation.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "from pprint import pprint\n",
    "import albumentations as A\n",
    "import cv2\n",
    "import yaml\n",
    "import sys\n",
    "\n",
    "# === Configuración Inicial ===\n",
    "def detect_environment():\n",
    "    \"\"\"\n",
    "    Detecta el entorno de ejecución (Kaggle, Google Colab o Local).\n",
    "\n",
    "    Returns:\n",
    "    - str: Nombre del entorno detectado.\n",
    "    \"\"\"\n",
    "    if \"google.colab\" in sys.modules:\n",
    "        environment = \"colab\"\n",
    "    elif os.path.exists(\"/kaggle\"):\n",
    "        environment = \"kaggle\"\n",
    "    else:\n",
    "        environment = \"local\"\n",
    "    pprint({\"Detected Environment\": environment})\n",
    "    return environment\n",
    "\n",
    "\n",
    "def setup_environment(base_path=\"/kaggle/working/output\"):\n",
    "    \"\"\"\n",
    "    Configura el entorno según el sistema detectado y prepara el dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - base_path (str): Carpeta base donde se configurará la salida.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Rutas configuradas para las imágenes y etiquetas crudas.\n",
    "    \"\"\"\n",
    "    environment = detect_environment()\n",
    "    print(f\"\\n[INFO] Entorno detectado: {environment.capitalize()}\\n\")\n",
    "\n",
    "    if environment == \"kaggle\":\n",
    "        dataset_path = \"/kaggle/input/spiled-lego-bricks\"\n",
    "        required_folders = [\"Images_600x800\", \"LabelMe_txt_bricks\"]\n",
    "        for folder in required_folders:\n",
    "            full_path = os.path.join(dataset_path, folder)\n",
    "            if not os.path.exists(full_path):\n",
    "                raise FileNotFoundError(f\"[ERROR] Carpeta requerida no encontrada: {full_path}\")\n",
    "            print(f\"[INFO] Carpeta verificada: {full_path}\")\n",
    "\n",
    "        return {\n",
    "            \"raw_images_path\": os.path.join(dataset_path, \"Images_600x800\"),\n",
    "            \"raw_labels_path\": os.path.join(dataset_path, \"LabelMe_txt_bricks\"),\n",
    "            \"output_path\": base_path\n",
    "        }\n",
    "    elif environment == \"colab\":\n",
    "            from google.colab import userdata\n",
    "            kaggle_path = \"kaggle.json\"\n",
    "            if not os.path.exists(kaggle_path):\n",
    "                # raise EnvironmentError(\"[ERROR] Sube tu archivo kaggle.json al entorno Colab en /root/.kaggle/\")\n",
    "                os.makedirs(\"/root/.kaggle\", exist_ok=True)\n",
    "            \n",
    "            \n",
    "                kaggle_user = userdata.get('KaggleUser')\n",
    "                kaggle_token = userdata.get('KaggleToken')\n",
    "                if not kaggle_user or not kaggle_token:\n",
    "                    raise EnvironmentError(\"[ERROR] No se encontraron las credenciales de Kaggle en Google Colab.\")\n",
    "                kaggle_data = {\n",
    "                    \"username\": kaggle_user,\n",
    "                    \"key\": kaggle_token\n",
    "                }\n",
    "                with open(\"/root/.kaggle/kaggle.json\", \"w\") as f:\n",
    "                    json.dump(kaggle_data, f)\n",
    "                    print(\"[INFO] Credenciales de Kaggle configuradas en Google Colab.\")\n",
    "            else:\n",
    "                os.makedirs(\"/root/.kaggle\", exist_ok=True)\n",
    "                shutil.move(kaggle_path, \"/root/.kaggle/kaggle.json\")\n",
    "                print(\"[INFO] Archivo kaggle.json movido a /root/.kaggle/\")\n",
    "            os.chmod(\"/root/.kaggle/kaggle.json\", 0o600)\n",
    "            os.makedirs(\"working\", exist_ok=True)\n",
    "            os.makedirs(\"working/spiled-lego-bricks\", exist_ok=True)\n",
    "            os.system(\"kaggle datasets download -d migueldilalla/spiled-lego-bricks -p working/spiled-lego-bricks --unzip\")\n",
    "            os.makedirs(\"/working/output\", exist_ok=True)\n",
    "            dataset_path = \"working/spiled-lego-bricks\"\n",
    "\n",
    "            return {\n",
    "                \"raw_images_path\": os.path.join(dataset_path, \"Images_600x800\"),\n",
    "                \"raw_labels_path\": os.path.join(dataset_path, \"LabelMe_txt_bricks\"),\n",
    "                \"output_path\": os.path.join(os.getcwd(), \"working\", \"output\")\n",
    "            }\n",
    "\n",
    "\n",
    "    elif environment == \"local\":\n",
    "        kaggle_json_path = os.path.expanduser(\"~/.kaggle/kaggle.json\")\n",
    "        if not os.path.exists(kaggle_json_path):\n",
    "            raise EnvironmentError(\"[ERROR] Archivo kaggle.json no encontrado en ~/.kaggle/\")\n",
    "        os.makedirs(\"working\", exist_ok=True)\n",
    "        os.makedirs(\"working/spiled-lego-bricks\", exist_ok=True)\n",
    "        if not os.listdir(\"working/spiled-lego-bricks\"):\n",
    "            os.system(\"kaggle datasets download -d migueldilalla/spiled-lego-bricks -p working/spiled-lego-bricks --unzip\")\n",
    "        os.makedirs(\"working/output\", exist_ok=True)\n",
    "        dataset_path = \"working/spiled-lego-bricks\"\n",
    "\n",
    "        return {\n",
    "             \"raw_images_path\": os.path.join(dataset_path, \"Images_600x800\"),\n",
    "            \"raw_labels_path\": os.path.join(dataset_path, \"LabelMe_txt_bricks\"),\n",
    "            \"output_path\": os.path.join(os.getcwd(), \"working\", \"output\")\n",
    "        }\n",
    "    else:\n",
    "        while True:\n",
    "            user_input = input(\"[PROMPT] No se detectó un entorno. Por favor, escribe 'k' para Kaggle, 'g' para Google Colab, o 'l' para Local: \").strip().lower()\n",
    "            if user_input in [\"k\", \"g\", \"l\"]:\n",
    "                return setup_environment_custom(user_input, base_path)\n",
    "            print(\"[ERROR] Entrada inválida. Intenta nuevamente.\")\n",
    "\n",
    "def setup_environment_custom(choice, base_path):\n",
    "    \"\"\"\n",
    "    Configura el entorno manualmente basado en la elección del usuario.\n",
    "\n",
    "    Parameters:\n",
    "    - choice (str): 'k' para Kaggle, 'g' para Colab, 'l' para Local.\n",
    "    - base_path (str): Ruta base para la salida.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Rutas configuradas para las imágenes y etiquetas crudas.\n",
    "    \"\"\"\n",
    "    if choice == \"k\":\n",
    "        return setup_environment()\n",
    "    elif choice == \"g\":\n",
    "        return setup_environment(base_path=\"working\")\n",
    "    elif choice == \"l\":\n",
    "        return setup_environment(base_path=\"working\")\n",
    "    else:\n",
    "        raise EnvironmentError(\"[ERROR] Configuración desconocida.\")\n",
    "\n",
    "def verify_dataset_structure(raw_images_path, raw_labels_path):\n",
    "    \"\"\"\n",
    "    Verifica la existencia de las carpetas requeridas en el dataset y muestra estadísticas iniciales.\n",
    "\n",
    "    Parameters:\n",
    "    - raw_images_path (str): Ruta a las imágenes crudas.\n",
    "    - raw_labels_path (str): Ruta a las etiquetas crudas.\n",
    "    \"\"\"\n",
    "    required_folders = [raw_images_path, raw_labels_path]\n",
    "    summary = {}\n",
    "    for folder in required_folders:\n",
    "        if not os.path.exists(folder):\n",
    "            raise FileNotFoundError(f\"[ERROR] Carpeta requerida no encontrada: {folder}\")\n",
    "\n",
    "        num_files = len([f for f in os.listdir(folder) if os.path.isfile(os.path.join(folder, f))])\n",
    "        if num_files == 0:\n",
    "            raise ValueError(f\"[ERROR] La carpeta {folder} está vacía.\")\n",
    "        summary[folder] = num_files\n",
    "\n",
    "    pprint({\"Dataset Estructura\": summary})\n",
    "\n",
    "def create_preprocessing_structure(output_dir=\"/kaggle/working/output\"):\n",
    "    \"\"\"\n",
    "    Crea la estructura de carpetas para PREPROCESSING/.\n",
    "\n",
    "    Parameters:\n",
    "    - output_dir (str): Ruta base para la carpeta PREPROCESSING/.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    subfolders = [\n",
    "        \"dataset/images/train\", \"dataset/images/val\", \"dataset/images/test\",\n",
    "        \"dataset/labels/train\", \"dataset/labels/val\", \"dataset/labels/test\",\n",
    "        \"test_images\"\n",
    "    ]\n",
    "    for subfolder in subfolders:\n",
    "        os.makedirs(os.path.join(output_dir, subfolder), exist_ok=True)\n",
    "    print(f\"[INFO] Estructura de carpetas creada en {output_dir}.\")\n",
    "\n",
    "def copy_and_partition_data(input_images, input_labels, output_dir):\n",
    "    \"\"\"\n",
    "    Copia imágenes y etiquetas a las carpetas correspondientes y realiza la partición de datos.\n",
    "\n",
    "    Parameters:\n",
    "    - input_images (str): Carpeta de imágenes de entrada.\n",
    "    - input_labels (str): Carpeta de etiquetas de entrada.\n",
    "    - output_dir (str): Carpeta base para PREPROCESSING/.\n",
    "    \"\"\"\n",
    "    images = sorted([f for f in os.listdir(input_images) if f.endswith(\".jpg\")])\n",
    "    labels = sorted([f for f in os.listdir(input_labels) if f.endswith(\".txt\")])\n",
    "\n",
    "    if len(images) != len(labels):\n",
    "        raise ValueError(\"[ERROR] Número de imágenes y etiquetas no coincide.\")\n",
    "\n",
    "    image_paths = [os.path.join(input_images, img) for img in images]\n",
    "    label_paths = [os.path.join(input_labels, lbl) for lbl in labels]\n",
    "\n",
    "    train_imgs, temp_imgs, train_lbls, temp_lbls = train_test_split(image_paths, label_paths, test_size=0.3, random_state=42)\n",
    "    val_imgs, test_imgs, val_lbls, test_lbls = train_test_split(temp_imgs, temp_lbls, test_size=0.33, random_state=42)\n",
    "\n",
    "    partitions = {\n",
    "        \"train\": (train_imgs, train_lbls),\n",
    "        \"val\": (val_imgs, val_lbls),\n",
    "        \"test\": (test_imgs, test_lbls)\n",
    "    }\n",
    "\n",
    "    for partition, (imgs, lbls) in partitions.items():\n",
    "        for img, lbl in zip(imgs, lbls):\n",
    "            shutil.copy(img, os.path.join(output_dir, f\"dataset/images/{partition}/\"))\n",
    "            shutil.copy(lbl, os.path.join(output_dir, f\"dataset/labels/{partition}/\"))\n",
    "\n",
    "    pprint({\"Partición Completada\": {partition: len(imgs) for partition, (imgs, _) in partitions.items()}})\n",
    "\n",
    "def augment_data(input_images, input_labels, output_dir, num_augmentations=2):\n",
    "    \"\"\"\n",
    "    Aplica aumentaciones al dataset y guarda imágenes y etiquetas aumentadas.\n",
    "\n",
    "    Parameters:\n",
    "    - input_images (str): Carpeta de imágenes originales.\n",
    "    - input_labels (str): Carpeta de etiquetas en formato YOLO.\n",
    "    - output_dir (str): Carpeta donde se guardarán los datos aumentados.\n",
    "    - num_augmentations (int): Número de versiones aumentadas por imagen.\n",
    "    \"\"\"\n",
    "    aug_images_dir = os.path.join(output_dir, \"augmented_images\")\n",
    "    aug_labels_dir = os.path.join(output_dir, \"augmented_labels\")\n",
    "    os.makedirs(aug_images_dir, exist_ok=True)\n",
    "    os.makedirs(aug_labels_dir, exist_ok=True)\n",
    "\n",
    "    transform = A.Compose([\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.RandomBrightnessContrast(p=0.2),\n",
    "        A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),\n",
    "        A.Resize(height=640, width=640),\n",
    "    ], bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels']))\n",
    "\n",
    "    images = sorted([f for f in os.listdir(input_images) if f.endswith(\".jpg\")])\n",
    "    for img_file in images:\n",
    "        img_path = os.path.join(input_images, img_file)\n",
    "        label_path = os.path.join(input_labels, img_file.replace(\".jpg\", \".txt\"))\n",
    "\n",
    "        if not os.path.exists(label_path):\n",
    "            continue\n",
    "\n",
    "        image = cv2.imread(img_path)\n",
    "        bboxes, class_labels = load_labels(label_path)\n",
    "\n",
    "        for i in range(num_augmentations):\n",
    "            augmented = transform(image=image, bboxes=bboxes, class_labels=class_labels)\n",
    "            aug_image = augmented[\"image\"]\n",
    "            aug_bboxes = augmented[\"bboxes\"]\n",
    "            aug_labels = augmented[\"class_labels\"]\n",
    "\n",
    "            aug_image_path = os.path.join(aug_images_dir, f\"{img_file.split('.')[0]}_aug{i}.jpg\")\n",
    "            cv2.imwrite(aug_image_path, aug_image)\n",
    "\n",
    "            aug_label_path = os.path.join(aug_labels_dir, f\"{img_file.split('.')[0]}_aug{i}.txt\")\n",
    "            save_labels(aug_label_path, aug_bboxes, aug_labels)\n",
    "\n",
    "    print(f\"[INFO] Augmented data saved to {output_dir}.\")\n",
    "\n",
    "def load_labels(label_path):\n",
    "    \"\"\"\n",
    "    Carga etiquetas en formato YOLO desde un archivo .txt.\n",
    "\n",
    "    Parameters:\n",
    "    - label_path (str): Ruta al archivo de etiquetas en formato YOLO.\n",
    "\n",
    "    Returns:\n",
    "    - bboxes (list): Lista de bounding boxes en formato YOLO.\n",
    "    - class_labels (list): Lista de etiquetas de clase.\n",
    "    \"\"\"\n",
    "    bboxes, class_labels = [], []\n",
    "    with open(label_path, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        class_id, x_center, y_center, width, height = map(float, line.strip().split())\n",
    "        bboxes.append([x_center, y_center, width, height])\n",
    "        class_labels.append(int(class_id))\n",
    "    return bboxes, class_labels\n",
    "\n",
    "def save_labels(output_path, bboxes, class_labels):\n",
    "    \"\"\"\n",
    "    Guarda etiquetas en formato YOLO en un archivo .txt.\n",
    "\n",
    "    Parameters:\n",
    "    - output_path (str): Ruta donde se guardará el archivo de etiquetas.\n",
    "    - bboxes (list): Lista de bounding boxes en formato YOLO.\n",
    "    - class_labels (list): Lista de etiquetas de clase.\n",
    "    \"\"\"\n",
    "    with open(output_path, \"w\") as f:\n",
    "        for bbox, label in zip(bboxes, class_labels):\n",
    "            f.write(f\"{label} {' '.join(map(str, bbox))}\\n\")\n",
    "\n",
    "\n",
    "def copy_augmented_to_train(augmented_dir, output_path):\n",
    "    \"\"\"\n",
    "    Copia los datos aumentados a las subcarpetas correspondientes de 'train'.\n",
    "\n",
    "    Parameters:\n",
    "    - augmented_dir (str): Directorio que contiene imágenes y etiquetas aumentadas.\n",
    "    - output_path(str): Ruta base para la salida.\n",
    "    \"\"\"\n",
    "    aug_images_dir = os.path.join(augmented_dir, \"augmented_images\")\n",
    "    aug_labels_dir = os.path.join(augmented_dir, \"augmented_labels\")\n",
    "    train_images_dir = os.path.join(output_path, \"dataset/images/train\")\n",
    "    train_labels_dir = os.path.join(output_path, \"dataset/labels/train\")\n",
    "\n",
    "    for img_file in os.listdir(aug_images_dir):\n",
    "        shutil.copy(os.path.join(aug_images_dir, img_file), train_images_dir)\n",
    "\n",
    "    for label_file in os.listdir(aug_labels_dir):\n",
    "        shutil.copy(os.path.join(aug_labels_dir, label_file), train_labels_dir)\n",
    "\n",
    "    print(f\"[INFO] Augmented data merged into train set at {output_path}.\")\n",
    "\n",
    "def create_dataset_yaml(output_path, num_classes, class_names):\n",
    "    \"\"\"\n",
    "    Creates a dataset.yaml file with absolute paths for YOLO training.\n",
    "\n",
    "    Parameters:\n",
    "    - output_path (str): Base directory where the dataset.yaml file will be saved.\n",
    "    - num_classes (int): Total number of classes.\n",
    "    - class_names (list): List of class names.\n",
    "    \"\"\"\n",
    "    # Resolve absolute paths for train and val folders\n",
    "    dataset_dir = os.path.abspath(output_path)\n",
    "    train_path = os.path.join(dataset_dir, \"images/train\")\n",
    "    val_path = os.path.join(dataset_dir, \"images/val\")\n",
    "\n",
    "    # Create the dataset configuration dictionary\n",
    "    dataset_config = {\n",
    "        \"path\": dataset_dir,\n",
    "        \"train\": train_path,\n",
    "        \"val\": val_path,\n",
    "        \"nc\": num_classes,\n",
    "        \"names\": {i: name for i, name in enumerate(class_names)}\n",
    "    }\n",
    "\n",
    "    # Save the configuration to the dataset.yaml file\n",
    "    yaml_path = os.path.join(dataset_dir, \"dataset.yaml\")\n",
    "    with open(yaml_path, \"w\") as f:\n",
    "        yaml.dump(dataset_config, f, default_flow_style=False)\n",
    "    \n",
    "    print(f\"[INFO] dataset.yaml created at: {yaml_path}\")\n",
    "\n",
    "def validate_final_structure(output_dir=\"/kaggle/working/output\"):\n",
    "    \"\"\"\n",
    "    Valida que las carpetas de imágenes y etiquetas contengan archivos coincidentes.\n",
    "\n",
    "    Parameters:\n",
    "    - output_dir (str): Carpeta base para PREPROCESSING/.\n",
    "    \"\"\"\n",
    "    partitions = [\"train\", \"val\", \"test\"]\n",
    "    summary = {}\n",
    "\n",
    "    # flag = True\n",
    "\n",
    "    for partition in partitions:\n",
    "        images = sorted(os.listdir(os.path.join(output_dir, f\"dataset/images/{partition}/\")))\n",
    "        labels = sorted(os.listdir(os.path.join(output_dir, f\"dataset/labels/{partition}/\")))\n",
    "\n",
    "        \n",
    "        # if flag:\n",
    "        #     print(output_dir, f\"dataset/images/{partition}/\")\n",
    "        #     flag = False\n",
    "        #     #open the folder in file explorer\n",
    "        #     os.system(f\"explorer {os.path.join(output_dir, f'dataset/images/{partition}/').replace('/', '\\\\')}\")\n",
    "        \n",
    "        if len(images) != len(labels):\n",
    "            raise ValueError(f\"[ERROR] Desbalance entre imágenes y etiquetas en {partition}.\")\n",
    "        summary[partition] = len(images)\n",
    "    \n",
    "    pprint({\"Validación Final\": summary})\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Ejecución principal del pipeline.\n",
    "    \"\"\"\n",
    "    paths = setup_environment()\n",
    "    pprint({\"Rutas Configuradas\": paths})\n",
    "\n",
    "    verify_dataset_structure(paths[\"raw_images_path\"], paths[\"raw_labels_path\"])\n",
    "\n",
    "    create_preprocessing_structure(paths[\"output_path\"])\n",
    "\n",
    "    copy_and_partition_data(paths[\"raw_images_path\"], paths[\"raw_labels_path\"], paths[\"output_path\"])\n",
    "\n",
    "    augment_data(\n",
    "        input_images=os.path.join(paths[\"output_path\"], \"dataset/images/train\"),\n",
    "        input_labels=os.path.join(paths[\"output_path\"], \"dataset/labels/train\"),\n",
    "        output_dir=os.path.join(paths[\"output_path\"], \"augmented_dataset\"),\n",
    "        num_augmentations=3\n",
    "    )\n",
    "\n",
    "    copy_augmented_to_train(\n",
    "        augmented_dir=os.path.join(paths[\"output_path\"], \"augmented_dataset\"),\n",
    "        output_path=paths[\"output_path\"]\n",
    "    )\n",
    "\n",
    "    create_dataset_yaml(\n",
    "        output_path=os.path.join(paths[\"output_path\"], \"dataset\"),\n",
    "        num_classes=1,  # Replace with the actual number of classes\n",
    "        class_names=[\"brick\"]  # Add all class names here\n",
    "    )\n",
    "\n",
    "    validate_final_structure(paths[\"output_path\"])\n",
    "    print(\"\\n[INFO] Pipeline setup completed with augmentations and dataset.yaml creation.\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pipeline_train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-22 12:31:26,015 - INFO - [INFO] Usando dataset.yaml en: c:\\Users\\User\\Projects_Unprotected\\LEGO_Bricks_ML_Vision\\notebooks\\working\\output\\dataset\\dataset.yaml\n",
      "2025-01-22 12:31:26,081 - INFO - [INFO] Iniciando entrenamiento regular...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.65 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.2.99  Python-3.12.7 torch-2.4.1+cpu CPU (Intel Core(TM) i5-8265U 1.60GHz)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=c:\\Users\\User\\Projects_Unprotected\\LEGO_Bricks_ML_Vision\\notebooks\\working\\output\\dataset\\dataset.yaml, epochs=50, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=cpu, workers=8, project=regular_yolo_training/20250122_123126, name=train, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.001, lrf=0.01, momentum=0.9, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=regular_yolo_training\\20250122_123126\\train\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    751507  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
      "Model summary: 225 layers, 3,011,043 parameters, 3,011,027 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\User\\Projects_Unprotected\\LEGO_Bricks_ML_Vision\\notebooks\\working\\output\\dataset\\labels\\train.cache... 5048 images, 0 backgrounds, 0 corrupt: 100%|██████████| 5048/5048 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\.conda\\envs\\MiguelEnvHaB\\Lib\\site-packages\\ultralytics\\data\\augment.py:1837: UserWarning: Argument 'quality_lower' is not valid and will be ignored.\n",
      "  A.ImageCompression(quality_lower=75, p=0.0),\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\User\\Projects_Unprotected\\LEGO_Bricks_ML_Vision\\notebooks\\working\\output\\dataset\\labels\\val.cache... 362 images, 0 backgrounds, 0 corrupt: 100%|██████████| 362/362 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to regular_yolo_training\\20250122_123126\\train\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.001' and 'momentum=0.9' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progreso del entrenamiento:   0%|          | 0/50 [00:00<?, ?época/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mregular_yolo_training\\20250122_123126\\train\u001b[0m\n",
      "Starting training for 50 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/50         0G      1.507      2.313      1.207         64        640:  14%|█▎        | 43/316 [07:26<47:16, 10.39s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 151\u001b[0m\n\u001b[0;32m    148\u001b[0m         train_model(dataset_yaml, imgsz\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m640\u001b[39m)  \u001b[38;5;66;03m# Tamaño de imagen predeterminado\u001b[39;00m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 151\u001b[0m     main(optuna_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[5], line 148\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(optuna_mode)\u001b[0m\n\u001b[0;32m    146\u001b[0m     run_optuna_study(dataset_yaml, n_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 148\u001b[0m     train_model(dataset_yaml, imgsz\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m640\u001b[39m)\n",
      "Cell \u001b[1;32mIn[5], line 120\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(dataset_yaml, pretrained_model, epochs, batch_size, learning_rate, momentum, imgsz)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    119\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[INFO] Iniciando entrenamiento regular...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 120\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain(\n\u001b[0;32m    121\u001b[0m         data\u001b[38;5;241m=\u001b[39mdataset_yaml,\n\u001b[0;32m    122\u001b[0m         epochs\u001b[38;5;241m=\u001b[39mepochs,\n\u001b[0;32m    123\u001b[0m         batch\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m    124\u001b[0m         imgsz\u001b[38;5;241m=\u001b[39mimgsz,\n\u001b[0;32m    125\u001b[0m         lr0\u001b[38;5;241m=\u001b[39mlearning_rate,\n\u001b[0;32m    126\u001b[0m         momentum\u001b[38;5;241m=\u001b[39mmomentum,\n\u001b[0;32m    127\u001b[0m         project\u001b[38;5;241m=\u001b[39moutput_dir,\n\u001b[0;32m    128\u001b[0m         name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    129\u001b[0m         device\u001b[38;5;241m=\u001b[39mget_device()\n\u001b[0;32m    130\u001b[0m     )\n\u001b[0;32m    131\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[INFO] Entrenamiento completado. Resultados guardados en \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\User\\.conda\\envs\\MiguelEnvHaB\\Lib\\site-packages\\ultralytics\\engine\\model.py:803\u001b[0m, in \u001b[0;36mModel.train\u001b[1;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[0;32m    800\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mmodel\n\u001b[0;32m    802\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mhub_session \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession  \u001b[38;5;66;03m# attach optional HUB session\u001b[39;00m\n\u001b[1;32m--> 803\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m    804\u001b[0m \u001b[38;5;66;03m# Update model and cfg after training\u001b[39;00m\n\u001b[0;32m    805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m}:\n",
      "File \u001b[1;32mc:\\Users\\User\\.conda\\envs\\MiguelEnvHaB\\Lib\\site-packages\\ultralytics\\engine\\trainer.py:207\u001b[0m, in \u001b[0;36mBaseTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    204\u001b[0m         ddp_cleanup(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mstr\u001b[39m(file))\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 207\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_train(world_size)\n",
      "File \u001b[1;32mc:\\Users\\User\\.conda\\envs\\MiguelEnvHaB\\Lib\\site-packages\\ultralytics\\engine\\trainer.py:367\u001b[0m, in \u001b[0;36mBaseTrainer._do_train\u001b[1;34m(self, world_size)\u001b[0m\n\u001b[0;32m    365\u001b[0m     pbar \u001b[38;5;241m=\u001b[39m TQDM(\u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loader), total\u001b[38;5;241m=\u001b[39mnb)\n\u001b[0;32m    366\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtloss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 367\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[0;32m    368\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_callbacks(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_train_batch_start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    369\u001b[0m     \u001b[38;5;66;03m# Warmup\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\.conda\\envs\\MiguelEnvHaB\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\.conda\\envs\\MiguelEnvHaB\\Lib\\site-packages\\ultralytics\\data\\build.py:48\u001b[0m, in \u001b[0;36mInfiniteDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Creates a sampler that repeats indefinitely.\"\"\"\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m)):\n\u001b[1;32m---> 48\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator)\n",
      "File \u001b[1;32mc:\\Users\\User\\.conda\\envs\\MiguelEnvHaB\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\User\\.conda\\envs\\MiguelEnvHaB\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\User\\.conda\\envs\\MiguelEnvHaB\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\User\\.conda\\envs\\MiguelEnvHaB\\Lib\\site-packages\\ultralytics\\data\\base.py:256\u001b[0m, in \u001b[0;36mBaseDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[0;32m    255\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns transformed label information for given index.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 256\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_image_and_label(index))\n",
      "File \u001b[1;32mc:\\Users\\User\\.conda\\envs\\MiguelEnvHaB\\Lib\\site-packages\\ultralytics\\data\\augment.py:201\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;124;03mApplies a series of transformations to input data. This method sequentially applies each transformation in the\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;124;03mCompose object's list of transforms to the input data.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;124;03m    >>> transformed_data = compose(input_data)\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m--> 201\u001b[0m     data \u001b[38;5;241m=\u001b[39m t(data)\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\User\\.conda\\envs\\MiguelEnvHaB\\Lib\\site-packages\\ultralytics\\data\\augment.py:1368\u001b[0m, in \u001b[0;36mRandomHSV.__call__\u001b[1;34m(self, labels)\u001b[0m\n\u001b[0;32m   1366\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhgain \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msgain \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvgain:\n\u001b[0;32m   1367\u001b[0m     r \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m) \u001b[38;5;241m*\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhgain, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msgain, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvgain] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# random gains\u001b[39;00m\n\u001b[1;32m-> 1368\u001b[0m     hue, sat, val \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39msplit(cv2\u001b[38;5;241m.\u001b[39mcvtColor(img, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2HSV))\n\u001b[0;32m   1369\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mdtype  \u001b[38;5;66;03m# uint8\u001b[39;00m\n\u001b[0;32m   1371\u001b[0m     x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m256\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mr\u001b[38;5;241m.\u001b[39mdtype)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import optuna\n",
    "from ultralytics import YOLO\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "\n",
    "EPOCAS = 4\n",
    "\n",
    "# === Configuración del Logger ===\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# === Detección del dispositivo ===\n",
    "def get_device():\n",
    "    \"\"\"\n",
    "    Detecta el dispositivo adecuado para la ejecución.\n",
    "\n",
    "    Returns:\n",
    "    - str: Dispositivo a usar (\"cpu\", \"0\", \"0,1\").\n",
    "    \"\"\"\n",
    "    if os.environ.get('COLAB_GPU') is not None:\n",
    "        return \"0\"  # Colab\n",
    "    elif os.path.exists(\"/kaggle\"):  # Kaggle\n",
    "        return \"0,1\"\n",
    "    else:\n",
    "        return \"cpu\"  # Local\n",
    "\n",
    "# === Callback personalizado para barra de progreso ===\n",
    "class ProgressBarCallback:\n",
    "    def __init__(self, total_epochs):\n",
    "        self.total_epochs = total_epochs\n",
    "        self.pbar = None\n",
    "\n",
    "    def on_train_start(self, trainer, **kwargs):\n",
    "        # Inicializar barra de progreso\n",
    "        self.pbar = tqdm(total=self.total_epochs, desc=\"Progreso del entrenamiento\", unit=\"época\")\n",
    "\n",
    "    def on_epoch_end(self, trainer, **kwargs):\n",
    "        # Actualizar barra de progreso al final de cada época\n",
    "        self.pbar.update(1)\n",
    "        self.pbar.set_postfix({\"Última época\": kwargs.get('epoch') + 1})\n",
    "\n",
    "    def on_train_end(self, trainer, **kwargs):\n",
    "        # Cerrar barra de progreso\n",
    "        self.pbar.close()\n",
    "\n",
    "# === Configuración de la Función Objetivo de Optuna ===\n",
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Función objetivo para Optuna que entrena el modelo YOLO utilizando hiperparámetros sugeridos.\n",
    "\n",
    "    Returns:\n",
    "    - mAP50 (float): Precisión media a IoU 0.5, métrica a optimizar.\n",
    "    \"\"\"\n",
    "    # Definir espacio de búsqueda para hiperparámetros\n",
    "    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-2)\n",
    "    batch_size = trial.suggest_int(\"batch_size\", 8, 32, step=8)\n",
    "    momentum = trial.suggest_uniform(\"momentum\", 0.8, 0.99)\n",
    "    imgsz = trial.suggest_categorical(\"imgsz\", [320, 480, 640, 800])  # Tamaños de imagen\n",
    "\n",
    "    # Inicializar modelo YOLO\n",
    "    model = YOLO(\"yolov8n.pt\")\n",
    "\n",
    "    # Configurar entrenamiento\n",
    "    project_name = \"optuna_yolo_training\"\n",
    "    dataset_yaml = os.path.join(os.getcwd(), \"working\", \"output\", \"dataset\", \"dataset.yaml\")\n",
    "    try:\n",
    "        results = model.train(\n",
    "            data=dataset_yaml,\n",
    "            epochs=EPOCAS,  # Épocas fijas para experimentos\n",
    "            batch=batch_size,\n",
    "            imgsz=imgsz,\n",
    "            lr0=learning_rate,\n",
    "            momentum=momentum,\n",
    "            project=project_name,\n",
    "            name=f\"trial_{trial.number}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n",
    "            device=get_device()\n",
    "        )\n",
    "\n",
    "        # Evaluar el modelo\n",
    "        metrics = model.val()\n",
    "        return metrics[\"mAP50\"]  # Devolver mAP50 como métrica objetivo\n",
    "    except Exception as e:\n",
    "        logging.error(f\"[ERROR] Error durante el entrenamiento en el trial {trial.number}: {e}\")\n",
    "        return float(\"nan\")\n",
    "\n",
    "# === Entrenamiento Regular (Sin Optuna) ===\n",
    "def train_model(dataset_yaml=None, pretrained_model=\"yolov8n.pt\", epochs=EPOCAS, batch_size=16, learning_rate=0.001, momentum=0.9, imgsz=640):\n",
    "    \"\"\"\n",
    "    Entrena el modelo YOLO con hiperparámetros definidos manualmente.\n",
    "\n",
    "    Parameters:\n",
    "    - dataset_yaml (str): Ruta al archivo dataset.yaml.\n",
    "    - pretrained_model (str): Modelo YOLO preentrenado.\n",
    "    - epochs (int): Número de épocas para el entrenamiento.\n",
    "    - batch_size (int): Tamaño del batch.\n",
    "    - learning_rate (float): Tasa de aprendizaje inicial.\n",
    "    - momentum (float): Momento para el optimizador.\n",
    "    - imgsz (int): Tamaño de las imágenes de entrada.\n",
    "    \"\"\"\n",
    "    dataset_yaml = dataset_yaml or os.path.join(os.getcwd(), \"working\", \"output\", \"dataset\", \"dataset.yaml\")\n",
    "\n",
    "    if not os.path.exists(dataset_yaml):\n",
    "        logging.error(f\"[ERROR] dataset.yaml no encontrado en {dataset_yaml}. Asegúrate de que el pipeline_setup.py lo haya generado.\")\n",
    "        return\n",
    "\n",
    "    logging.info(f\"[INFO] Usando dataset.yaml en: {dataset_yaml}\")\n",
    "\n",
    "    model = YOLO(pretrained_model)\n",
    "\n",
    "    output_dir = f\"regular_yolo_training/{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Registrar el callback de barra de progreso\n",
    "    progress_bar = ProgressBarCallback(total_epochs=epochs)\n",
    "    model.add_callback(\"on_train_start\", progress_bar.on_train_start)\n",
    "    model.add_callback(\"on_epoch_end\", progress_bar.on_epoch_end)\n",
    "    model.add_callback(\"on_train_end\", progress_bar.on_train_end)\n",
    "\n",
    "    try:\n",
    "        logging.info(\"[INFO] Iniciando entrenamiento regular...\")\n",
    "        model.train(\n",
    "            data=dataset_yaml,\n",
    "            epochs=epochs,\n",
    "            batch=batch_size,\n",
    "            imgsz=imgsz,\n",
    "            lr0=learning_rate,\n",
    "            momentum=momentum,\n",
    "            project=output_dir,\n",
    "            name=\"train\",\n",
    "            device=get_device()\n",
    "        )\n",
    "        logging.info(f\"[INFO] Entrenamiento completado. Resultados guardados en {output_dir}.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"[ERROR] Error durante el entrenamiento: {e}\")\n",
    "\n",
    "# === Integración de Optuna en el Pipeline ===\n",
    "def run_optuna_study(dataset_yaml=None, n_trials=20):\n",
    "    \"\"\"\n",
    "    Ejecuta un estudio de Optuna para optimizar los hiperparámetros de YOLO.\n",
    "\n",
    "    Parameters:\n",
    "    - dataset_yaml (str): Ruta al archivo dataset.yaml.\n",
    "    - n_trials (int): Número de pruebas a ejecutar.\n",
    "    \"\"\"\n",
    "    dataset_yaml = dataset_yaml or os.path.join(os.getcwd(), \"working\", \"output\", \"dataset\", \"dataset.yaml\")\n",
    "\n",
    "    if not os.path.exists(dataset_yaml):\n",
    "        logging.error(f\"[ERROR] dataset.yaml no encontrado en {dataset_yaml}. Asegúrate de que el pipeline_setup.py lo haya generado.\")\n",
    "        return\n",
    "\n",
    "    logging.info(f\"[INFO] Usando dataset.yaml en: {dataset_yaml}\")\n",
    "\n",
    "    logging.info(\"[INFO] Iniciando optimización con Optuna...\")\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "    # Mostrar resultados\n",
    "    logging.info(f\"[INFO] Mejor conjunto de hiperparámetros: {study.best_params}\")\n",
    "    logging.info(f\"[INFO] Mejor mAP50 obtenido: {study.best_value}\")\n",
    "\n",
    "    # Guardar resultados\n",
    "    study.trials_dataframe().to_csv(\"optuna_results.csv\")\n",
    "    optuna.visualization.plot_optimization_history(study).write_html(\"optuna_optimization_history.html\")\n",
    "\n",
    "\n",
    "# === Función Principal ===\n",
    "def main(optuna_mode=False):\n",
    "    \"\"\"\n",
    "    Ejecuta el entrenamiento con o sin Optuna.\n",
    "\n",
    "    Parameters:\n",
    "    - optuna_mode (bool): Si es True, utiliza Optuna para optimizar hiperparámetros.\n",
    "    \"\"\"\n",
    "    if \"google.colab\" in sys.modules:\n",
    "        dataset_yaml = os.path.join(os.getcwd(), \"working\", \"output\", \"dataset\", \"dataset.yaml\")\n",
    "        print(dataset_yaml)\n",
    "    elif os.path.exists(\"/kaggle\"):\n",
    "        dataset_yaml = os.path.join(os.getcwd(), \"output\", \"dataset\", \"dataset.yaml\")\n",
    "        print(dataset_yaml)\n",
    "    else:\n",
    "        dataset_yaml = os.path.join(os.getcwd(), \"output\", \"dataset\", \"dataset.yaml\")\n",
    "        print(dataset_yaml)\n",
    "\n",
    "\n",
    "    if optuna_mode:\n",
    "        run_optuna_study(dataset_yaml, n_trials=20)\n",
    "    else:\n",
    "        train_model(dataset_yaml, imgsz=640)  # Tamaño de imagen predeterminado\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(optuna_mode=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"last_expr\"  # Muestra solo el último resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m dataset_yaml \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mgetcwd(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mworking\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m train_model(dataset_yaml, imgsz\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m640\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "dataset_yaml = os.path.join(os.getcwd(), \"working\", \"output\", \"dataset\", \"dataset.yaml\")\n",
    "train_model(dataset_yaml, imgsz=640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"nvidia-smi\" no se reconoce como un comando interno o externo,\n",
      "programa o archivo por lotes ejecutable.\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MiguelEnvHaB",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
