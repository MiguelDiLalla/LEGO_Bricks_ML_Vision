{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"..\")  # Ajusta seg√∫n tu estructura\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate_folders_ImageDataProject.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project structure created successfully at: c:\\Users\\User\\Projects_Unprotected\\LEGO_Bricks_ML_Vision\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def create_project_structure():\n",
    "    \"\"\"\n",
    "    Create a folder structure for an image data project.\n",
    "\n",
    "    The base path is automatically set to the current working directory.\n",
    "    \"\"\"\n",
    "    # Set the base directory to the current working directory\n",
    "    base_path = os.getcwd()\n",
    "\n",
    "    # Define the folder structure\n",
    "    structure = {\n",
    "        \"data\": [\n",
    "            \"raw\",\n",
    "            \"processed\",\n",
    "            \"annotations\"\n",
    "        ],\n",
    "        \"src\": [\n",
    "            \"data_preprocessing\",\n",
    "            \"models\",\n",
    "            \"visualization\"\n",
    "        ],\n",
    "        \"models\": [\n",
    "            \"checkpoints\",\n",
    "            \"final\"\n",
    "        ],\n",
    "        \"results\": [\n",
    "            \"predictions\",\n",
    "            \"logs\"\n",
    "        ],\n",
    "        \"docs\": [],\n",
    "        \"notebooks\": [],\n",
    "        \"tests\": [],\n",
    "        \"presentation\": []\n",
    "    }\n",
    "\n",
    "    # Function to create folders recursively\n",
    "    def create_folders(base, folders):\n",
    "        for folder, subfolders in folders.items():\n",
    "            folder_path = os.path.join(base, folder)\n",
    "            os.makedirs(folder_path, exist_ok=True)\n",
    "            for subfolder in subfolders:\n",
    "                os.makedirs(os.path.join(folder_path, subfolder), exist_ok=True)\n",
    "\n",
    "    # Create the structure\n",
    "    create_folders(base_path, structure)\n",
    "\n",
    "    print(f\"Project structure created successfully at: {base_path}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    create_project_structure()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exportar Refurbished PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook LEGO_Bricks_Identification_Project_Overview.ipynb to pdf\n",
      "[NbConvertApp] Writing 34205 bytes to notebook.tex\n",
      "[NbConvertApp] Building PDF\n",
      "[NbConvertApp] Running xelatex 3 times: ['xelatex', 'notebook.tex', '-quiet']\n",
      "[NbConvertApp] Running bibtex 1 time: ['bibtex', 'notebook']\n",
      "[NbConvertApp] WARNING | b had problems, most likely because there were no citations\n",
      "[NbConvertApp] PDF successfully created\n",
      "[NbConvertApp] Writing 42940 bytes to LEGO_Bricks_Identification_Project_Overview.pdf\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to pdf LEGO_Bricks_Identification_Project_Overview.ipynb\n",
    "# !jupyter nbconvert --to pdf LEGO_Bricks_Identification_Project_Technical_Report.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refurbishing Pipeline\n",
    "\n",
    "(chatGPT is having bugs remembering converstions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"\n",
    "\n",
    "¬°Perfecto! Vamos a estructurar esta entrevista para guiarte a reflexionar sobre cada etapa del proyecto y asegurarnos de que tu storytelling est√© alineado con las mejores pr√°cticas. Este enfoque tambi√©n nos permitir√° planear c√≥mo documentar, reproducir y organizar el proyecto de manera profesional.\n",
    "\n",
    "Entrevista Inicial\n",
    "Te har√© preguntas relevantes por etapas del proyecto. Responde seg√∫n tu experiencia, y construir√© secciones del documento con base en tus respuestas. Tambi√©n sugerir√© scripts o ajustes seg√∫n lo que compartas.\n",
    "\n",
    "1. Origen del Proyecto\n",
    "¬øQu√© te motiv√≥ a iniciar este proyecto? ¬øFue un desaf√≠o t√©cnico que quer√≠as resolver, una curiosidad personal, o algo m√°s?\n",
    "¬øC√≥mo conectaste tus habilidades previas con esta idea? Por ejemplo, ¬øqu√© experiencias te influyeron a elegir visi√≥n por computadora y no otro campo?\n",
    "2. Creaci√≥n del Dataset\n",
    "¬øQu√© tipo de datos recopilaste y c√≥mo? Comparte detalles sobre el proceso, como herramientas utilizadas, n√∫mero de im√°genes, y estrategias para capturar diferentes √°ngulos y condiciones.\n",
    "¬øC√≥mo manejaste la anotaci√≥n de los datos? ¬øUsaste alguna herramienta espec√≠fica como LabelMe o desarrollaste algo propio? ¬øHubo desaf√≠os en esta etapa?\n",
    "¬øQu√© criterios usaste para limpiar y preparar los datos? Por ejemplo, ¬øeliminaste im√°genes borrosas o hiciste alg√∫n balanceo entre clases?\n",
    "3. Selecci√≥n del Modelo\n",
    "¬øPor qu√© elegiste YOLO? ¬øProbaste otros modelos antes? Si es as√≠, ¬øcu√°les y por qu√© los descartaste?\n",
    "¬øC√≥mo ajustaste los hiperpar√°metros durante el entrenamiento? Por ejemplo, tama√±o de lotes, n√∫mero de √©pocas, etc.\n",
    "¬øQu√© limitaciones encontraste en el modelo, y c√≥mo las abordaste?\n",
    "4. Implementaci√≥n y Resultados\n",
    "¬øC√≥mo transformaste los resultados del modelo en algo pr√°ctico? Por ejemplo, ¬øc√≥mo calculaste las dimensiones de las piezas bas√°ndote en la detecci√≥n de studs?\n",
    "¬øQu√© m√©tricas utilizaste para medir el √©xito? ¬øPrecision, recall, F1-score, u otras? ¬øC√≥mo documentaste estos resultados?\n",
    "¬øQu√© visualizaciones creaste para entender mejor los resultados?\n",
    "5. Reflexi√≥n y Futuro\n",
    "¬øQu√© aprendiste en t√©rminos t√©cnicos y personales? ¬øQu√© habilidades desarrollaste o reforzaste?\n",
    "Si comenzaras de nuevo, ¬øqu√© har√≠as diferente?\n",
    "¬øQu√© pasos futuros te gustar√≠a explorar? Por ejemplo, ¬ømejorar el modelo, ampliar el dataset, o implementar nuevas funcionalidades?\n",
    "Pr√≥ximos Pasos\n",
    "Conforme respondas, construir√© borradores de las secciones correspondientes del documento. Si surgen necesidades espec√≠ficas (como scripts para repetir entrenamientos), las desarrollaremos y definiremos c√≥mo integrarlas al proyecto.\n",
    "\n",
    "Comencemos con tus respuestas a las primeras preguntas de la secci√≥n 1 sobre la motivaci√≥n y el origen del proyecto. üòä\n",
    "\n",
    "# \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementacion de pipeline central\n",
    "\n",
    "## pipeline.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # -*- coding: utf-8 -*-\n",
    "\n",
    "# import os\n",
    "\n",
    "# def generate_structure_file(output_file=\"estructura.txt\"):\n",
    "#     \"\"\"\n",
    "#     Genera un archivo con la estructura de directorios solo si el directorio actual\n",
    "#     contiene una carpeta 'src' o est√° relacionado con ella.\n",
    "\n",
    "#     Args:\n",
    "#         output_file (str): Nombre del archivo para guardar la estructura.\n",
    "#     \"\"\"\n",
    "#     # Directorio actual\n",
    "#     current_dir = os.getcwd()\n",
    "\n",
    "#     # Verificar si existe la carpeta 'src' en el directorio actual\n",
    "#     if \"src\" in os.listdir(current_dir):\n",
    "#         # Crear la estructura en el directorio actual\n",
    "#         target_dir = current_dir\n",
    "#         print(f\"Directorio 'src' encontrado en el directorio actual: {target_dir}\")\n",
    "#     else:\n",
    "#         # Subir al directorio padre y verificar si 'src' es una carpeta hermana\n",
    "#         parent_dir = os.path.abspath(os.path.join(current_dir, os.pardir))\n",
    "#         if \"src\" in os.listdir(parent_dir):\n",
    "#             # Crear la estructura en el directorio padre\n",
    "#             target_dir = parent_dir\n",
    "#             print(f\"Directorio 'src' encontrado como carpeta hermana. Generando estructura en: {target_dir}\")\n",
    "#         else:\n",
    "#             print(\"No se encontr√≥ la carpeta 'src'. No se generar√° el archivo de estructura.\")\n",
    "#             return\n",
    "\n",
    "#     # Generar la estructura\n",
    "#     with open(output_file, \"w\") as f:\n",
    "#         for root, dirs, files in os.walk(target_dir):\n",
    "#             level = root.replace(target_dir, \"\").count(os.sep)\n",
    "#             indent = \" \" * 4 * level\n",
    "#             f.write(f\"{indent}{os.path.basename(root)}/\\n\")\n",
    "#             subindent = \" \" * 4 * (level + 1)\n",
    "#             for file in files:\n",
    "#                 f.write(f\"{subindent}{file}\\n\")\n",
    "\n",
    "#     print(f\"Estructura generada exitosamente en: {output_file}\")\n",
    "\n",
    "# # Ejecutar la funci√≥n\n",
    "# if __name__ == \"__main__\":\n",
    "#     generate_structure_file()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Projects_Unprotected\\LEGO_Bricks_ML_Vision\n",
      "c:\\Users\\User\\Projects_Unprotected\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Muestra la ruta actual\n",
    "print(os.getcwd())\n",
    "\n",
    "# Cambia al directorio ra√≠z del proyecto\n",
    "os.chdir(\"..\")  # Cambia al directorio ra√≠z\n",
    "\n",
    "# Confirma el cambio de directorio\n",
    "print(os.getcwd())\n",
    "\n",
    "# Importa funciones desde el pipeline\n",
    "from scripts.pipeline import (\n",
    "    setup_environment,\n",
    "    get_kaggle_credentials,\n",
    "    download_dataset_from_kaggle,\n",
    "    validate_directories,\n",
    "    preprocess_images,\n",
    "    labelme_to_yolo,\n",
    "    train_yolo_pipeline,\n",
    "    test_model_on_real_images,\n",
    "    visualize_results,\n",
    ")\n",
    "\n",
    "# Prueba de las funciones importadas\n",
    "setup_environment()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview.pdf\n",
    "\n",
    "## visualize_presentation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import random\n",
    "# import matplotlib.pyplot as plt\n",
    "# from PIL import Image\n",
    "# from pipeline import test_model_on_real_images\n",
    "\n",
    "# # Crear grids de muestras del dataset\n",
    "# def create_dataset_grid(input_folder, output_folder, grid_size=(3, 3)):\n",
    "#     \"\"\"\n",
    "#     Genera un grid de im√°genes de muestra del dataset y las guarda en la carpeta especificada.\n",
    "\n",
    "#     Parameters:\n",
    "#     - input_folder (str): Ruta de la carpeta con las im√°genes de entrada.\n",
    "#     - output_folder (str): Ruta donde se guardar√° el grid generado.\n",
    "#     - grid_size (tuple): Dimensiones del grid (filas, columnas).\n",
    "#     \"\"\"\n",
    "#     os.makedirs(output_folder, exist_ok=True)\n",
    "#     image_files = [f for f in os.listdir(input_folder) if f.endswith(('.jpg', '.png'))]\n",
    "#     random.shuffle(image_files)\n",
    "    \n",
    "#     selected_files = image_files[:grid_size[0] * grid_size[1]]\n",
    "#     fig, axes = plt.subplots(*grid_size, figsize=(grid_size[1] * 4, grid_size[0] * 4))\n",
    "\n",
    "#     for idx, img_file in enumerate(selected_files):\n",
    "#         img_path = os.path.join(input_folder, img_file)\n",
    "#         img = Image.open(img_path)\n",
    "#         row, col = divmod(idx, grid_size[1])\n",
    "#         axes[row, col].imshow(img)\n",
    "#         axes[row, col].axis('off')\n",
    "#         axes[row, col].set_title(f\"{img_file}\", fontsize=10)\n",
    "\n",
    "#     for ax in axes.flat[len(selected_files):]:\n",
    "#         ax.axis('off')\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     grid_path = os.path.join(output_folder, \"dataset_grid.png\")\n",
    "#     plt.savefig(grid_path, bbox_inches='tight')\n",
    "#     plt.close()\n",
    "#     print(f\"Grid de dataset guardado en {grid_path}.\")\n",
    "\n",
    "# # Generar visualizaciones anotadas\n",
    "# def annotate_model_results(model_path, input_folder, output_folder, conf_threshold=0.5):\n",
    "#     \"\"\"\n",
    "#     Genera im√°genes con anotaciones del modelo y las guarda en la carpeta especificada.\n",
    "\n",
    "#     Parameters:\n",
    "#     - model_path (str): Ruta al modelo YOLO entrenado.\n",
    "#     - input_folder (str): Carpeta con im√°genes de entrada.\n",
    "#     - output_folder (str): Carpeta para guardar las im√°genes anotadas.\n",
    "#     - conf_threshold (float): Umbral de confianza para las detecciones.\n",
    "#     \"\"\"\n",
    "#     os.makedirs(output_folder, exist_ok=True)\n",
    "#     test_model_on_real_images(model_path, input_folder, output_folder)\n",
    "#     print(f\"Resultados anotados guardados en {output_folder}.\")\n",
    "\n",
    "# # Crear comparaciones antes/despu√©s\n",
    "# def generate_comparison_grid(model_path, input_folder, output_folder, num_samples=5):\n",
    "#     \"\"\"\n",
    "#     Crea un grid comparativo de im√°genes antes y despu√©s de las predicciones del modelo.\n",
    "\n",
    "#     Parameters:\n",
    "#     - model_path (str): Ruta al modelo YOLO entrenado.\n",
    "#     - input_folder (str): Carpeta con im√°genes de entrada.\n",
    "#     - output_folder (str): Carpeta para guardar los grids generados.\n",
    "#     - num_samples (int): N√∫mero de im√°genes a comparar.\n",
    "#     \"\"\"\n",
    "#     os.makedirs(output_folder, exist_ok=True)\n",
    "#     image_files = [f for f in os.listdir(input_folder) if f.endswith(('.jpg', '.png'))]\n",
    "#     random.shuffle(image_files)\n",
    "#     selected_files = image_files[:num_samples]\n",
    "\n",
    "#     for img_file in selected_files:\n",
    "#         img_path = os.path.join(input_folder, img_file)\n",
    "#         result_path = os.path.join(output_folder, img_file)\n",
    "\n",
    "#         test_model_on_real_images(model_path, img_path, output_folder)\n",
    "#         annotated_img = Image.open(result_path)\n",
    "#         original_img = Image.open(img_path)\n",
    "\n",
    "#         fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "#         axes[0].imshow(original_img)\n",
    "#         axes[0].axis('off')\n",
    "#         axes[0].set_title(\"Original\")\n",
    "\n",
    "#         axes[1].imshow(annotated_img)\n",
    "#         axes[1].axis('off')\n",
    "#         axes[1].set_title(\"Anotado\")\n",
    "\n",
    "#         plt.tight_layout()\n",
    "#         comparison_path = os.path.join(output_folder, f\"comparison_{img_file}\")\n",
    "#         plt.savefig(comparison_path, bbox_inches='tight')\n",
    "#         plt.close()\n",
    "#         print(f\"Grid de comparaci√≥n guardado en {comparison_path}.\")\n",
    "\n",
    "# # Organizaci√≥n autom√°tica de subcarpetas\n",
    "# def organize_presentation_folders(base_folder):\n",
    "#     \"\"\"\n",
    "#     Crea y organiza las subcarpetas necesarias para las visualizaciones.\n",
    "\n",
    "#     Parameters:\n",
    "#     - base_folder (str): Carpeta ra√≠z para las subcarpetas.\n",
    "#     \"\"\"\n",
    "#     subfolders = [\"dataset_samples\", \"model_results\", \"before_after\"]\n",
    "#     for subfolder in subfolders:\n",
    "#         os.makedirs(os.path.join(base_folder, subfolder), exist_ok=True)\n",
    "#     print(f\"Subcarpetas creadas en {base_folder}: {', '.join(subfolders)}\")\n",
    "\n",
    "# # Ejecuci√≥n del pipeline de visualizaci√≥n\n",
    "# def main():\n",
    "#     base_folder = \"presentation\"\n",
    "#     organize_presentation_folders(base_folder)\n",
    "\n",
    "#     dataset_folder = \"datasets/processed_images\"\n",
    "#     model_path = \"YOLO_Lego_Detection/best.pt\"\n",
    "\n",
    "#     create_dataset_grid(dataset_folder, os.path.join(base_folder, \"dataset_samples\"))\n",
    "#     annotate_model_results(model_path, dataset_folder, os.path.join(base_folder, \"model_results\"))\n",
    "#     generate_comparison_grid(model_path, dataset_folder, os.path.join(base_folder, \"before_after\"))\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Projects_Unprotected\\LEGO_Bricks_ML_Vision\\notebooks\n",
      "c:\\Users\\User\\Projects_Unprotected\\LEGO_Bricks_ML_Vision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-20 00:55:26,826 - INFO - Subcarpetas creadas en presentation: dataset_samples, model_results, before_after\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Muestra la ruta actual\n",
    "print(os.getcwd())\n",
    "\n",
    "# Cambia al directorio ra√≠z del proyecto\n",
    "os.chdir(\"..\")  # Cambia al directorio ra√≠z\n",
    "\n",
    "# Confirma el cambio de directorio\n",
    "print(os.getcwd())\n",
    "\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "# Importar funciones clave\n",
    "from scripts.visualize_presentation import (\n",
    "    create_dataset_grid,\n",
    "    annotate_model_results,\n",
    "    generate_comparison_grid,\n",
    "    organize_presentation_folders\n",
    ")\n",
    "\n",
    "# Definir rutas de entrada y salida\n",
    "BASE_FOLDER = \"presentation\"\n",
    "DATASET_FOLDER = \"datasets/processed_images\"\n",
    "MODEL_PATH = \"YOLO_Lego_Detection/best.pt\"\n",
    "\n",
    "# Crear y organizar las carpetas de presentaci√≥n\n",
    "organize_presentation_folders(BASE_FOLDER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Projects_Unprotected\\LEGO_Bricks_ML_Vision\n",
      "c:\\Users\\User\\Projects_Unprotected\n",
      "['c:\\\\Users\\\\User\\\\Projects_Unprotected\\\\LEGO_Bricks_ML_Vision\\\\notebooks',\n",
      " 'c:\\\\Users\\\\User\\\\.conda\\\\envs\\\\MiguelEnvHaB\\\\python312.zip',\n",
      " 'c:\\\\Users\\\\User\\\\.conda\\\\envs\\\\MiguelEnvHaB\\\\DLLs',\n",
      " 'c:\\\\Users\\\\User\\\\.conda\\\\envs\\\\MiguelEnvHaB\\\\Lib',\n",
      " 'c:\\\\Users\\\\User\\\\.conda\\\\envs\\\\MiguelEnvHaB',\n",
      " '',\n",
      " 'C:\\\\Users\\\\User\\\\AppData\\\\Roaming\\\\Python\\\\Python312\\\\site-packages',\n",
      " 'c:\\\\Users\\\\User\\\\.conda\\\\envs\\\\MiguelEnvHaB\\\\Lib\\\\site-packages',\n",
      " 'c:\\\\Users\\\\User\\\\.conda\\\\envs\\\\MiguelEnvHaB\\\\Lib\\\\site-packages\\\\win32',\n",
      " 'c:\\\\Users\\\\User\\\\.conda\\\\envs\\\\MiguelEnvHaB\\\\Lib\\\\site-packages\\\\win32\\\\lib',\n",
      " 'c:\\\\Users\\\\User\\\\.conda\\\\envs\\\\MiguelEnvHaB\\\\Lib\\\\site-packages\\\\Pythonwin',\n",
      " 'c:\\\\Users\\\\User\\\\.conda\\\\envs\\\\MiguelEnvHaB\\\\Lib\\\\site-packages\\\\setuptools\\\\_vendor',\n",
      " 'c:\\\\Users\\\\User\\\\Projects_Unprotected\\\\LEGO_Bricks_ML_Vision',\n",
      " 'c:\\\\Users\\\\User\\\\Projects_Unprotected']\n"
     ]
    }
   ],
   "source": [
    "# import sys\n",
    "# import os\n",
    "# from pprint import pprint\n",
    "\n",
    "# # Muestra la ruta actual\n",
    "# print(os.getcwd())\n",
    "\n",
    "# # Cambia al directorio ra√≠z del proyecto\n",
    "# os.chdir(\"..\")  # Cambia al directorio ra√≠z\n",
    "\n",
    "# # Confirma el cambio de directorio\n",
    "# print(os.getcwd())\n",
    "\n",
    "# sys.path.append(os.getcwd()) \n",
    "\n",
    "# pprint(sys.path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-20 00:55:34,996 - INFO - Subcarpetas creadas en presentation: dataset_samples, model_results, before_after\n",
      "2025-01-20 00:55:34,997 - ERROR - La carpeta de entrada no existe.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Carpeta de entrada no encontrada.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m     annotate_model_results(MODEL_PATH, DATASET_FOLDER, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBASE_FOLDER\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/model_results\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m     generate_comparison_grid(MODEL_PATH, DATASET_FOLDER, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBASE_FOLDER\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/before_after\u001b[39m\u001b[38;5;124m\"\u001b[39m, num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m run_visualization_pipeline()\n",
      "Cell \u001b[1;32mIn[2], line 4\u001b[0m, in \u001b[0;36mrun_visualization_pipeline\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_visualization_pipeline\u001b[39m():\n\u001b[0;32m      3\u001b[0m     organize_presentation_folders(BASE_FOLDER)\n\u001b[1;32m----> 4\u001b[0m     create_dataset_grid(DATASET_FOLDER, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBASE_FOLDER\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/dataset_samples\u001b[39m\u001b[38;5;124m\"\u001b[39m, grid_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m))\n\u001b[0;32m      5\u001b[0m     annotate_model_results(MODEL_PATH, DATASET_FOLDER, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBASE_FOLDER\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/model_results\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m     generate_comparison_grid(MODEL_PATH, DATASET_FOLDER, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBASE_FOLDER\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/before_after\u001b[39m\u001b[38;5;124m\"\u001b[39m, num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\User\\Projects_Unprotected\\LEGO_Bricks_ML_Vision\\scripts\\visualize_presentation.py:25\u001b[0m, in \u001b[0;36mcreate_dataset_grid\u001b[1;34m(input_folder, output_folder, grid_size)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(input_folder):\n\u001b[0;32m     24\u001b[0m     logging\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLa carpeta de entrada no existe.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 25\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCarpeta de entrada no encontrada.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     27\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(output_folder, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     28\u001b[0m image_files \u001b[38;5;241m=\u001b[39m [f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(input_folder) \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m'\u001b[39m))]\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: Carpeta de entrada no encontrada."
     ]
    }
   ],
   "source": [
    "# Ejecutar todo el pipeline de visualizaci√≥n\n",
    "def run_visualization_pipeline():\n",
    "    organize_presentation_folders(BASE_FOLDER)\n",
    "    create_dataset_grid(DATASET_FOLDER, f\"{BASE_FOLDER}/dataset_samples\", grid_size=(3, 3))\n",
    "    annotate_model_results(MODEL_PATH, DATASET_FOLDER, f\"{BASE_FOLDER}/model_results\")\n",
    "    generate_comparison_grid(MODEL_PATH, DATASET_FOLDER, f\"{BASE_FOLDER}/before_after\", num_samples=5)\n",
    "\n",
    "run_visualization_pipeline()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setup.py \n",
    "\n",
    "Packege publication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from setuptools import setup, find_packages\n",
    "\n",
    "# setup(\n",
    "#     name=\"lego_bricks_ml_vision\",\n",
    "#     version=\"0.1.0\",\n",
    "#     packages=find_packages(),\n",
    "#     install_requires=[\n",
    "#         \"torch>=1.8.0\",\n",
    "#         \"ultralytics\",\n",
    "#         \"pillow\",\n",
    "#         \"matplotlib\",\n",
    "#         \"kaggle\",\n",
    "#     ],\n",
    "#     entry_points={\n",
    "#         \"console_scripts\": [\n",
    "#             \"run-pipeline=scripts.pipeline:main\",\n",
    "#             \"run-visualize=scripts.visualize_presentation:main\",\n",
    "#         ],\n",
    "#     },\n",
    "#     author=\"Miguel Di Lalla\",\n",
    "#     description=\"A package for LEGO brick detection and visualization using YOLO.\",\n",
    "#     long_description=open(\"README.md\").read(),\n",
    "#     long_description_content_type=\"text/markdown\",\n",
    "#     license=\"Apache License 2.0\",\n",
    "#     classifiers=[\n",
    "#         \"Programming Language :: Python :: 3\",\n",
    "#         \"License :: OSI Approved :: Apache Software License\",\n",
    "#         \"Operating System :: OS Independent\",\n",
    "#     ],\n",
    "#     python_requires=\">=3.8\",\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rqueriments.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Versions:\n",
      "PyTorch: 2.4.1+cpu\n",
      "Ultralytics: 8.2.99\n",
      "Pillow (PIL): 11.1.0\n",
      "Matplotlib: 3.8.4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import ultralytics\n",
    "import PIL\n",
    "import matplotlib\n",
    "import kaggle\n",
    "\n",
    "print(\"Environment Versions:\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Ultralytics: {ultralytics.__version__}\")\n",
    "print(f\"Pillow (PIL): {PIL.__version__}\")\n",
    "print(f\"Matplotlib: {matplotlib.__version__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: kaggle\n",
      "Version: 1.6.17\n",
      "Summary: Kaggle API\n",
      "Home-page: https://github.com/Kaggle/kaggle-api\n",
      "Author: Kaggle\n",
      "Author-email: support@kaggle.com\n",
      "License: Apache 2.0\n",
      "Location: C:\\Users\\User\\.conda\\envs\\MiguelEnvHaB\\Lib\\site-packages\n",
      "Requires: bleach, certifi, python-dateutil, python-slugify, requests, six, tqdm, urllib3\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "# print(f\"Kaggle: {kaggle.__version__}\")\n",
    "! pip show kaggle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch==2.0.1+cpu\n",
    "ultralytics==8.2.99\n",
    "pillow==11.1.0\n",
    "matplotlib==3.8.4\n",
    "kaggle==1.6.17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# README.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LEGO Bricks ML Vision\n",
    "\n",
    "A Python package for detecting and visualizing LEGO bricks using YOLO and custom visualization pipelines. This project is designed to streamline the process of detecting LEGO pieces in cluttered environments and producing meaningful visualizations for analysis and presentation.\n",
    "\n",
    "---\n",
    "\n",
    "## Features\n",
    "\n",
    "- **Object Detection**: Detect LEGO bricks using YOLOv8.\n",
    "- **Preprocessing Tools**: Scripts to preprocess images and datasets.\n",
    "- **Dataset Conversion**: Convert LabelMe annotations to YOLO format.\n",
    "- **Visualization Pipelines**: Create annotated grids, comparisons, and organized folders for presentations.\n",
    "- **Modular Design**: Easy-to-use package structure for scaling or adapting the project.\n",
    "\n",
    "---\n",
    "\n",
    "## Installation\n",
    "\n",
    "To get started with the project, clone the repository and install the package:\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/MiguelDiLalla/LEGO_Bricks_ML_Vision.git\n",
    "cd LEGO_Bricks_ML_Vision\n",
    "pip install -e .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prueba\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.pipeline import train_yolo_pipeline\n",
    "from scripts.visualize_presentation import create_dataset_grid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funciona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example: Train YOLO pipeline\n",
    "train_yolo_pipeline(\"path/to/dataset\")\n",
    "\n",
    "# Example: Create a dataset grid\n",
    "create_dataset_grid(\"path/to/images\", \"path/to/output\", grid_size=(3, 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt conversacion del pipeline.py\n",
    "\n",
    "para actualizar el comentariado luego de la publicacion en PyPl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"\n",
    "\n",
    "Tengo un script en Python que necesita ser revisado y refinado para alcanzar un nivel profesional. Aqu√≠ est√°n los objetivos espec√≠ficos que quiero lograr:\n",
    "\n",
    "1. **Comentarios claros y precisos**: Cada funci√≥n debe tener una docstring descriptiva que explique claramente su prop√≥sito, par√°metros, y valores de retorno (si los hay). Los comentarios en l√≠nea deben ser breves, relevantes y ayudar a entender el c√≥digo.\n",
    "\n",
    "2. **Formato consistente**: El c√≥digo debe seguir el est√°ndar **PEP 8**, incluyendo:\n",
    "   - Longitud de l√≠nea m√°xima de 79 caracteres.\n",
    "   - Identaci√≥n adecuada (4 espacios).\n",
    "   - Espaciado entre bloques y funciones.\n",
    "   - Organizaci√≥n l√≥gica de las importaciones (primero est√°ndar, luego librer√≠as externas, y finalmente locales).\n",
    "\n",
    "3. **Organizaci√≥n de secciones**: Agrupa las funciones relacionadas y a√±ade encabezados de comentarios (# ===) para separar las secciones principales del script.\n",
    "\n",
    "4. **Buena pr√°ctica en manejo de errores**: Incluye excepciones manejadas correctamente donde sea relevante (e.g., para operaciones de I/O o dependencias externas).\n",
    "\n",
    "5. **Estilo profesional en los print()**: Usa mensajes formateados de manera clara y profesional, por ejemplo:\n",
    "   ```python\n",
    "   print(f\"[INFO] Dataset descargado y extra√≠do en {destination}\")\n",
    "\n",
    "6. **Detalles adicionales:** Si algo en el c√≥digo puede ser optimizado o mejorado, prop√≥n cambios para hacerlo m√°s eficiente y profesional.\n",
    "\n",
    "# \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from PIL import Image\n",
    "# import torch\n",
    "# import shutil\n",
    "# import kaggle\n",
    "# import zipfile\n",
    "\n",
    "# # === Configuraci√≥n Inicial ===\n",
    "# def setup_environment():\n",
    "#     \"\"\"\n",
    "#     Clona el repositorio y configura el entorno necesario para ejecutar el pipeline.\n",
    "\n",
    "#     - Clona el repositorio de GitHub.\n",
    "#     - Instala las dependencias desde el archivo requirements.txt.\n",
    "#     - Configura el dispositivo de ejecuci√≥n (CPU o GPU).\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         os.system(\"git clone https://github.com/MiguelDiLalla/LEGO_Bricks_ML_Vision.git\")\n",
    "#         os.chdir(\"LEGO_Bricks_ML_Vision\")\n",
    "#         os.system(\"pip install -r requirements.txt\")\n",
    "#         device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#         print(f\"[INFO] Using device: {device}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"[ERROR] Error al configurar el entorno: {e}\")\n",
    "\n",
    "# # === Manejo de Credenciales de Kaggle ===\n",
    "# def get_kaggle_credentials():\n",
    "#     \"\"\"\n",
    "#     Obtiene las credenciales de Kaggle desde variables de entorno o archivo kaggle.json.\n",
    "\n",
    "#     Prioridad de b√∫squeda:\n",
    "#     1. Variables de entorno: KAGGLE_USERNAME y KAGGLE_KEY.\n",
    "#     2. Archivo ~/.kaggle/kaggle.json.\n",
    "\n",
    "#     Returns:\n",
    "#     - dict: Diccionario con 'username' y 'key'.\n",
    "#     \"\"\"\n",
    "#     username = os.getenv(\"KAGGLE_USERNAME\")\n",
    "#     key = os.getenv(\"KAGGLE_KEY\")\n",
    "\n",
    "#     if username and key:\n",
    "#         return {\"username\": username, \"key\": key}\n",
    "\n",
    "#     kaggle_json_path = os.path.expanduser(\"~/.kaggle/kaggle.json\")\n",
    "#     if os.path.exists(kaggle_json_path):\n",
    "#         try:\n",
    "#             with open(kaggle_json_path, \"r\") as f:\n",
    "#                 return json.load(f)\n",
    "#         except Exception as e:\n",
    "#             raise ValueError(f\"[ERROR] No se pudo leer kaggle.json: {e}\")\n",
    "\n",
    "#     raise ValueError(\"[ERROR] Credenciales de Kaggle no encontradas. Configure las variables de entorno o coloque kaggle.json en ~/.kaggle.\")\n",
    "\n",
    "# # === Descarga de Dataset ===\n",
    "# def download_dataset_from_kaggle(dataset, destination):\n",
    "#     \"\"\"\n",
    "#     Descarga y extrae un dataset de Kaggle.\n",
    "\n",
    "#     Parameters:\n",
    "#     - dataset (str): Nombre del dataset en el formato \"usuario/dataset\".\n",
    "#     - destination (str): Ruta donde se extraer√°n los archivos.\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         credentials = get_kaggle_credentials()\n",
    "\n",
    "#         os.makedirs(destination, exist_ok=True)\n",
    "#         os.environ[\"KAGGLE_USERNAME\"] = credentials[\"username\"]\n",
    "#         os.environ[\"KAGGLE_KEY\"] = credentials[\"key\"]\n",
    "\n",
    "#         kaggle.api.dataset_download_files(dataset, path=destination, unzip=True)\n",
    "#         print(f\"[INFO] Dataset descargado y extra√≠do en {destination}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"[ERROR] No se pudo descargar el dataset: {e}\")\n",
    "\n",
    "# # === Validaci√≥n de Directorios ===\n",
    "# def validate_directories(directories):\n",
    "#     \"\"\"\n",
    "#     Valida la existencia de los directorios especificados.\n",
    "\n",
    "#     Parameters:\n",
    "#     - directories (list): Lista de rutas a validar.\n",
    "\n",
    "#     Returns:\n",
    "#     - bool: True si todos los directorios existen, False en caso contrario.\n",
    "#     \"\"\"\n",
    "#     for directory in directories:\n",
    "#         if not os.path.exists(directory):\n",
    "#             print(f\"[ERROR] Directorio no encontrado: {directory}\")\n",
    "#             return False\n",
    "#     print(\"[INFO] Todos los directorios est√°n correctamente configurados.\")\n",
    "#     return True\n",
    "\n",
    "# # === Preprocesamiento de Im√°genes ===\n",
    "# def preprocess_images(input_dir, output_dir, target_size=(256, 256)):\n",
    "#     \"\"\"\n",
    "#     Redimensiona im√°genes y asegura consistencia en nombres de archivos.\n",
    "\n",
    "#     Parameters:\n",
    "#     - input_dir (str): Ruta de la carpeta con im√°genes originales.\n",
    "#     - output_dir (str): Ruta de la carpeta para guardar las im√°genes procesadas.\n",
    "#     - target_size (tuple): Dimensiones objetivo para las im√°genes (ancho, alto).\n",
    "#     \"\"\"\n",
    "#     os.makedirs(output_dir, exist_ok=True)\n",
    "#     for i, filename in enumerate(sorted(os.listdir(input_dir))):\n",
    "#         if filename.endswith(\".jpg\"):\n",
    "#             try:\n",
    "#                 img = Image.open(os.path.join(input_dir, filename))\n",
    "#                 img_resized = img.resize(target_size)\n",
    "#                 new_filename = f\"image_{i}.jpg\"\n",
    "#                 img_resized.save(os.path.join(output_dir, new_filename))\n",
    "#                 print(f\"[INFO] Procesado: {filename} -> {new_filename}\")\n",
    "#             except Exception as e:\n",
    "#                 print(f\"[ERROR] No se pudo procesar {filename}: {e}\")\n",
    "\n",
    "# # === Conversi√≥n de Anotaciones de LabelMe a YOLO ===\n",
    "# def labelme_to_yolo(input_folder, output_folder):\n",
    "#     \"\"\"\n",
    "#     Convierte archivos JSON de LabelMe al formato YOLO.\n",
    "\n",
    "#     Parameters:\n",
    "#     - input_folder (str): Carpeta con archivos JSON de LabelMe.\n",
    "#     - output_folder (str): Carpeta donde se guardar√°n los archivos YOLO.\n",
    "#     \"\"\"\n",
    "#     os.makedirs(output_folder, exist_ok=True)\n",
    "#     for filename in os.listdir(input_folder):\n",
    "#         if filename.endswith('.json'):\n",
    "#             try:\n",
    "#                 json_file = os.path.join(input_folder, filename)\n",
    "#                 yolo_file = os.path.join(output_folder, filename.replace('.json', '.txt'))\n",
    "#                 # Conversi√≥n aqu√≠ (implementaci√≥n omitida para brevedad)\n",
    "#                 print(f\"[INFO] Convertido: {json_file} -> {yolo_file}\")\n",
    "#             except Exception as e:\n",
    "#                 print(f\"[ERROR] Error al convertir {filename}: {e}\")\n",
    "\n",
    "# # === Entrenamiento del Modelo YOLOv8n ===\n",
    "# def train_yolo_pipeline(dataset_path, annotations_format=\"YOLO\", epochs=50, img_size=256):\n",
    "#     \"\"\"\n",
    "#     Configura y entrena el modelo YOLO.\n",
    "\n",
    "#     Parameters:\n",
    "#     - dataset_path (str): Ruta del dataset procesado.\n",
    "#     - annotations_format (str): Formato de las anotaciones (por defecto \"YOLO\").\n",
    "#     - epochs (int): N√∫mero de √©pocas de entrenamiento.\n",
    "#     - img_size (int): Tama√±o de las im√°genes usadas para entrenamiento.\n",
    "#     \"\"\"\n",
    "#     from ultralytics import YOLO\n",
    "\n",
    "#     dataset_dir = os.path.join(dataset_path, \"processed_images\")\n",
    "#     annotations_dir = os.path.join(dataset_path, \"annotations\")\n",
    "\n",
    "#     if not validate_directories([dataset_dir, annotations_dir]):\n",
    "#         return\n",
    "\n",
    "#     try:\n",
    "#         model = YOLO(\"yolov8n.pt\")\n",
    "#         results = model.train(\n",
    "#             data=annotations_format,\n",
    "#             imgsz=img_size,\n",
    "#             epochs=epochs,\n",
    "#             batch=16,\n",
    "#             project=\"LEGO_Training\",\n",
    "#             name=\"YOLO_Lego_Detection\"\n",
    "#         )\n",
    "#         print(\"[INFO] Entrenamiento finalizado. Resultados:\", results)\n",
    "#     except Exception as e:\n",
    "#         print(f\"[ERROR] Error durante el entrenamiento: {e}\")\n",
    "\n",
    "# # === Evaluaci√≥n del Modelo Entrenado ===\n",
    "# def test_model_on_real_images(model_path, test_images_dir, output_dir):\n",
    "#     \"\"\"\n",
    "#     Eval√∫a el modelo YOLO entrenado en im√°genes reales.\n",
    "\n",
    "#     Parameters:\n",
    "#     - model_path (str): Ruta del modelo YOLO entrenado.\n",
    "#     - test_images_dir (str): Carpeta con im√°genes para evaluaci√≥n.\n",
    "#     - output_dir (str): Carpeta para guardar los resultados visualizados.\n",
    "#     \"\"\"\n",
    "#     from ultralytics import YOLO\n",
    "\n",
    "#     os.makedirs(output_dir, exist_ok=True)\n",
    "#     model = YOLO(model_path)\n",
    "\n",
    "#     for img_file in os.listdir(test_images_dir):\n",
    "#         if img_file.endswith(\".jpg\"):\n",
    "#             try:\n",
    "#                 img_path = os.path.join(test_images_dir, img_file)\n",
    "#                 results = model(img_path)\n",
    "#                 result_image = results[0].plot()\n",
    "#                 output_path = os.path.join(output_dir, img_file)\n",
    "#                 Image.fromarray(result_image).save(output_path)\n",
    "#                 print(f\"[INFO] Procesado: {img_file} -> {output_path}\")\n",
    "#             except Exception as e:\n",
    "#                 print(f\"[ERROR] Error al procesar {img_file}: {e}\")\n",
    "\n",
    "# # === Visualizaci√≥n de Resultados ===\n",
    "# def visualize_results(dataset_path):\n",
    "#     \"\"\"\n",
    "#     Visualiza detecciones en un grid de im√°genes anotadas.\n",
    "\n",
    "#     Parameters:\n",
    "#     - dataset_path (str): Ruta del dataset procesado.\n",
    "#     \"\"\"\n",
    "#     import matplotlib.pyplot as plt\n",
    "\n",
    "#     processed_dir = os.path.join(dataset_path, \"processed_images\")\n",
    "#     images = [os.path.join(processed_dir, img) for img in os.listdir(processed_dir) if img.endswith(\".jpg\")]\n",
    "\n",
    "#     plt.figure(figsize=(10, 10))\n",
    "#     for i, img_path in enumerate(images[:16]):  # Mostrar 16 im√°genes\n",
    "#         try:\n",
    "#             img = Image.open(img_path)\n",
    "#             plt.subplot(4, 4, i + 1)\n",
    "#             plt.imshow(img)\n",
    "#             plt.axis('off')\n",
    "#         except Exception as e:\n",
    "#             print(f\"[ERROR] No se pudo cargar {img_path}: {e}\")\n",
    "#     plt.show()\n",
    "\n",
    "# # === Ejecuci√≥n del Pipeline ===\n",
    "# def main():\n",
    "#     \"\"\"Ejecuci√≥n principal del pipeline de detecci√≥n de LEGO.\"\"\"\n",
    "#     setup_environment()\n",
    "#     download_dataset_from_kaggle(\"usuario/dataset\", \"datasets\")\n",
    "#     preprocess_images(\"datasets/raw\", \"datasets/processed\")\n",
    "#     labelme_to_yolo(\"datasets/processed\", \"datasets/annotations\")\n",
    "#     train_yolo_pipeline(\"datasets\")\n",
    "#     test_model_on_real_images(\"YOLO_Lego_Detection/best.pt\", \"test_images\", \"results\")\n",
    "#     visualize_results(\"datasets\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt conversacion del visualization_presentation.py\n",
    "\n",
    "para actualizar el comentariado luego de la publicacion en PyPl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"\n",
    "\n",
    "Tengo el siguiente script en Python que quiero mejorar para alcanzar un nivel profesional. El script incluye varias funciones relacionadas con la visualizaci√≥n de resultados de modelos y organizaci√≥n de carpetas. Estos son los aspectos espec√≠ficos que quiero mejorar:\n",
    "\n",
    "1. **Comentarios m√°s claros y precisos**:\n",
    "   - Las docstrings deben ser detalladas, siguiendo el est√°ndar Google o NumPy, e incluir:\n",
    "     - Prop√≥sito de la funci√≥n.\n",
    "     - Par√°metros (`Args`).\n",
    "     - Valores de retorno (`Returns`), si aplica.\n",
    "   - Los comentarios en l√≠nea deben ser breves y espec√≠ficos para mejorar la comprensi√≥n del c√≥digo.\n",
    "\n",
    "2. **Formato y organizaci√≥n del c√≥digo**:\n",
    "   - Aplicar el est√°ndar **PEP 8**, como longitud m√°xima de l√≠neas (79 caracteres) y espaciado adecuado.\n",
    "   - Asegurar que las importaciones est√©n organizadas (est√°ndar, externas, locales).\n",
    "\n",
    "3. **Manejo de errores**:\n",
    "   - Proponer o incluir excepciones donde sea relevante, como manejo de archivos o directorios inexistentes.\n",
    "\n",
    "4. **Mensajes m√°s profesionales**:\n",
    "   - Reemplazar mensajes `print()` por logs informativos utilizando la librer√≠a est√°ndar `logging`.\n",
    "\n",
    "5. **Optimizaci√≥n del flujo de funciones**:\n",
    "   - Identificar redundancias o mejoras en la eficiencia de las funciones.\n",
    "\n",
    "6. **Mejoras generales**:\n",
    "   - Proponer o aplicar cualquier ajuste que haga el script m√°s limpio, profesional y eficiente.\n",
    "\n",
    "A continuaci√≥n, te comparto el script para que lo refines seg√∫n estos criterios:\n",
    "\n",
    "\n",
    "Por favor, realiza los ajustes directamente en el c√≥digo y explica las mejoras realizadas para que pueda aprender del proceso.\n",
    "\n",
    "\n",
    "# \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import random\n",
    "# import logging\n",
    "# import matplotlib.pyplot as plt\n",
    "# from PIL import Image\n",
    "# from scripts.pipeline import test_model_on_real_images\n",
    "\n",
    "# # Configuraci√≥n del logger\n",
    "# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# def create_dataset_grid(input_folder: str, output_folder: str, grid_size=(3, 3)) -> None:\n",
    "#     \"\"\"\n",
    "#     Genera un grid de im√°genes de muestra del dataset y lo guarda en la carpeta especificada.\n",
    "\n",
    "#     Args:\n",
    "#         input_folder (str): Ruta de la carpeta con las im√°genes de entrada.\n",
    "#         output_folder (str): Ruta donde se guardar√° el grid generado.\n",
    "#         grid_size (tuple): Dimensiones del grid (filas, columnas).\n",
    "\n",
    "#     Raises:\n",
    "#         FileNotFoundError: Si la carpeta de entrada no existe o est√° vac√≠a.\n",
    "#     \"\"\"\n",
    "#     if not os.path.exists(input_folder):\n",
    "#         logging.error(\"La carpeta de entrada no existe.\")\n",
    "#         raise FileNotFoundError(\"Carpeta de entrada no encontrada.\")\n",
    "\n",
    "#     os.makedirs(output_folder, exist_ok=True)\n",
    "#     image_files = [f for f in os.listdir(input_folder) if f.endswith(('.jpg', '.png'))]\n",
    "#     if not image_files:\n",
    "#         logging.error(\"No se encontraron im√°genes en la carpeta de entrada.\")\n",
    "#         raise FileNotFoundError(\"No se encontraron im√°genes en la carpeta de entrada.\")\n",
    "\n",
    "#     random.shuffle(image_files)\n",
    "#     selected_files = image_files[:grid_size[0] * grid_size[1]]\n",
    "#     fig, axes = plt.subplots(*grid_size, figsize=(grid_size[1] * 4, grid_size[0] * 4))\n",
    "\n",
    "#     for idx, img_file in enumerate(selected_files):\n",
    "#         img_path = os.path.join(input_folder, img_file)\n",
    "#         img = Image.open(img_path)\n",
    "#         row, col = divmod(idx, grid_size[1])\n",
    "#         axes[row, col].imshow(img)\n",
    "#         axes[row, col].axis('off')\n",
    "#         axes[row, col].set_title(f\"{img_file}\", fontsize=10)\n",
    "\n",
    "#     for ax in axes.flat[len(selected_files):]:\n",
    "#         ax.axis('off')\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     grid_path = os.path.join(output_folder, \"dataset_grid.png\")\n",
    "#     plt.savefig(grid_path, bbox_inches='tight')\n",
    "#     plt.close()\n",
    "#     logging.info(f\"Grid de dataset guardado en {grid_path}.\")\n",
    "\n",
    "# def annotate_model_results(model_path: str, input_folder: str, output_folder: str, conf_threshold=0.5) -> None:\n",
    "#     \"\"\"\n",
    "#     Genera im√°genes con anotaciones del modelo y las guarda en la carpeta especificada.\n",
    "\n",
    "#     Args:\n",
    "#         model_path (str): Ruta al modelo YOLO entrenado.\n",
    "#         input_folder (str): Carpeta con im√°genes de entrada.\n",
    "#         output_folder (str): Carpeta para guardar las im√°genes anotadas.\n",
    "#         conf_threshold (float): Umbral de confianza para las detecciones.\n",
    "\n",
    "#     Raises:\n",
    "#         FileNotFoundError: Si la carpeta de entrada no contiene im√°genes.\n",
    "#     \"\"\"\n",
    "#     if not os.path.exists(input_folder):\n",
    "#         logging.error(\"La carpeta de entrada no existe.\")\n",
    "#         raise FileNotFoundError(\"Carpeta de entrada no encontrada.\")\n",
    "\n",
    "#     os.makedirs(output_folder, exist_ok=True)\n",
    "#     test_model_on_real_images(model_path, input_folder, output_folder)\n",
    "#     logging.info(f\"Resultados anotados guardados en {output_folder}.\")\n",
    "\n",
    "# def generate_comparison_grid(model_path: str, input_folder: str, output_folder: str, num_samples=5) -> None:\n",
    "#     \"\"\"\n",
    "#     Crea un grid comparativo de im√°genes antes y despu√©s de las predicciones del modelo.\n",
    "\n",
    "#     Args:\n",
    "#         model_path (str): Ruta al modelo YOLO entrenado.\n",
    "#         input_folder (str): Carpeta con im√°genes de entrada.\n",
    "#         output_folder (str): Carpeta para guardar los grids generados.\n",
    "#         num_samples (int): N√∫mero de im√°genes a comparar.\n",
    "\n",
    "#     Raises:\n",
    "#         FileNotFoundError: Si la carpeta de entrada no contiene im√°genes.\n",
    "#     \"\"\"\n",
    "#     if not os.path.exists(input_folder):\n",
    "#         logging.error(\"La carpeta de entrada no existe.\")\n",
    "#         raise FileNotFoundError(\"Carpeta de entrada no encontrada.\")\n",
    "\n",
    "#     os.makedirs(output_folder, exist_ok=True)\n",
    "#     image_files = [f for f in os.listdir(input_folder) if f.endswith(('.jpg', '.png'))]\n",
    "#     if not image_files:\n",
    "#         logging.error(\"No se encontraron im√°genes en la carpeta de entrada.\")\n",
    "#         raise FileNotFoundError(\"No se encontraron im√°genes en la carpeta de entrada.\")\n",
    "\n",
    "#     random.shuffle(image_files)\n",
    "#     selected_files = image_files[:num_samples]\n",
    "\n",
    "#     for img_file in selected_files:\n",
    "#         img_path = os.path.join(input_folder, img_file)\n",
    "#         result_path = os.path.join(output_folder, img_file)\n",
    "\n",
    "#         test_model_on_real_images(model_path, img_path, output_folder)\n",
    "#         annotated_img = Image.open(result_path)\n",
    "#         original_img = Image.open(img_path)\n",
    "\n",
    "#         fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "#         axes[0].imshow(original_img)\n",
    "#         axes[0].axis('off')\n",
    "#         axes[0].set_title(\"Original\")\n",
    "\n",
    "#         axes[1].imshow(annotated_img)\n",
    "#         axes[1].axis('off')\n",
    "#         axes[1].set_title(\"Anotado\")\n",
    "\n",
    "#         plt.tight_layout()\n",
    "#         comparison_path = os.path.join(output_folder, f\"comparison_{img_file}\")\n",
    "#         plt.savefig(comparison_path, bbox_inches='tight')\n",
    "#         plt.close()\n",
    "#         logging.info(f\"Grid de comparaci√≥n guardado en {comparison_path}.\")\n",
    "\n",
    "# def organize_presentation_folders(base_folder: str) -> None:\n",
    "#     \"\"\"\n",
    "#     Crea y organiza las subcarpetas necesarias para las visualizaciones.\n",
    "\n",
    "#     Args:\n",
    "#         base_folder (str): Carpeta ra√≠z para las subcarpetas.\n",
    "#     \"\"\"\n",
    "#     subfolders = [\"dataset_samples\", \"model_results\", \"before_after\"]\n",
    "#     for subfolder in subfolders:\n",
    "#         os.makedirs(os.path.join(base_folder, subfolder), exist_ok=True)\n",
    "#     logging.info(f\"Subcarpetas creadas en {base_folder}: {', '.join(subfolders)}\")\n",
    "\n",
    "# def main() -> None:\n",
    "#     \"\"\"\n",
    "#     Ejecuci√≥n principal del pipeline de visualizaci√≥n.\n",
    "#     \"\"\"\n",
    "#     base_folder = \"presentation\"\n",
    "#     organize_presentation_folders(base_folder)\n",
    "\n",
    "#     dataset_folder = \"datasets/processed_images\"\n",
    "#     model_path = \"YOLO_Lego_Detection/best.pt\"\n",
    "\n",
    "#     create_dataset_grid(dataset_folder, os.path.join(base_folder, \"dataset_samples\"))\n",
    "#     annotate_model_results(model_path, dataset_folder, os.path.join(base_folder, \"model_results\"))\n",
    "#     generate_comparison_grid(model_path, dataset_folder, os.path.join(base_folder, \"before_after\"))\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt conversacion de refinamiento de README.md\n",
    "\n",
    "para actualizar el comentariado luego de la publicacion en PyPl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"\n",
    "\n",
    "Tengo dos scripts bien comentados y refinados que forman el n√∫cleo de mi proyecto. Tambi√©n tengo un archivo `README.md` existente, pero necesito actualizarlo para reflejar los cambios y documentar de forma profesional el proyecto. Aqu√≠ est√°n las instrucciones:\n",
    "\n",
    "1. **Descripci√≥n clara del proyecto**:\n",
    "   - Explica qu√© resuelve el proyecto y por qu√© es √∫til.\n",
    "   - Destaca las funcionalidades principales basadas en los scripts proporcionados.\n",
    "\n",
    "2. **Estructura profesional**:\n",
    "   - Introducci√≥n: Breve descripci√≥n del proyecto.\n",
    "   - Instalaci√≥n: C√≥mo instalar las dependencias y preparar el entorno.\n",
    "   - Uso: Instrucciones para ejecutar las funcionalidades principales.\n",
    "   - Detalle de scripts y funciones: Documenta cada funci√≥n clave basada en las docstrings de los scripts refinados.\n",
    "   - Contribuciones: Gu√≠a para colaborar.\n",
    "\n",
    "3. **Ejemplos pr√°cticos**:\n",
    "   - Muestra c√≥mo usar funciones principales, con ejemplos claros y f√°ciles de entender en bloques de c√≥digo.\n",
    "\n",
    "4. **Estructura del proyecto**:\n",
    "   - Describe la organizaci√≥n de carpetas y archivos, indicando d√≥nde encontrar scripts, datos y resultados.\n",
    "\n",
    "5. **Consistencia y claridad**:\n",
    "   - El README.md debe ser accesible tanto para principiantes como para desarrolladores avanzados.\n",
    "\n",
    "A continuaci√≥n, tienes los archivos necesarios:\n",
    "1. El estado actual del archivo `README.md`.\n",
    "2. Los dos scripts refinados (`pipeline.py` y `visualize_presentation.py`).\n",
    "\n",
    "Por favor, actualiza el `README.md` para reflejar toda la funcionalidad implementada, utilizando una estructura clara y profesional. Aseg√∫rate de incluir ejemplos pr√°cticos y explicaciones detalladas basadas en las docstrings de los scripts.\n",
    "\n",
    "\n",
    "# \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt para actualizar el technical_report.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# \"\n",
    "\n",
    "Quiero integrar el c√≥digo de mi paquete publicado en PyPI (lego-bricks-ml-vision) en el documento t√©cnico LEGO_Bricks_Identification_Project_Technical_Report.pdf. El objetivo principal es actualizar el documento para que:\n",
    "\n",
    "Utilice las funciones del paquete:\n",
    "\n",
    "Sustituir bloques de c√≥digo existentes por ejemplos claros de c√≥mo usar el paquete.\n",
    "Mostrar c√≥mo el pipeline completo puede ser ejecutado con las herramientas del paquete.\n",
    "Simplifique la reproducci√≥n de la experiencia:\n",
    "\n",
    "Incluir comandos claros para instalar el paquete y ejecutar pipelines directamente desde el CLI (run-pipeline y run-visualize).\n",
    "Agregar ejemplos de configuraci√≥n inicial y manejo de errores.\n",
    "Actualice la estructura del documento:\n",
    "\n",
    "Reorganizar las secciones de \"Creaci√≥n del Dataset\", \"Entrenamiento del Modelo\" y \"Visualizaci√≥n de Resultados\" para reflejar el flujo modular del paquete.\n",
    "Ajustar las secciones de reflexi√≥n para resaltar la escalabilidad y facilidad de uso del paquete.\n",
    "Incluya referencias al README:\n",
    "\n",
    "Citar el README y documentaci√≥n del paquete como recursos adicionales para ampliar la informaci√≥n t√©cnica.\n",
    "Tareas espec√≠ficas:\n",
    "Revise la secci√≥n de introducci√≥n para incorporar una descripci√≥n breve del paquete y c√≥mo facilita el flujo de trabajo.\n",
    "Sustituya el c√≥digo en Python por ejemplos concretos de uso de las funciones del paquete (preprocess_images, train_yolo_pipeline, etc.).\n",
    "Proporcione instrucciones para ejecutar pipelines y generar visualizaciones utilizando los comandos de CLI (run-pipeline y run-visualize).\n",
    "Verifique la claridad y profesionalismo del lenguaje, garantizando un tono t√©cnico accesible.\n",
    "Por favor, comencemos revisando la secci√≥n de \"Creaci√≥n del Dataset\" y trabajemos iterativamente para integrar las funciones del paquete.\n",
    "\n",
    "\n",
    "\n",
    "# \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First iteration technical_report.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LEGO Bricks Identification Project: A Technical Report\n",
    "\n",
    "### 1. Introduction and Motivation üåü\n",
    "\n",
    "This project leverages the **`lego-bricks-ml-vision`** package to implement an efficient pipeline for detecting and classifying LEGO bricks. Inspired by the challenge of identifying specific pieces within a cluttered set of LEGO bricks, this project combines computer vision, machine learning, and dataset management to achieve scalable and replicable results.\n",
    "\n",
    "The key objectives include:\n",
    "\n",
    "1. Designing a pipeline for object detection using YOLOv8.\n",
    "2. Documenting the process to ensure reproducibility and scalability.\n",
    "3. Providing tools for visualization and analysis to showcase the model‚Äôs performance.\n",
    "\n",
    "### 2. Dataset Creation üìù\n",
    "\n",
    "The dataset creation process is streamlined using the **`lego-bricks-ml-vision`** package. This package provides commands for downloading datasets, preprocessing images, and converting annotations.\n",
    "\n",
    "#### 2.1 Dataset Overview \n",
    "\n",
    "The dataset used for this project is hosted on Kaggle:\n",
    "- **Dataset Name**: [Spiled LEGO Bricks](https://www.kaggle.com/datasets/migueldilalla/spiled-lego-bricks)\n",
    "- **Contents**:\n",
    "  - **Images**: 1803 images (600√ó800 resolution)\n",
    "  - **Annotations**: LabelMe-compatible `.txt` files with bounding box data.\n",
    "\n",
    "#### 2.2 Setting Up the Environment \n",
    "\n",
    "Install the **`lego-bricks-ml-vision`** package from PyPI:\n",
    "\n",
    "```python\n",
    "!pip install lego-bricks-ml-vision\n",
    "```\n",
    "\n",
    "Configure the pipeline:\n",
    "\n",
    "```python\n",
    "from lego_bricks_ml_vision import setup_environment\n",
    "setup_environment()\n",
    "```\n",
    "\n",
    "This command ensures that all dependencies are installed and that your environment is ready.\n",
    "\n",
    "#### 2.3 Downloading the Dataset \n",
    "\n",
    "The dataset can be directly downloaded and extracted using the following commands:\n",
    "\n",
    "```python\n",
    "!run-pipeline download-dataset \\\n",
    "    --kaggle-dataset \"migueldilalla/spiled-lego-bricks\" \\\n",
    "    --output-dir \"datasets\"\n",
    "```\n",
    "\n",
    "#### 2.4 Preprocessing Images \n",
    "\n",
    "Resize the images to a consistent size (e.g., 256x256) for model training:\n",
    "\n",
    "```python\n",
    "!run-pipeline preprocess-images \\\n",
    "    --input-dir \"datasets/Images_600x800\" \\\n",
    "    --output-dir \"datasets/processed_images\" \\\n",
    "    --target-size 256\n",
    "```\n",
    "\n",
    "#### 2.5 Converting Annotations \n",
    "\n",
    "Convert annotations from LabelMe format to YOLO format:\n",
    "\n",
    "```python\n",
    "!run-pipeline labelme-to-yolo \\\n",
    "    --input-folder \"datasets/LabelMe_txt_bricks\" \\\n",
    "    --output-folder \"datasets/annotations\"\n",
    "```\n",
    "\n",
    "By automating these tasks, the package ensures consistency and reduces manual effort.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Model Training üß¨\n",
    "\n",
    "The project uses YOLOv8 for LEGO brick detection. Training is performed using the preprocessed dataset and YOLO-compatible annotations.\n",
    "\n",
    "#### 3.1 Training the YOLO Model\n",
    "\n",
    "The `train_yolo_pipeline` function enables straightforward model training:\n",
    "\n",
    "```python\n",
    "!run-pipeline train-yolo \\\n",
    "    --dataset-path \"datasets\" \\\n",
    "    --epochs 50 \\\n",
    "    --img-size 256\n",
    "```\n",
    "\n",
    "#### 3.2 Validating the Model\n",
    "\n",
    "Evaluate the trained model on test images:\n",
    "\n",
    "```python\n",
    "!run-pipeline test-model \\\n",
    "    --model-path \"YOLO_Lego_Detection/best.pt\" \\\n",
    "    --test-images-dir \"test_images\" \\\n",
    "    --output-dir \"results\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Visualization and Results üé®\n",
    "\n",
    "#### 4.1 Visualizing Results\n",
    "\n",
    "Visualize predictions and annotations using:\n",
    "\n",
    "```python\n",
    "!run-visualize annotate-results \\\n",
    "    --model-path \"YOLO_Lego_Detection/best.pt\" \\\n",
    "    --input-folder \"datasets/processed_images\" \\\n",
    "    --output-folder \"presentation/model_results\"\n",
    "```\n",
    "\n",
    "#### 4.2 Generating Comparison Grids\n",
    "\n",
    "Compare predictions against ground truth:\n",
    "\n",
    "```python\n",
    "!run-visualize comparison-grid \\\n",
    "    --model-path \"YOLO_Lego_Detection/best.pt\" \\\n",
    "    --input-folder \"datasets/processed_images\" \\\n",
    "    --output-folder \"presentation/comparison\"\n",
    "```\n",
    "\n",
    "#### 4.3 Creating Presentation Grids\n",
    "\n",
    "Summarize dataset samples in a grid format:\n",
    "\n",
    "```python\n",
    "!run-visualize create-grid \\\n",
    "    --input-folder \"datasets/processed_images\" \\\n",
    "    --output-folder \"presentation/dataset_samples\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Reflection and Future Work üîÑ\n",
    "\n",
    "This project demonstrates the effectiveness of modular pipelines for scalable machine learning workflows. Key insights include:\n",
    "\n",
    "- **Data Quality**: High-quality, annotated datasets significantly enhance model performance.\n",
    "- **Modularity**: Breaking down the pipeline into distinct stages improves reproducibility.\n",
    "- **Visualization**: Effective visual tools aid in debugging and communicating results.\n",
    "\n",
    "#### Future Improvements:\n",
    "1. Expanding the dataset to include more LEGO pieces.\n",
    "2. Integrating semi-automated annotation tools.\n",
    "3. Developing an interactive interface for real-time predictions.\n",
    "\n",
    "---\n",
    "\n",
    "For more details, refer to the [LEGO Bricks ML Vision Documentation](https://github.com/MiguelDiLalla/LEGO_Bricks_ML_Vision).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook saved to: C:\\Users\\User\\Projects_Unprotected\\LEGO_Bricks_ML_Vision\\notebooks\\Technical_Report.ipynb\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nbformat as nbf\n",
    "\n",
    "# Paths for input and output\n",
    "input_md_path = r\"C:\\Users\\User\\Projects_Unprotected\\LEGO_Bricks_ML_Vision\\notebooks\\Technical_Report.md\"\n",
    "output_nb_path = r\"C:\\Users\\User\\Projects_Unprotected\\LEGO_Bricks_ML_Vision\\notebooks\\Technical_Report.ipynb\"\n",
    "\n",
    "# Read the markdown file\n",
    "with open(input_md_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Initialize a new notebook\n",
    "notebook = nbf.v4.new_notebook()\n",
    "\n",
    "# Process the markdown file line by line\n",
    "current_cell_type = \"markdown\"\n",
    "current_cell_content = []\n",
    "\n",
    "for line in lines:\n",
    "    # Detect code block start or end\n",
    "    if line.strip().startswith(\"```\"):  # Toggle between markdown and code\n",
    "        if current_cell_type == \"code\":\n",
    "            # End of a code cell\n",
    "            notebook.cells.append(nbf.v4.new_code_cell(\"\".join(current_cell_content)))\n",
    "            current_cell_content = []\n",
    "            current_cell_type = \"markdown\"\n",
    "        else:\n",
    "            # End of a markdown cell\n",
    "            if current_cell_content:\n",
    "                notebook.cells.append(nbf.v4.new_markdown_cell(\"\".join(current_cell_content)))\n",
    "            current_cell_content = []\n",
    "            current_cell_type = \"code\"\n",
    "    else:\n",
    "        # Append content to the current cell\n",
    "        current_cell_content.append(line)\n",
    "\n",
    "# Add any remaining content as a final cell\n",
    "if current_cell_content:\n",
    "    if current_cell_type == \"markdown\":\n",
    "        notebook.cells.append(nbf.v4.new_markdown_cell(\"\".join(current_cell_content)))\n",
    "    elif current_cell_type == \"code\":\n",
    "        notebook.cells.append(nbf.v4.new_code_cell(\"\".join(current_cell_content)))\n",
    "\n",
    "# Save the notebook\n",
    "nbf.write(notebook, output_nb_path)\n",
    "print(f\"Notebook saved to: {output_nb_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to publish on PyPl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Creating isolated environment: venv+pip...\n",
      "* Installing packages in isolated environment:\n",
      "  - setuptools>=64\n",
      "  - wheel\n",
      "* Getting build dependencies for sdist...\n",
      "running egg_info\n",
      "writing lego_bricks_ml_vision.egg-info\\PKG-INFO\n",
      "writing dependency_links to lego_bricks_ml_vision.egg-info\\dependency_links.txt\n",
      "writing entry points to lego_bricks_ml_vision.egg-info\\entry_points.txt\n",
      "writing requirements to lego_bricks_ml_vision.egg-info\\requires.txt\n",
      "writing top-level names to lego_bricks_ml_vision.egg-info\\top_level.txt\n",
      "reading manifest file 'lego_bricks_ml_vision.egg-info\\SOURCES.txt'\n",
      "reading manifest template 'MANIFEST.in'\n",
      "adding license file 'LICENSE'\n",
      "writing manifest file 'lego_bricks_ml_vision.egg-info\\SOURCES.txt'\n",
      "* Building sdist...\n",
      "running sdist\n",
      "running egg_info\n",
      "writing lego_bricks_ml_vision.egg-info\\PKG-INFO\n",
      "writing dependency_links to lego_bricks_ml_vision.egg-info\\dependency_links.txt\n",
      "writing entry points to lego_bricks_ml_vision.egg-info\\entry_points.txt\n",
      "writing requirements to lego_bricks_ml_vision.egg-info\\requires.txt\n",
      "writing top-level names to lego_bricks_ml_vision.egg-info\\top_level.txt\n",
      "reading manifest file 'lego_bricks_ml_vision.egg-info\\SOURCES.txt'\n",
      "reading manifest template 'MANIFEST.in'\n",
      "adding license file 'LICENSE'\n",
      "writing manifest file 'lego_bricks_ml_vision.egg-info\\SOURCES.txt'\n",
      "running check\n",
      "creating lego_bricks_ml_vision-0.1.31\n",
      "creating lego_bricks_ml_vision-0.1.31\\lego_bricks_ml_vision.egg-info\n",
      "creating lego_bricks_ml_vision-0.1.31\\scripts\n",
      "copying files to lego_bricks_ml_vision-0.1.31...\n",
      "copying LICENSE -> lego_bricks_ml_vision-0.1.31\n",
      "copying MANIFEST.in -> lego_bricks_ml_vision-0.1.31\n",
      "copying README.md -> lego_bricks_ml_vision-0.1.31\n",
      "copying pyproject.toml -> lego_bricks_ml_vision-0.1.31\n",
      "copying requirements.txt -> lego_bricks_ml_vision-0.1.31\n",
      "copying setup.py -> lego_bricks_ml_vision-0.1.31\n",
      "copying lego_bricks_ml_vision.egg-info\\PKG-INFO -> lego_bricks_ml_vision-0.1.31\\lego_bricks_ml_vision.egg-info\n",
      "copying lego_bricks_ml_vision.egg-info\\SOURCES.txt -> lego_bricks_ml_vision-0.1.31\\lego_bricks_ml_vision.egg-info\n",
      "copying lego_bricks_ml_vision.egg-info\\dependency_links.txt -> lego_bricks_ml_vision-0.1.31\\lego_bricks_ml_vision.egg-info\n",
      "copying lego_bricks_ml_vision.egg-info\\entry_points.txt -> lego_bricks_ml_vision-0.1.31\\lego_bricks_ml_vision.egg-info\n",
      "copying lego_bricks_ml_vision.egg-info\\requires.txt -> lego_bricks_ml_vision-0.1.31\\lego_bricks_ml_vision.egg-info\n",
      "copying lego_bricks_ml_vision.egg-info\\top_level.txt -> lego_bricks_ml_vision-0.1.31\\lego_bricks_ml_vision.egg-info\n",
      "copying scripts\\__init__.py -> lego_bricks_ml_vision-0.1.31\\scripts\n",
      "copying scripts\\pipeline.py -> lego_bricks_ml_vision-0.1.31\\scripts\n",
      "copying scripts\\visualize_presentation.py -> lego_bricks_ml_vision-0.1.31\\scripts\n",
      "copying lego_bricks_ml_vision.egg-info\\SOURCES.txt -> lego_bricks_ml_vision-0.1.31\\lego_bricks_ml_vision.egg-info\n",
      "Writing lego_bricks_ml_vision-0.1.31\\setup.cfg\n",
      "Creating tar archive\n",
      "removing 'lego_bricks_ml_vision-0.1.31' (and everything under it)\n",
      "* Building wheel from sdist\n",
      "* Creating isolated environment: venv+pip...\n",
      "* Installing packages in isolated environment:\n",
      "  - setuptools>=64\n",
      "  - wheel\n",
      "* Getting build dependencies for wheel...\n",
      "running egg_info\n",
      "writing lego_bricks_ml_vision.egg-info\\PKG-INFO\n",
      "writing dependency_links to lego_bricks_ml_vision.egg-info\\dependency_links.txt\n",
      "writing entry points to lego_bricks_ml_vision.egg-info\\entry_points.txt\n",
      "writing requirements to lego_bricks_ml_vision.egg-info\\requires.txt\n",
      "writing top-level names to lego_bricks_ml_vision.egg-info\\top_level.txt\n",
      "reading manifest file 'lego_bricks_ml_vision.egg-info\\SOURCES.txt'\n",
      "reading manifest template 'MANIFEST.in'\n",
      "adding license file 'LICENSE'\n",
      "writing manifest file 'lego_bricks_ml_vision.egg-info\\SOURCES.txt'\n",
      "* Building wheel...\n",
      "running bdist_wheel\n",
      "running build\n",
      "running build_py\n",
      "creating build\\lib\\scripts\n",
      "copying scripts\\pipeline.py -> build\\lib\\scripts\n",
      "copying scripts\\visualize_presentation.py -> build\\lib\\scripts\n",
      "copying scripts\\__init__.py -> build\\lib\\scripts\n",
      "installing to build\\bdist.win-amd64\\wheel\n",
      "running install\n",
      "running install_lib\n",
      "creating build\\bdist.win-amd64\\wheel\n",
      "creating build\\bdist.win-amd64\\wheel\\scripts\n",
      "copying build\\lib\\scripts\\pipeline.py -> build\\bdist.win-amd64\\wheel\\.\\scripts\n",
      "copying build\\lib\\scripts\\visualize_presentation.py -> build\\bdist.win-amd64\\wheel\\.\\scripts\n",
      "copying build\\lib\\scripts\\__init__.py -> build\\bdist.win-amd64\\wheel\\.\\scripts\n",
      "running install_egg_info\n",
      "running egg_info\n",
      "writing lego_bricks_ml_vision.egg-info\\PKG-INFO\n",
      "writing dependency_links to lego_bricks_ml_vision.egg-info\\dependency_links.txt\n",
      "writing entry points to lego_bricks_ml_vision.egg-info\\entry_points.txt\n",
      "writing requirements to lego_bricks_ml_vision.egg-info\\requires.txt\n",
      "writing top-level names to lego_bricks_ml_vision.egg-info\\top_level.txt\n",
      "reading manifest file 'lego_bricks_ml_vision.egg-info\\SOURCES.txt'\n",
      "reading manifest template 'MANIFEST.in'\n",
      "adding license file 'LICENSE'\n",
      "writing manifest file 'lego_bricks_ml_vision.egg-info\\SOURCES.txt'\n",
      "Copying lego_bricks_ml_vision.egg-info to build\\bdist.win-amd64\\wheel\\.\\lego_bricks_ml_vision-0.1.31-py3.12.egg-info\n",
      "running install_scripts\n",
      "creating build\\bdist.win-amd64\\wheel\\lego_bricks_ml_vision-0.1.31.dist-info\\WHEEL\n",
      "creating 'c:\\Users\\User\\Projects_Unprotected\\LEGO_Bricks_ML_Vision\\dist\\.tmp-qag61j__\\lego_bricks_ml_vision-0.1.31-py3-none-any.whl' and adding 'build\\bdist.win-amd64\\wheel' to it\n",
      "adding 'scripts/__init__.py'\n",
      "adding 'scripts/pipeline.py'\n",
      "adding 'scripts/visualize_presentation.py'\n",
      "adding 'lego_bricks_ml_vision-0.1.31.dist-info/LICENSE'\n",
      "adding 'lego_bricks_ml_vision-0.1.31.dist-info/METADATA'\n",
      "adding 'lego_bricks_ml_vision-0.1.31.dist-info/WHEEL'\n",
      "adding 'lego_bricks_ml_vision-0.1.31.dist-info/entry_points.txt'\n",
      "adding 'lego_bricks_ml_vision-0.1.31.dist-info/top_level.txt'\n",
      "adding 'lego_bricks_ml_vision-0.1.31.dist-info/RECORD'\n",
      "removing build\\bdist.win-amd64\\wheel\n",
      "Successfully built lego_bricks_ml_vision-0.1.31.tar.gz and lego_bricks_ml_vision-0.1.31-py3-none-any.whl\n"
     ]
    }
   ],
   "source": [
    "!python -m build\n",
    "\n",
    "!python -m twine upload dist/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pipeline_setup.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Detected Environment': 'local'}\n",
      "\n",
      "[INFO] Entorno detectado: Local\n",
      "\n",
      "{'Rutas Configuradas': {'output_path': 'c:\\\\Users\\\\User\\\\Projects_Unprotected\\\\LEGO_Bricks_ML_Vision\\\\notebooks\\\\working\\\\output',\n",
      "                        'raw_images_path': 'working/spiled-lego-bricks\\\\Images_600x800',\n",
      "                        'raw_labels_path': 'working/spiled-lego-bricks\\\\LabelMe_txt_bricks'}}\n",
      "{'Dataset Estructura': {'working/spiled-lego-bricks\\\\Images_600x800': 1803,\n",
      "                        'working/spiled-lego-bricks\\\\LabelMe_txt_bricks': 1803}}\n",
      "[INFO] Estructura de carpetas creada en c:\\Users\\User\\Projects_Unprotected\\LEGO_Bricks_ML_Vision\\notebooks\\working\\output.\n",
      "{'Partici√≥n Completada': {'test': 179, 'train': 1262, 'val': 362}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\.conda\\envs\\MiguelEnvHaB\\Lib\\site-packages\\albumentations\\core\\validation.py:45: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
      "  original_init(self, **validated_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Augmented data saved to c:\\Users\\User\\Projects_Unprotected\\LEGO_Bricks_ML_Vision\\notebooks\\working\\output\\augmented_dataset.\n",
      "[INFO] Augmented data merged into train set at c:\\Users\\User\\Projects_Unprotected\\LEGO_Bricks_ML_Vision\\notebooks\\working\\output.\n",
      "[INFO] dataset.yaml created at: c:\\Users\\User\\Projects_Unprotected\\LEGO_Bricks_ML_Vision\\notebooks\\working\\output\\dataset\\dataset.yaml\n",
      "{'Validaci√≥n Final': {'test': 179, 'train': 5048, 'val': 362}}\n",
      "\n",
      "[INFO] Pipeline setup completed with augmentations and dataset.yaml creation.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "from pprint import pprint\n",
    "import albumentations as A\n",
    "import cv2\n",
    "import yaml\n",
    "import sys\n",
    "\n",
    "# === Configuraci√≥n Inicial ===\n",
    "def detect_environment():\n",
    "    \"\"\"\n",
    "    Detecta el entorno de ejecuci√≥n (Kaggle, Google Colab o Local).\n",
    "\n",
    "    Returns:\n",
    "    - str: Nombre del entorno detectado.\n",
    "    \"\"\"\n",
    "    if \"google.colab\" in sys.modules:\n",
    "        environment = \"colab\"\n",
    "    elif os.path.exists(\"/kaggle\"):\n",
    "        environment = \"kaggle\"\n",
    "    else:\n",
    "        environment = \"local\"\n",
    "    pprint({\"Detected Environment\": environment})\n",
    "    return environment\n",
    "\n",
    "\n",
    "def setup_environment(base_path=\"/kaggle/working/output\"):\n",
    "    \"\"\"\n",
    "    Configura el entorno seg√∫n el sistema detectado y prepara el dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - base_path (str): Carpeta base donde se configurar√° la salida.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Rutas configuradas para las im√°genes y etiquetas crudas.\n",
    "    \"\"\"\n",
    "    environment = detect_environment()\n",
    "    print(f\"\\n[INFO] Entorno detectado: {environment.capitalize()}\\n\")\n",
    "\n",
    "    if environment == \"kaggle\":\n",
    "        dataset_path = \"/kaggle/input/spiled-lego-bricks\"\n",
    "        required_folders = [\"Images_600x800\", \"LabelMe_txt_bricks\"]\n",
    "        for folder in required_folders:\n",
    "            full_path = os.path.join(dataset_path, folder)\n",
    "            if not os.path.exists(full_path):\n",
    "                raise FileNotFoundError(f\"[ERROR] Carpeta requerida no encontrada: {full_path}\")\n",
    "            print(f\"[INFO] Carpeta verificada: {full_path}\")\n",
    "\n",
    "        return {\n",
    "            \"raw_images_path\": os.path.join(dataset_path, \"Images_600x800\"),\n",
    "            \"raw_labels_path\": os.path.join(dataset_path, \"LabelMe_txt_bricks\"),\n",
    "            \"output_path\": base_path\n",
    "        }\n",
    "    elif environment == \"colab\":\n",
    "            from google.colab import userdata\n",
    "            kaggle_path = \"kaggle.json\"\n",
    "            if not os.path.exists(kaggle_path):\n",
    "                # raise EnvironmentError(\"[ERROR] Sube tu archivo kaggle.json al entorno Colab en /root/.kaggle/\")\n",
    "                os.makedirs(\"/root/.kaggle\", exist_ok=True)\n",
    "            \n",
    "            \n",
    "                kaggle_user = userdata.get('KaggleUser')\n",
    "                kaggle_token = userdata.get('KaggleToken')\n",
    "                if not kaggle_user or not kaggle_token:\n",
    "                    raise EnvironmentError(\"[ERROR] No se encontraron las credenciales de Kaggle en Google Colab.\")\n",
    "                kaggle_data = {\n",
    "                    \"username\": kaggle_user,\n",
    "                    \"key\": kaggle_token\n",
    "                }\n",
    "                with open(\"/root/.kaggle/kaggle.json\", \"w\") as f:\n",
    "                    json.dump(kaggle_data, f)\n",
    "                    print(\"[INFO] Credenciales de Kaggle configuradas en Google Colab.\")\n",
    "            else:\n",
    "                os.makedirs(\"/root/.kaggle\", exist_ok=True)\n",
    "                shutil.move(kaggle_path, \"/root/.kaggle/kaggle.json\")\n",
    "                print(\"[INFO] Archivo kaggle.json movido a /root/.kaggle/\")\n",
    "            os.chmod(\"/root/.kaggle/kaggle.json\", 0o600)\n",
    "            os.makedirs(\"working\", exist_ok=True)\n",
    "            os.makedirs(\"working/spiled-lego-bricks\", exist_ok=True)\n",
    "            os.system(\"kaggle datasets download -d migueldilalla/spiled-lego-bricks -p working/spiled-lego-bricks --unzip\")\n",
    "            os.makedirs(\"/working/output\", exist_ok=True)\n",
    "            dataset_path = \"working/spiled-lego-bricks\"\n",
    "\n",
    "            return {\n",
    "                \"raw_images_path\": os.path.join(dataset_path, \"Images_600x800\"),\n",
    "                \"raw_labels_path\": os.path.join(dataset_path, \"LabelMe_txt_bricks\"),\n",
    "                \"output_path\": os.path.join(os.getcwd(), \"working\", \"output\")\n",
    "            }\n",
    "\n",
    "\n",
    "    elif environment == \"local\":\n",
    "        kaggle_json_path = os.path.expanduser(\"~/.kaggle/kaggle.json\")\n",
    "        if not os.path.exists(kaggle_json_path):\n",
    "            raise EnvironmentError(\"[ERROR] Archivo kaggle.json no encontrado en ~/.kaggle/\")\n",
    "        os.makedirs(\"working\", exist_ok=True)\n",
    "        os.makedirs(\"working/spiled-lego-bricks\", exist_ok=True)\n",
    "        if not os.listdir(\"working/spiled-lego-bricks\"):\n",
    "            os.system(\"kaggle datasets download -d migueldilalla/spiled-lego-bricks -p working/spiled-lego-bricks --unzip\")\n",
    "        os.makedirs(\"working/output\", exist_ok=True)\n",
    "        dataset_path = \"working/spiled-lego-bricks\"\n",
    "\n",
    "        return {\n",
    "             \"raw_images_path\": os.path.join(dataset_path, \"Images_600x800\"),\n",
    "            \"raw_labels_path\": os.path.join(dataset_path, \"LabelMe_txt_bricks\"),\n",
    "            \"output_path\": os.path.join(os.getcwd(), \"working\", \"output\")\n",
    "        }\n",
    "    else:\n",
    "        while True:\n",
    "            user_input = input(\"[PROMPT] No se detect√≥ un entorno. Por favor, escribe 'k' para Kaggle, 'g' para Google Colab, o 'l' para Local: \").strip().lower()\n",
    "            if user_input in [\"k\", \"g\", \"l\"]:\n",
    "                return setup_environment_custom(user_input, base_path)\n",
    "            print(\"[ERROR] Entrada inv√°lida. Intenta nuevamente.\")\n",
    "\n",
    "def setup_environment_custom(choice, base_path):\n",
    "    \"\"\"\n",
    "    Configura el entorno manualmente basado en la elecci√≥n del usuario.\n",
    "\n",
    "    Parameters:\n",
    "    - choice (str): 'k' para Kaggle, 'g' para Colab, 'l' para Local.\n",
    "    - base_path (str): Ruta base para la salida.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Rutas configuradas para las im√°genes y etiquetas crudas.\n",
    "    \"\"\"\n",
    "    if choice == \"k\":\n",
    "        return setup_environment()\n",
    "    elif choice == \"g\":\n",
    "        return setup_environment(base_path=\"working\")\n",
    "    elif choice == \"l\":\n",
    "        return setup_environment(base_path=\"working\")\n",
    "    else:\n",
    "        raise EnvironmentError(\"[ERROR] Configuraci√≥n desconocida.\")\n",
    "\n",
    "def verify_dataset_structure(raw_images_path, raw_labels_path):\n",
    "    \"\"\"\n",
    "    Verifica la existencia de las carpetas requeridas en el dataset y muestra estad√≠sticas iniciales.\n",
    "\n",
    "    Parameters:\n",
    "    - raw_images_path (str): Ruta a las im√°genes crudas.\n",
    "    - raw_labels_path (str): Ruta a las etiquetas crudas.\n",
    "    \"\"\"\n",
    "    required_folders = [raw_images_path, raw_labels_path]\n",
    "    summary = {}\n",
    "    for folder in required_folders:\n",
    "        if not os.path.exists(folder):\n",
    "            raise FileNotFoundError(f\"[ERROR] Carpeta requerida no encontrada: {folder}\")\n",
    "\n",
    "        num_files = len([f for f in os.listdir(folder) if os.path.isfile(os.path.join(folder, f))])\n",
    "        if num_files == 0:\n",
    "            raise ValueError(f\"[ERROR] La carpeta {folder} est√° vac√≠a.\")\n",
    "        summary[folder] = num_files\n",
    "\n",
    "    pprint({\"Dataset Estructura\": summary})\n",
    "\n",
    "def create_preprocessing_structure(output_dir=\"/kaggle/working/output\"):\n",
    "    \"\"\"\n",
    "    Crea la estructura de carpetas para PREPROCESSING/.\n",
    "\n",
    "    Parameters:\n",
    "    - output_dir (str): Ruta base para la carpeta PREPROCESSING/.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    subfolders = [\n",
    "        \"dataset/images/train\", \"dataset/images/val\", \"dataset/images/test\",\n",
    "        \"dataset/labels/train\", \"dataset/labels/val\", \"dataset/labels/test\",\n",
    "        \"test_images\"\n",
    "    ]\n",
    "    for subfolder in subfolders:\n",
    "        os.makedirs(os.path.join(output_dir, subfolder), exist_ok=True)\n",
    "    print(f\"[INFO] Estructura de carpetas creada en {output_dir}.\")\n",
    "\n",
    "def copy_and_partition_data(input_images, input_labels, output_dir):\n",
    "    \"\"\"\n",
    "    Copia im√°genes y etiquetas a las carpetas correspondientes y realiza la partici√≥n de datos.\n",
    "\n",
    "    Parameters:\n",
    "    - input_images (str): Carpeta de im√°genes de entrada.\n",
    "    - input_labels (str): Carpeta de etiquetas de entrada.\n",
    "    - output_dir (str): Carpeta base para PREPROCESSING/.\n",
    "    \"\"\"\n",
    "    images = sorted([f for f in os.listdir(input_images) if f.endswith(\".jpg\")])\n",
    "    labels = sorted([f for f in os.listdir(input_labels) if f.endswith(\".txt\")])\n",
    "\n",
    "    if len(images) != len(labels):\n",
    "        raise ValueError(\"[ERROR] N√∫mero de im√°genes y etiquetas no coincide.\")\n",
    "\n",
    "    image_paths = [os.path.join(input_images, img) for img in images]\n",
    "    label_paths = [os.path.join(input_labels, lbl) for lbl in labels]\n",
    "\n",
    "    train_imgs, temp_imgs, train_lbls, temp_lbls = train_test_split(image_paths, label_paths, test_size=0.3, random_state=42)\n",
    "    val_imgs, test_imgs, val_lbls, test_lbls = train_test_split(temp_imgs, temp_lbls, test_size=0.33, random_state=42)\n",
    "\n",
    "    partitions = {\n",
    "        \"train\": (train_imgs, train_lbls),\n",
    "        \"val\": (val_imgs, val_lbls),\n",
    "        \"test\": (test_imgs, test_lbls)\n",
    "    }\n",
    "\n",
    "    for partition, (imgs, lbls) in partitions.items():\n",
    "        for img, lbl in zip(imgs, lbls):\n",
    "            shutil.copy(img, os.path.join(output_dir, f\"dataset/images/{partition}/\"))\n",
    "            shutil.copy(lbl, os.path.join(output_dir, f\"dataset/labels/{partition}/\"))\n",
    "\n",
    "    pprint({\"Partici√≥n Completada\": {partition: len(imgs) for partition, (imgs, _) in partitions.items()}})\n",
    "\n",
    "def augment_data(input_images, input_labels, output_dir, num_augmentations=2):\n",
    "    \"\"\"\n",
    "    Aplica aumentaciones al dataset y guarda im√°genes y etiquetas aumentadas.\n",
    "\n",
    "    Parameters:\n",
    "    - input_images (str): Carpeta de im√°genes originales.\n",
    "    - input_labels (str): Carpeta de etiquetas en formato YOLO.\n",
    "    - output_dir (str): Carpeta donde se guardar√°n los datos aumentados.\n",
    "    - num_augmentations (int): N√∫mero de versiones aumentadas por imagen.\n",
    "    \"\"\"\n",
    "    aug_images_dir = os.path.join(output_dir, \"augmented_images\")\n",
    "    aug_labels_dir = os.path.join(output_dir, \"augmented_labels\")\n",
    "    os.makedirs(aug_images_dir, exist_ok=True)\n",
    "    os.makedirs(aug_labels_dir, exist_ok=True)\n",
    "\n",
    "    transform = A.Compose([\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.RandomBrightnessContrast(p=0.2),\n",
    "        A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),\n",
    "        A.Resize(height=640, width=640),\n",
    "    ], bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels']))\n",
    "\n",
    "    images = sorted([f for f in os.listdir(input_images) if f.endswith(\".jpg\")])\n",
    "    for img_file in images:\n",
    "        img_path = os.path.join(input_images, img_file)\n",
    "        label_path = os.path.join(input_labels, img_file.replace(\".jpg\", \".txt\"))\n",
    "\n",
    "        if not os.path.exists(label_path):\n",
    "            continue\n",
    "\n",
    "        image = cv2.imread(img_path)\n",
    "        bboxes, class_labels = load_labels(label_path)\n",
    "\n",
    "        for i in range(num_augmentations):\n",
    "            augmented = transform(image=image, bboxes=bboxes, class_labels=class_labels)\n",
    "            aug_image = augmented[\"image\"]\n",
    "            aug_bboxes = augmented[\"bboxes\"]\n",
    "            aug_labels = augmented[\"class_labels\"]\n",
    "\n",
    "            aug_image_path = os.path.join(aug_images_dir, f\"{img_file.split('.')[0]}_aug{i}.jpg\")\n",
    "            cv2.imwrite(aug_image_path, aug_image)\n",
    "\n",
    "            aug_label_path = os.path.join(aug_labels_dir, f\"{img_file.split('.')[0]}_aug{i}.txt\")\n",
    "            save_labels(aug_label_path, aug_bboxes, aug_labels)\n",
    "\n",
    "    print(f\"[INFO] Augmented data saved to {output_dir}.\")\n",
    "\n",
    "def load_labels(label_path):\n",
    "    \"\"\"\n",
    "    Carga etiquetas en formato YOLO desde un archivo .txt.\n",
    "\n",
    "    Parameters:\n",
    "    - label_path (str): Ruta al archivo de etiquetas en formato YOLO.\n",
    "\n",
    "    Returns:\n",
    "    - bboxes (list): Lista de bounding boxes en formato YOLO.\n",
    "    - class_labels (list): Lista de etiquetas de clase.\n",
    "    \"\"\"\n",
    "    bboxes, class_labels = [], []\n",
    "    with open(label_path, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        class_id, x_center, y_center, width, height = map(float, line.strip().split())\n",
    "        bboxes.append([x_center, y_center, width, height])\n",
    "        class_labels.append(int(class_id))\n",
    "    return bboxes, class_labels\n",
    "\n",
    "def save_labels(output_path, bboxes, class_labels):\n",
    "    \"\"\"\n",
    "    Guarda etiquetas en formato YOLO en un archivo .txt.\n",
    "\n",
    "    Parameters:\n",
    "    - output_path (str): Ruta donde se guardar√° el archivo de etiquetas.\n",
    "    - bboxes (list): Lista de bounding boxes en formato YOLO.\n",
    "    - class_labels (list): Lista de etiquetas de clase.\n",
    "    \"\"\"\n",
    "    with open(output_path, \"w\") as f:\n",
    "        for bbox, label in zip(bboxes, class_labels):\n",
    "            f.write(f\"{label} {' '.join(map(str, bbox))}\\n\")\n",
    "\n",
    "\n",
    "def copy_augmented_to_train(augmented_dir, output_path):\n",
    "    \"\"\"\n",
    "    Copia los datos aumentados a las subcarpetas correspondientes de 'train'.\n",
    "\n",
    "    Parameters:\n",
    "    - augmented_dir (str): Directorio que contiene im√°genes y etiquetas aumentadas.\n",
    "    - output_path(str): Ruta base para la salida.\n",
    "    \"\"\"\n",
    "    aug_images_dir = os.path.join(augmented_dir, \"augmented_images\")\n",
    "    aug_labels_dir = os.path.join(augmented_dir, \"augmented_labels\")\n",
    "    train_images_dir = os.path.join(output_path, \"dataset/images/train\")\n",
    "    train_labels_dir = os.path.join(output_path, \"dataset/labels/train\")\n",
    "\n",
    "    for img_file in os.listdir(aug_images_dir):\n",
    "        shutil.copy(os.path.join(aug_images_dir, img_file), train_images_dir)\n",
    "\n",
    "    for label_file in os.listdir(aug_labels_dir):\n",
    "        shutil.copy(os.path.join(aug_labels_dir, label_file), train_labels_dir)\n",
    "\n",
    "    print(f\"[INFO] Augmented data merged into train set at {output_path}.\")\n",
    "\n",
    "def create_dataset_yaml(output_path, num_classes, class_names):\n",
    "    \"\"\"\n",
    "    Creates a dataset.yaml file with absolute paths for YOLO training.\n",
    "\n",
    "    Parameters:\n",
    "    - output_path (str): Base directory where the dataset.yaml file will be saved.\n",
    "    - num_classes (int): Total number of classes.\n",
    "    - class_names (list): List of class names.\n",
    "    \"\"\"\n",
    "    # Resolve absolute paths for train and val folders\n",
    "    dataset_dir = os.path.abspath(output_path)\n",
    "    train_path = os.path.join(dataset_dir, \"images/train\")\n",
    "    val_path = os.path.join(dataset_dir, \"images/val\")\n",
    "\n",
    "    # Create the dataset configuration dictionary\n",
    "    dataset_config = {\n",
    "        \"path\": dataset_dir,\n",
    "        \"train\": train_path,\n",
    "        \"val\": val_path,\n",
    "        \"nc\": num_classes,\n",
    "        \"names\": {i: name for i, name in enumerate(class_names)}\n",
    "    }\n",
    "\n",
    "    # Save the configuration to the dataset.yaml file\n",
    "    yaml_path = os.path.join(dataset_dir, \"dataset.yaml\")\n",
    "    with open(yaml_path, \"w\") as f:\n",
    "        yaml.dump(dataset_config, f, default_flow_style=False)\n",
    "    \n",
    "    print(f\"[INFO] dataset.yaml created at: {yaml_path}\")\n",
    "\n",
    "def validate_final_structure(output_dir=\"/kaggle/working/output\"):\n",
    "    \"\"\"\n",
    "    Valida que las carpetas de im√°genes y etiquetas contengan archivos coincidentes.\n",
    "\n",
    "    Parameters:\n",
    "    - output_dir (str): Carpeta base para PREPROCESSING/.\n",
    "    \"\"\"\n",
    "    partitions = [\"train\", \"val\", \"test\"]\n",
    "    summary = {}\n",
    "\n",
    "    # flag = True\n",
    "\n",
    "    for partition in partitions:\n",
    "        images = sorted(os.listdir(os.path.join(output_dir, f\"dataset/images/{partition}/\")))\n",
    "        labels = sorted(os.listdir(os.path.join(output_dir, f\"dataset/labels/{partition}/\")))\n",
    "\n",
    "        \n",
    "        # if flag:\n",
    "        #     print(output_dir, f\"dataset/images/{partition}/\")\n",
    "        #     flag = False\n",
    "        #     #open the folder in file explorer\n",
    "        #     os.system(f\"explorer {os.path.join(output_dir, f'dataset/images/{partition}/').replace('/', '\\\\')}\")\n",
    "        \n",
    "        if len(images) != len(labels):\n",
    "            raise ValueError(f\"[ERROR] Desbalance entre im√°genes y etiquetas en {partition}.\")\n",
    "        summary[partition] = len(images)\n",
    "    \n",
    "    pprint({\"Validaci√≥n Final\": summary})\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Ejecuci√≥n principal del pipeline.\n",
    "    \"\"\"\n",
    "    paths = setup_environment()\n",
    "    pprint({\"Rutas Configuradas\": paths})\n",
    "\n",
    "    verify_dataset_structure(paths[\"raw_images_path\"], paths[\"raw_labels_path\"])\n",
    "\n",
    "    create_preprocessing_structure(paths[\"output_path\"])\n",
    "\n",
    "    copy_and_partition_data(paths[\"raw_images_path\"], paths[\"raw_labels_path\"], paths[\"output_path\"])\n",
    "\n",
    "    augment_data(\n",
    "        input_images=os.path.join(paths[\"output_path\"], \"dataset/images/train\"),\n",
    "        input_labels=os.path.join(paths[\"output_path\"], \"dataset/labels/train\"),\n",
    "        output_dir=os.path.join(paths[\"output_path\"], \"augmented_dataset\"),\n",
    "        num_augmentations=3\n",
    "    )\n",
    "\n",
    "    copy_augmented_to_train(\n",
    "        augmented_dir=os.path.join(paths[\"output_path\"], \"augmented_dataset\"),\n",
    "        output_path=paths[\"output_path\"]\n",
    "    )\n",
    "\n",
    "    create_dataset_yaml(\n",
    "        output_path=os.path.join(paths[\"output_path\"], \"dataset\"),\n",
    "        num_classes=1,  # Replace with the actual number of classes\n",
    "        class_names=[\"brick\"]  # Add all class names here\n",
    "    )\n",
    "\n",
    "    validate_final_structure(paths[\"output_path\"])\n",
    "    print(\"\\n[INFO] Pipeline setup completed with augmentations and dataset.yaml creation.\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pipeline_train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-22 12:31:26,015 - INFO - [INFO] Usando dataset.yaml en: c:\\Users\\User\\Projects_Unprotected\\LEGO_Bricks_ML_Vision\\notebooks\\working\\output\\dataset\\dataset.yaml\n",
      "2025-01-22 12:31:26,081 - INFO - [INFO] Iniciando entrenamiento regular...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.65 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.2.99  Python-3.12.7 torch-2.4.1+cpu CPU (Intel Core(TM) i5-8265U 1.60GHz)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=c:\\Users\\User\\Projects_Unprotected\\LEGO_Bricks_ML_Vision\\notebooks\\working\\output\\dataset\\dataset.yaml, epochs=50, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=cpu, workers=8, project=regular_yolo_training/20250122_123126, name=train, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.001, lrf=0.01, momentum=0.9, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=regular_yolo_training\\20250122_123126\\train\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    751507  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
      "Model summary: 225 layers, 3,011,043 parameters, 3,011,027 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\User\\Projects_Unprotected\\LEGO_Bricks_ML_Vision\\notebooks\\working\\output\\dataset\\labels\\train.cache... 5048 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5048/5048 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\.conda\\envs\\MiguelEnvHaB\\Lib\\site-packages\\ultralytics\\data\\augment.py:1837: UserWarning: Argument 'quality_lower' is not valid and will be ignored.\n",
      "  A.ImageCompression(quality_lower=75, p=0.0),\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\User\\Projects_Unprotected\\LEGO_Bricks_ML_Vision\\notebooks\\working\\output\\dataset\\labels\\val.cache... 362 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 362/362 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to regular_yolo_training\\20250122_123126\\train\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.001' and 'momentum=0.9' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progreso del entrenamiento:   0%|          | 0/50 [00:00<?, ?√©poca/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mregular_yolo_training\\20250122_123126\\train\u001b[0m\n",
      "Starting training for 50 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/50         0G      1.507      2.313      1.207         64        640:  14%|‚ñà‚ñé        | 43/316 [07:26<47:16, 10.39s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 151\u001b[0m\n\u001b[0;32m    148\u001b[0m         train_model(dataset_yaml, imgsz\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m640\u001b[39m)  \u001b[38;5;66;03m# Tama√±o de imagen predeterminado\u001b[39;00m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 151\u001b[0m     main(optuna_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[5], line 148\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(optuna_mode)\u001b[0m\n\u001b[0;32m    146\u001b[0m     run_optuna_study(dataset_yaml, n_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 148\u001b[0m     train_model(dataset_yaml, imgsz\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m640\u001b[39m)\n",
      "Cell \u001b[1;32mIn[5], line 120\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(dataset_yaml, pretrained_model, epochs, batch_size, learning_rate, momentum, imgsz)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    119\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[INFO] Iniciando entrenamiento regular...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 120\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain(\n\u001b[0;32m    121\u001b[0m         data\u001b[38;5;241m=\u001b[39mdataset_yaml,\n\u001b[0;32m    122\u001b[0m         epochs\u001b[38;5;241m=\u001b[39mepochs,\n\u001b[0;32m    123\u001b[0m         batch\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m    124\u001b[0m         imgsz\u001b[38;5;241m=\u001b[39mimgsz,\n\u001b[0;32m    125\u001b[0m         lr0\u001b[38;5;241m=\u001b[39mlearning_rate,\n\u001b[0;32m    126\u001b[0m         momentum\u001b[38;5;241m=\u001b[39mmomentum,\n\u001b[0;32m    127\u001b[0m         project\u001b[38;5;241m=\u001b[39moutput_dir,\n\u001b[0;32m    128\u001b[0m         name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    129\u001b[0m         device\u001b[38;5;241m=\u001b[39mget_device()\n\u001b[0;32m    130\u001b[0m     )\n\u001b[0;32m    131\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[INFO] Entrenamiento completado. Resultados guardados en \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\User\\.conda\\envs\\MiguelEnvHaB\\Lib\\site-packages\\ultralytics\\engine\\model.py:803\u001b[0m, in \u001b[0;36mModel.train\u001b[1;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[0;32m    800\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mmodel\n\u001b[0;32m    802\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mhub_session \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession  \u001b[38;5;66;03m# attach optional HUB session\u001b[39;00m\n\u001b[1;32m--> 803\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m    804\u001b[0m \u001b[38;5;66;03m# Update model and cfg after training\u001b[39;00m\n\u001b[0;32m    805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m}:\n",
      "File \u001b[1;32mc:\\Users\\User\\.conda\\envs\\MiguelEnvHaB\\Lib\\site-packages\\ultralytics\\engine\\trainer.py:207\u001b[0m, in \u001b[0;36mBaseTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    204\u001b[0m         ddp_cleanup(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mstr\u001b[39m(file))\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 207\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_train(world_size)\n",
      "File \u001b[1;32mc:\\Users\\User\\.conda\\envs\\MiguelEnvHaB\\Lib\\site-packages\\ultralytics\\engine\\trainer.py:367\u001b[0m, in \u001b[0;36mBaseTrainer._do_train\u001b[1;34m(self, world_size)\u001b[0m\n\u001b[0;32m    365\u001b[0m     pbar \u001b[38;5;241m=\u001b[39m TQDM(\u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loader), total\u001b[38;5;241m=\u001b[39mnb)\n\u001b[0;32m    366\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtloss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 367\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[0;32m    368\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_callbacks(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_train_batch_start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    369\u001b[0m     \u001b[38;5;66;03m# Warmup\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\.conda\\envs\\MiguelEnvHaB\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\.conda\\envs\\MiguelEnvHaB\\Lib\\site-packages\\ultralytics\\data\\build.py:48\u001b[0m, in \u001b[0;36mInfiniteDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Creates a sampler that repeats indefinitely.\"\"\"\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m)):\n\u001b[1;32m---> 48\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator)\n",
      "File \u001b[1;32mc:\\Users\\User\\.conda\\envs\\MiguelEnvHaB\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\User\\.conda\\envs\\MiguelEnvHaB\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\User\\.conda\\envs\\MiguelEnvHaB\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\User\\.conda\\envs\\MiguelEnvHaB\\Lib\\site-packages\\ultralytics\\data\\base.py:256\u001b[0m, in \u001b[0;36mBaseDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[0;32m    255\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns transformed label information for given index.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 256\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_image_and_label(index))\n",
      "File \u001b[1;32mc:\\Users\\User\\.conda\\envs\\MiguelEnvHaB\\Lib\\site-packages\\ultralytics\\data\\augment.py:201\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;124;03mApplies a series of transformations to input data. This method sequentially applies each transformation in the\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;124;03mCompose object's list of transforms to the input data.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;124;03m    >>> transformed_data = compose(input_data)\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m--> 201\u001b[0m     data \u001b[38;5;241m=\u001b[39m t(data)\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\User\\.conda\\envs\\MiguelEnvHaB\\Lib\\site-packages\\ultralytics\\data\\augment.py:1368\u001b[0m, in \u001b[0;36mRandomHSV.__call__\u001b[1;34m(self, labels)\u001b[0m\n\u001b[0;32m   1366\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhgain \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msgain \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvgain:\n\u001b[0;32m   1367\u001b[0m     r \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m) \u001b[38;5;241m*\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhgain, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msgain, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvgain] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# random gains\u001b[39;00m\n\u001b[1;32m-> 1368\u001b[0m     hue, sat, val \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39msplit(cv2\u001b[38;5;241m.\u001b[39mcvtColor(img, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2HSV))\n\u001b[0;32m   1369\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mdtype  \u001b[38;5;66;03m# uint8\u001b[39;00m\n\u001b[0;32m   1371\u001b[0m     x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m256\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mr\u001b[38;5;241m.\u001b[39mdtype)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import optuna\n",
    "from ultralytics import YOLO\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "\n",
    "EPOCAS = 4\n",
    "\n",
    "# === Configuraci√≥n del Logger ===\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# === Detecci√≥n del dispositivo ===\n",
    "def get_device():\n",
    "    \"\"\"\n",
    "    Detecta el dispositivo adecuado para la ejecuci√≥n.\n",
    "\n",
    "    Returns:\n",
    "    - str: Dispositivo a usar (\"cpu\", \"0\", \"0,1\").\n",
    "    \"\"\"\n",
    "    if os.environ.get('COLAB_GPU') is not None:\n",
    "        return \"0\"  # Colab\n",
    "    elif os.path.exists(\"/kaggle\"):  # Kaggle\n",
    "        return \"0,1\"\n",
    "    else:\n",
    "        return \"cpu\"  # Local\n",
    "\n",
    "# === Callback personalizado para barra de progreso ===\n",
    "class ProgressBarCallback:\n",
    "    def __init__(self, total_epochs):\n",
    "        self.total_epochs = total_epochs\n",
    "        self.pbar = None\n",
    "\n",
    "    def on_train_start(self, trainer, **kwargs):\n",
    "        # Inicializar barra de progreso\n",
    "        self.pbar = tqdm(total=self.total_epochs, desc=\"Progreso del entrenamiento\", unit=\"√©poca\")\n",
    "\n",
    "    def on_epoch_end(self, trainer, **kwargs):\n",
    "        # Actualizar barra de progreso al final de cada √©poca\n",
    "        self.pbar.update(1)\n",
    "        self.pbar.set_postfix({\"√öltima √©poca\": kwargs.get('epoch') + 1})\n",
    "\n",
    "    def on_train_end(self, trainer, **kwargs):\n",
    "        # Cerrar barra de progreso\n",
    "        self.pbar.close()\n",
    "\n",
    "# === Configuraci√≥n de la Funci√≥n Objetivo de Optuna ===\n",
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Funci√≥n objetivo para Optuna que entrena el modelo YOLO utilizando hiperpar√°metros sugeridos.\n",
    "\n",
    "    Returns:\n",
    "    - mAP50 (float): Precisi√≥n media a IoU 0.5, m√©trica a optimizar.\n",
    "    \"\"\"\n",
    "    # Definir espacio de b√∫squeda para hiperpar√°metros\n",
    "    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-2)\n",
    "    batch_size = trial.suggest_int(\"batch_size\", 8, 32, step=8)\n",
    "    momentum = trial.suggest_uniform(\"momentum\", 0.8, 0.99)\n",
    "    imgsz = trial.suggest_categorical(\"imgsz\", [320, 480, 640, 800])  # Tama√±os de imagen\n",
    "\n",
    "    # Inicializar modelo YOLO\n",
    "    model = YOLO(\"yolov8n.pt\")\n",
    "\n",
    "    # Configurar entrenamiento\n",
    "    project_name = \"optuna_yolo_training\"\n",
    "    dataset_yaml = os.path.join(os.getcwd(), \"working\", \"output\", \"dataset\", \"dataset.yaml\")\n",
    "    try:\n",
    "        results = model.train(\n",
    "            data=dataset_yaml,\n",
    "            epochs=EPOCAS,  # √âpocas fijas para experimentos\n",
    "            batch=batch_size,\n",
    "            imgsz=imgsz,\n",
    "            lr0=learning_rate,\n",
    "            momentum=momentum,\n",
    "            project=project_name,\n",
    "            name=f\"trial_{trial.number}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n",
    "            device=get_device()\n",
    "        )\n",
    "\n",
    "        # Evaluar el modelo\n",
    "        metrics = model.val()\n",
    "        return metrics[\"mAP50\"]  # Devolver mAP50 como m√©trica objetivo\n",
    "    except Exception as e:\n",
    "        logging.error(f\"[ERROR] Error durante el entrenamiento en el trial {trial.number}: {e}\")\n",
    "        return float(\"nan\")\n",
    "\n",
    "# === Entrenamiento Regular (Sin Optuna) ===\n",
    "def train_model(dataset_yaml=None, pretrained_model=\"yolov8n.pt\", epochs=EPOCAS, batch_size=16, learning_rate=0.001, momentum=0.9, imgsz=640):\n",
    "    \"\"\"\n",
    "    Entrena el modelo YOLO con hiperpar√°metros definidos manualmente.\n",
    "\n",
    "    Parameters:\n",
    "    - dataset_yaml (str): Ruta al archivo dataset.yaml.\n",
    "    - pretrained_model (str): Modelo YOLO preentrenado.\n",
    "    - epochs (int): N√∫mero de √©pocas para el entrenamiento.\n",
    "    - batch_size (int): Tama√±o del batch.\n",
    "    - learning_rate (float): Tasa de aprendizaje inicial.\n",
    "    - momentum (float): Momento para el optimizador.\n",
    "    - imgsz (int): Tama√±o de las im√°genes de entrada.\n",
    "    \"\"\"\n",
    "    dataset_yaml = dataset_yaml or os.path.join(os.getcwd(), \"working\", \"output\", \"dataset\", \"dataset.yaml\")\n",
    "\n",
    "    if not os.path.exists(dataset_yaml):\n",
    "        logging.error(f\"[ERROR] dataset.yaml no encontrado en {dataset_yaml}. Aseg√∫rate de que el pipeline_setup.py lo haya generado.\")\n",
    "        return\n",
    "\n",
    "    logging.info(f\"[INFO] Usando dataset.yaml en: {dataset_yaml}\")\n",
    "\n",
    "    model = YOLO(pretrained_model)\n",
    "\n",
    "    output_dir = f\"regular_yolo_training/{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Registrar el callback de barra de progreso\n",
    "    progress_bar = ProgressBarCallback(total_epochs=epochs)\n",
    "    model.add_callback(\"on_train_start\", progress_bar.on_train_start)\n",
    "    model.add_callback(\"on_epoch_end\", progress_bar.on_epoch_end)\n",
    "    model.add_callback(\"on_train_end\", progress_bar.on_train_end)\n",
    "\n",
    "    try:\n",
    "        logging.info(\"[INFO] Iniciando entrenamiento regular...\")\n",
    "        model.train(\n",
    "            data=dataset_yaml,\n",
    "            epochs=epochs,\n",
    "            batch=batch_size,\n",
    "            imgsz=imgsz,\n",
    "            lr0=learning_rate,\n",
    "            momentum=momentum,\n",
    "            project=output_dir,\n",
    "            name=\"train\",\n",
    "            device=get_device()\n",
    "        )\n",
    "        logging.info(f\"[INFO] Entrenamiento completado. Resultados guardados en {output_dir}.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"[ERROR] Error durante el entrenamiento: {e}\")\n",
    "\n",
    "# === Integraci√≥n de Optuna en el Pipeline ===\n",
    "def run_optuna_study(dataset_yaml=None, n_trials=20):\n",
    "    \"\"\"\n",
    "    Ejecuta un estudio de Optuna para optimizar los hiperpar√°metros de YOLO.\n",
    "\n",
    "    Parameters:\n",
    "    - dataset_yaml (str): Ruta al archivo dataset.yaml.\n",
    "    - n_trials (int): N√∫mero de pruebas a ejecutar.\n",
    "    \"\"\"\n",
    "    dataset_yaml = dataset_yaml or os.path.join(os.getcwd(), \"working\", \"output\", \"dataset\", \"dataset.yaml\")\n",
    "\n",
    "    if not os.path.exists(dataset_yaml):\n",
    "        logging.error(f\"[ERROR] dataset.yaml no encontrado en {dataset_yaml}. Aseg√∫rate de que el pipeline_setup.py lo haya generado.\")\n",
    "        return\n",
    "\n",
    "    logging.info(f\"[INFO] Usando dataset.yaml en: {dataset_yaml}\")\n",
    "\n",
    "    logging.info(\"[INFO] Iniciando optimizaci√≥n con Optuna...\")\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "    # Mostrar resultados\n",
    "    logging.info(f\"[INFO] Mejor conjunto de hiperpar√°metros: {study.best_params}\")\n",
    "    logging.info(f\"[INFO] Mejor mAP50 obtenido: {study.best_value}\")\n",
    "\n",
    "    # Guardar resultados\n",
    "    study.trials_dataframe().to_csv(\"optuna_results.csv\")\n",
    "    optuna.visualization.plot_optimization_history(study).write_html(\"optuna_optimization_history.html\")\n",
    "\n",
    "\n",
    "# === Funci√≥n Principal ===\n",
    "def main(optuna_mode=False):\n",
    "    \"\"\"\n",
    "    Ejecuta el entrenamiento con o sin Optuna.\n",
    "\n",
    "    Parameters:\n",
    "    - optuna_mode (bool): Si es True, utiliza Optuna para optimizar hiperpar√°metros.\n",
    "    \"\"\"\n",
    "    if \"google.colab\" in sys.modules:\n",
    "        dataset_yaml = os.path.join(os.getcwd(), \"working\", \"output\", \"dataset\", \"dataset.yaml\")\n",
    "        print(dataset_yaml)\n",
    "    elif os.path.exists(\"/kaggle\"):\n",
    "        dataset_yaml = os.path.join(os.getcwd(), \"output\", \"dataset\", \"dataset.yaml\")\n",
    "        print(dataset_yaml)\n",
    "    else:\n",
    "        dataset_yaml = os.path.join(os.getcwd(), \"output\", \"dataset\", \"dataset.yaml\")\n",
    "        print(dataset_yaml)\n",
    "\n",
    "\n",
    "    if optuna_mode:\n",
    "        run_optuna_study(dataset_yaml, n_trials=20)\n",
    "    else:\n",
    "        train_model(dataset_yaml, imgsz=640)  # Tama√±o de imagen predeterminado\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(optuna_mode=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"last_expr\"  # Muestra solo el √∫ltimo resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m dataset_yaml \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mgetcwd(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mworking\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m train_model(dataset_yaml, imgsz\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m640\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "dataset_yaml = os.path.join(os.getcwd(), \"working\", \"output\", \"dataset\", \"dataset.yaml\")\n",
    "train_model(dataset_yaml, imgsz=640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"nvidia-smi\" no se reconoce como un comando interno o externo,\n",
      "programa o archivo por lotes ejecutable.\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MiguelEnvHaB",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
