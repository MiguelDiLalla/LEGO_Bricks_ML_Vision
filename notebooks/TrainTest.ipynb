{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.environ.get('COLAB_GPU') is not None:\n",
    "    !pip install ultralytics\n",
    "    !pip install optuna\n",
    "\n",
    "elif os.path.exists(\"/kaggle\"):  # Kaggle\n",
    "    !pip install ultralytics\n",
    "    !pip install optuna\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pipeline_satup.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "from pprint import pprint\n",
    "import albumentations as A\n",
    "import cv2\n",
    "import yaml\n",
    "import sys\n",
    "\n",
    "# === Configuración Inicial ===\n",
    "def detect_environment():\n",
    "    \"\"\"\n",
    "    Detecta el entorno de ejecución (Kaggle, Google Colab o Local).\n",
    "\n",
    "    Returns:\n",
    "    - str: Nombre del entorno detectado.\n",
    "    \"\"\"\n",
    "    if \"google.colab\" in sys.modules:\n",
    "        environment = \"colab\"\n",
    "    elif os.path.exists(\"/kaggle\"):\n",
    "        environment = \"kaggle\"\n",
    "    else:\n",
    "        environment = \"local\"\n",
    "    pprint({\"Detected Environment\": environment})\n",
    "    return environment\n",
    "\n",
    "\n",
    "def setup_environment(base_path=\"/kaggle/working/output\"):\n",
    "    \"\"\"\n",
    "    Configura el entorno según el sistema detectado y prepara el dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - base_path (str): Carpeta base donde se configurará la salida.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Rutas configuradas para las imágenes y etiquetas crudas.\n",
    "    \"\"\"\n",
    "    environment = detect_environment()\n",
    "    print(f\"\\n[INFO] Entorno detectado: {environment.capitalize()}\\n\")\n",
    "\n",
    "    if environment == \"kaggle\":\n",
    "        dataset_path = \"/kaggle/input/spiled-lego-bricks\"\n",
    "        required_folders = [\"Images_600x800\", \"LabelMe_txt_bricks\"]\n",
    "        for folder in required_folders:\n",
    "            full_path = os.path.join(dataset_path, folder)\n",
    "            if not os.path.exists(full_path):\n",
    "                raise FileNotFoundError(f\"[ERROR] Carpeta requerida no encontrada: {full_path}\")\n",
    "            print(f\"[INFO] Carpeta verificada: {full_path}\")\n",
    "\n",
    "        return {\n",
    "            \"raw_images_path\": os.path.join(dataset_path, \"Images_600x800\"),\n",
    "            \"raw_labels_path\": os.path.join(dataset_path, \"LabelMe_txt_bricks\"),\n",
    "            \"output_path\": base_path\n",
    "        }\n",
    "    elif environment == \"colab\":\n",
    "            from google.colab import userdata\n",
    "            kaggle_path = \"kaggle.json\"\n",
    "            if not os.path.exists(kaggle_path):\n",
    "                # raise EnvironmentError(\"[ERROR] Sube tu archivo kaggle.json al entorno Colab en /root/.kaggle/\")\n",
    "                os.makedirs(\"/root/.kaggle\", exist_ok=True)\n",
    "            \n",
    "            \n",
    "                kaggle_user = userdata.get('KaggleUser')\n",
    "                kaggle_token = userdata.get('KaggleToken')\n",
    "                if not kaggle_user or not kaggle_token:\n",
    "                    raise EnvironmentError(\"[ERROR] No se encontraron las credenciales de Kaggle en Google Colab.\")\n",
    "                kaggle_data = {\n",
    "                    \"username\": kaggle_user,\n",
    "                    \"key\": kaggle_token\n",
    "                }\n",
    "                with open(\"/root/.kaggle/kaggle.json\", \"w\") as f:\n",
    "                    json.dump(kaggle_data, f)\n",
    "                    print(\"[INFO] Credenciales de Kaggle configuradas en Google Colab.\")\n",
    "            else:\n",
    "                os.makedirs(\"/root/.kaggle\", exist_ok=True)\n",
    "                shutil.move(kaggle_path, \"/root/.kaggle/kaggle.json\")\n",
    "                print(\"[INFO] Archivo kaggle.json movido a /root/.kaggle/\")\n",
    "            os.chmod(\"/root/.kaggle/kaggle.json\", 0o600)\n",
    "            os.makedirs(\"working\", exist_ok=True)\n",
    "            os.makedirs(\"working/spiled-lego-bricks\", exist_ok=True)\n",
    "            os.system(\"kaggle datasets download -d migueldilalla/spiled-lego-bricks -p working/spiled-lego-bricks --unzip\")\n",
    "            os.makedirs(\"/working/output\", exist_ok=True)\n",
    "            dataset_path = \"working/spiled-lego-bricks\"\n",
    "\n",
    "            return {\n",
    "                \"raw_images_path\": os.path.join(dataset_path, \"Images_600x800\"),\n",
    "                \"raw_labels_path\": os.path.join(dataset_path, \"LabelMe_txt_bricks\"),\n",
    "                \"output_path\": os.path.join(os.getcwd(), \"working\", \"output\")\n",
    "            }\n",
    "\n",
    "\n",
    "    elif environment == \"local\":\n",
    "        kaggle_json_path = os.path.expanduser(\"~/.kaggle/kaggle.json\")\n",
    "        if not os.path.exists(kaggle_json_path):\n",
    "            raise EnvironmentError(\"[ERROR] Archivo kaggle.json no encontrado en ~/.kaggle/\")\n",
    "        os.makedirs(\"working\", exist_ok=True)\n",
    "        os.makedirs(\"working/spiled-lego-bricks\", exist_ok=True)\n",
    "        if not os.listdir(\"working/spiled-lego-bricks\"):\n",
    "            os.system(\"kaggle datasets download -d migueldilalla/spiled-lego-bricks -p working/spiled-lego-bricks --unzip\")\n",
    "        os.makedirs(\"working/output\", exist_ok=True)\n",
    "        dataset_path = \"working/spiled-lego-bricks\"\n",
    "\n",
    "        return {\n",
    "             \"raw_images_path\": os.path.join(dataset_path, \"Images_600x800\"),\n",
    "            \"raw_labels_path\": os.path.join(dataset_path, \"LabelMe_txt_bricks\"),\n",
    "            \"output_path\": os.path.join(os.getcwd(), \"working\", \"output\")\n",
    "        }\n",
    "    else:\n",
    "        while True:\n",
    "            user_input = input(\"[PROMPT] No se detectó un entorno. Por favor, escribe 'k' para Kaggle, 'g' para Google Colab, o 'l' para Local: \").strip().lower()\n",
    "            if user_input in [\"k\", \"g\", \"l\"]:\n",
    "                return setup_environment_custom(user_input, base_path)\n",
    "            print(\"[ERROR] Entrada inválida. Intenta nuevamente.\")\n",
    "\n",
    "def setup_environment_custom(choice, base_path):\n",
    "    \"\"\"\n",
    "    Configura el entorno manualmente basado en la elección del usuario.\n",
    "\n",
    "    Parameters:\n",
    "    - choice (str): 'k' para Kaggle, 'g' para Colab, 'l' para Local.\n",
    "    - base_path (str): Ruta base para la salida.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Rutas configuradas para las imágenes y etiquetas crudas.\n",
    "    \"\"\"\n",
    "    if choice == \"k\":\n",
    "        return setup_environment()\n",
    "    elif choice == \"g\":\n",
    "        return setup_environment(base_path=\"working\")\n",
    "    elif choice == \"l\":\n",
    "        return setup_environment(base_path=\"working\")\n",
    "    else:\n",
    "        raise EnvironmentError(\"[ERROR] Configuración desconocida.\")\n",
    "\n",
    "def verify_dataset_structure(raw_images_path, raw_labels_path):\n",
    "    \"\"\"\n",
    "    Verifica la existencia de las carpetas requeridas en el dataset y muestra estadísticas iniciales.\n",
    "\n",
    "    Parameters:\n",
    "    - raw_images_path (str): Ruta a las imágenes crudas.\n",
    "    - raw_labels_path (str): Ruta a las etiquetas crudas.\n",
    "    \"\"\"\n",
    "    required_folders = [raw_images_path, raw_labels_path]\n",
    "    summary = {}\n",
    "    for folder in required_folders:\n",
    "        if not os.path.exists(folder):\n",
    "            raise FileNotFoundError(f\"[ERROR] Carpeta requerida no encontrada: {folder}\")\n",
    "\n",
    "        num_files = len([f for f in os.listdir(folder) if os.path.isfile(os.path.join(folder, f))])\n",
    "        if num_files == 0:\n",
    "            raise ValueError(f\"[ERROR] La carpeta {folder} está vacía.\")\n",
    "        summary[folder] = num_files\n",
    "\n",
    "    pprint({\"Dataset Estructura\": summary})\n",
    "\n",
    "def create_preprocessing_structure(output_dir=\"/kaggle/working/output\"):\n",
    "    \"\"\"\n",
    "    Crea la estructura de carpetas para PREPROCESSING/.\n",
    "\n",
    "    Parameters:\n",
    "    - output_dir (str): Ruta base para la carpeta PREPROCESSING/.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    subfolders = [\n",
    "        \"dataset/images/train\", \"dataset/images/val\", \"dataset/images/test\",\n",
    "        \"dataset/labels/train\", \"dataset/labels/val\", \"dataset/labels/test\",\n",
    "        \"test_images\"\n",
    "    ]\n",
    "    for subfolder in subfolders:\n",
    "        os.makedirs(os.path.join(output_dir, subfolder), exist_ok=True)\n",
    "    print(f\"[INFO] Estructura de carpetas creada en {output_dir}.\")\n",
    "\n",
    "def copy_and_partition_data(input_images, input_labels, output_dir):\n",
    "    \"\"\"\n",
    "    Copia imágenes y etiquetas a las carpetas correspondientes y realiza la partición de datos.\n",
    "\n",
    "    Parameters:\n",
    "    - input_images (str): Carpeta de imágenes de entrada.\n",
    "    - input_labels (str): Carpeta de etiquetas de entrada.\n",
    "    - output_dir (str): Carpeta base para PREPROCESSING/.\n",
    "    \"\"\"\n",
    "    images = sorted([f for f in os.listdir(input_images) if f.endswith(\".jpg\")])\n",
    "    labels = sorted([f for f in os.listdir(input_labels) if f.endswith(\".txt\")])\n",
    "\n",
    "    if len(images) != len(labels):\n",
    "        raise ValueError(\"[ERROR] Número de imágenes y etiquetas no coincide.\")\n",
    "\n",
    "    image_paths = [os.path.join(input_images, img) for img in images]\n",
    "    label_paths = [os.path.join(input_labels, lbl) for lbl in labels]\n",
    "\n",
    "    train_imgs, temp_imgs, train_lbls, temp_lbls = train_test_split(image_paths, label_paths, test_size=0.3, random_state=42)\n",
    "    val_imgs, test_imgs, val_lbls, test_lbls = train_test_split(temp_imgs, temp_lbls, test_size=0.33, random_state=42)\n",
    "\n",
    "    partitions = {\n",
    "        \"train\": (train_imgs, train_lbls),\n",
    "        \"val\": (val_imgs, val_lbls),\n",
    "        \"test\": (test_imgs, test_lbls)\n",
    "    }\n",
    "\n",
    "    for partition, (imgs, lbls) in partitions.items():\n",
    "        for img, lbl in zip(imgs, lbls):\n",
    "            shutil.copy(img, os.path.join(output_dir, f\"dataset/images/{partition}/\"))\n",
    "            shutil.copy(lbl, os.path.join(output_dir, f\"dataset/labels/{partition}/\"))\n",
    "\n",
    "    pprint({\"Partición Completada\": {partition: len(imgs) for partition, (imgs, _) in partitions.items()}})\n",
    "\n",
    "def augment_data(input_images, input_labels, output_dir, num_augmentations=2):\n",
    "    \"\"\"\n",
    "    Aplica aumentaciones al dataset y guarda imágenes y etiquetas aumentadas.\n",
    "\n",
    "    Parameters:\n",
    "    - input_images (str): Carpeta de imágenes originales.\n",
    "    - input_labels (str): Carpeta de etiquetas en formato YOLO.\n",
    "    - output_dir (str): Carpeta donde se guardarán los datos aumentados.\n",
    "    - num_augmentations (int): Número de versiones aumentadas por imagen.\n",
    "    \"\"\"\n",
    "    aug_images_dir = os.path.join(output_dir, \"augmented_images\")\n",
    "    aug_labels_dir = os.path.join(output_dir, \"augmented_labels\")\n",
    "    os.makedirs(aug_images_dir, exist_ok=True)\n",
    "    os.makedirs(aug_labels_dir, exist_ok=True)\n",
    "\n",
    "    transform = A.Compose([\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.RandomBrightnessContrast(p=0.2),\n",
    "        A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),\n",
    "        A.Resize(height=640, width=640),\n",
    "    ], bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels']))\n",
    "\n",
    "    images = sorted([f for f in os.listdir(input_images) if f.endswith(\".jpg\")])\n",
    "    for img_file in images:\n",
    "        img_path = os.path.join(input_images, img_file)\n",
    "        label_path = os.path.join(input_labels, img_file.replace(\".jpg\", \".txt\"))\n",
    "\n",
    "        if not os.path.exists(label_path):\n",
    "            continue\n",
    "\n",
    "        image = cv2.imread(img_path)\n",
    "        bboxes, class_labels = load_labels(label_path)\n",
    "\n",
    "        for i in range(num_augmentations):\n",
    "            augmented = transform(image=image, bboxes=bboxes, class_labels=class_labels)\n",
    "            aug_image = augmented[\"image\"]\n",
    "            aug_bboxes = augmented[\"bboxes\"]\n",
    "            aug_labels = augmented[\"class_labels\"]\n",
    "\n",
    "            aug_image_path = os.path.join(aug_images_dir, f\"{img_file.split('.')[0]}_aug{i}.jpg\")\n",
    "            cv2.imwrite(aug_image_path, aug_image)\n",
    "\n",
    "            aug_label_path = os.path.join(aug_labels_dir, f\"{img_file.split('.')[0]}_aug{i}.txt\")\n",
    "            save_labels(aug_label_path, aug_bboxes, aug_labels)\n",
    "\n",
    "    print(f\"[INFO] Augmented data saved to {output_dir}.\")\n",
    "\n",
    "def load_labels(label_path):\n",
    "    \"\"\"\n",
    "    Carga etiquetas en formato YOLO desde un archivo .txt.\n",
    "\n",
    "    Parameters:\n",
    "    - label_path (str): Ruta al archivo de etiquetas en formato YOLO.\n",
    "\n",
    "    Returns:\n",
    "    - bboxes (list): Lista de bounding boxes en formato YOLO.\n",
    "    - class_labels (list): Lista de etiquetas de clase.\n",
    "    \"\"\"\n",
    "    bboxes, class_labels = [], []\n",
    "    with open(label_path, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        class_id, x_center, y_center, width, height = map(float, line.strip().split())\n",
    "        bboxes.append([x_center, y_center, width, height])\n",
    "        class_labels.append(int(class_id))\n",
    "    return bboxes, class_labels\n",
    "\n",
    "def save_labels(output_path, bboxes, class_labels):\n",
    "    \"\"\"\n",
    "    Guarda etiquetas en formato YOLO en un archivo .txt.\n",
    "\n",
    "    Parameters:\n",
    "    - output_path (str): Ruta donde se guardará el archivo de etiquetas.\n",
    "    - bboxes (list): Lista de bounding boxes en formato YOLO.\n",
    "    - class_labels (list): Lista de etiquetas de clase.\n",
    "    \"\"\"\n",
    "    with open(output_path, \"w\") as f:\n",
    "        for bbox, label in zip(bboxes, class_labels):\n",
    "            f.write(f\"{label} {' '.join(map(str, bbox))}\\n\")\n",
    "\n",
    "\n",
    "def copy_augmented_to_train(augmented_dir, output_path):\n",
    "    \"\"\"\n",
    "    Copia los datos aumentados a las subcarpetas correspondientes de 'train'.\n",
    "\n",
    "    Parameters:\n",
    "    - augmented_dir (str): Directorio que contiene imágenes y etiquetas aumentadas.\n",
    "    - output_path(str): Ruta base para la salida.\n",
    "    \"\"\"\n",
    "    aug_images_dir = os.path.join(augmented_dir, \"augmented_images\")\n",
    "    aug_labels_dir = os.path.join(augmented_dir, \"augmented_labels\")\n",
    "    train_images_dir = os.path.join(output_path, \"dataset/images/train\")\n",
    "    train_labels_dir = os.path.join(output_path, \"dataset/labels/train\")\n",
    "\n",
    "    for img_file in os.listdir(aug_images_dir):\n",
    "        shutil.copy(os.path.join(aug_images_dir, img_file), train_images_dir)\n",
    "\n",
    "    for label_file in os.listdir(aug_labels_dir):\n",
    "        shutil.copy(os.path.join(aug_labels_dir, label_file), train_labels_dir)\n",
    "\n",
    "    print(f\"[INFO] Augmented data merged into train set at {output_path}.\")\n",
    "\n",
    "def create_dataset_yaml(output_path, num_classes, class_names):\n",
    "    \"\"\"\n",
    "    Creates a dataset.yaml file with absolute paths for YOLO training.\n",
    "\n",
    "    Parameters:\n",
    "    - output_path (str): Base directory where the dataset.yaml file will be saved.\n",
    "    - num_classes (int): Total number of classes.\n",
    "    - class_names (list): List of class names.\n",
    "    \"\"\"\n",
    "    # Resolve absolute paths for train and val folders\n",
    "    dataset_dir = os.path.abspath(output_path)\n",
    "    train_path = os.path.join(dataset_dir, \"images/train\")\n",
    "    val_path = os.path.join(dataset_dir, \"images/val\")\n",
    "\n",
    "    # Create the dataset configuration dictionary\n",
    "    dataset_config = {\n",
    "        \"path\": dataset_dir,\n",
    "        \"train\": train_path,\n",
    "        \"val\": val_path,\n",
    "        \"nc\": num_classes,\n",
    "        \"names\": {i: name for i, name in enumerate(class_names)}\n",
    "    }\n",
    "\n",
    "    # Save the configuration to the dataset.yaml file\n",
    "    yaml_path = os.path.join(dataset_dir, \"dataset.yaml\")\n",
    "    with open(yaml_path, \"w\") as f:\n",
    "        yaml.dump(dataset_config, f, default_flow_style=False)\n",
    "    \n",
    "    print(f\"[INFO] dataset.yaml created at: {yaml_path}\")\n",
    "\n",
    "def validate_final_structure(output_dir=\"/kaggle/working/output\"):\n",
    "    \"\"\"\n",
    "    Valida que las carpetas de imágenes y etiquetas contengan archivos coincidentes.\n",
    "\n",
    "    Parameters:\n",
    "    - output_dir (str): Carpeta base para PREPROCESSING/.\n",
    "    \"\"\"\n",
    "    partitions = [\"train\", \"val\", \"test\"]\n",
    "    summary = {}\n",
    "\n",
    "    # flag = True\n",
    "\n",
    "    for partition in partitions:\n",
    "        images = sorted(os.listdir(os.path.join(output_dir, f\"dataset/images/{partition}/\")))\n",
    "        labels = sorted(os.listdir(os.path.join(output_dir, f\"dataset/labels/{partition}/\")))\n",
    "\n",
    "        \n",
    "        # if flag:\n",
    "        #     print(output_dir, f\"dataset/images/{partition}/\")\n",
    "        #     flag = False\n",
    "        #     #open the folder in file explorer\n",
    "        #     os.system(f\"explorer {os.path.join(output_dir, f'dataset/images/{partition}/').replace('/', '\\\\')}\")\n",
    "        \n",
    "        if len(images) != len(labels):\n",
    "            raise ValueError(f\"[ERROR] Desbalance entre imágenes y etiquetas en {partition}.\")\n",
    "        summary[partition] = len(images)\n",
    "    \n",
    "    pprint({\"Validación Final\": summary})\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Ejecución principal del pipeline.\n",
    "    \"\"\"\n",
    "    paths = setup_environment()\n",
    "    pprint({\"Rutas Configuradas\": paths})\n",
    "\n",
    "    verify_dataset_structure(paths[\"raw_images_path\"], paths[\"raw_labels_path\"])\n",
    "\n",
    "    create_preprocessing_structure(paths[\"output_path\"])\n",
    "\n",
    "    copy_and_partition_data(paths[\"raw_images_path\"], paths[\"raw_labels_path\"], paths[\"output_path\"])\n",
    "\n",
    "    augment_data(\n",
    "        input_images=os.path.join(paths[\"output_path\"], \"dataset/images/train\"),\n",
    "        input_labels=os.path.join(paths[\"output_path\"], \"dataset/labels/train\"),\n",
    "        output_dir=os.path.join(paths[\"output_path\"], \"augmented_dataset\"),\n",
    "        num_augmentations=3\n",
    "    )\n",
    "\n",
    "    copy_augmented_to_train(\n",
    "        augmented_dir=os.path.join(paths[\"output_path\"], \"augmented_dataset\"),\n",
    "        output_path=paths[\"output_path\"]\n",
    "    )\n",
    "\n",
    "    create_dataset_yaml(\n",
    "        output_path=os.path.join(paths[\"output_path\"], \"dataset\"),\n",
    "        num_classes=1,  # Replace with the actual number of classes\n",
    "        class_names=[\"brick\"]  # Add all class names here\n",
    "    )\n",
    "\n",
    "    validate_final_structure(paths[\"output_path\"])\n",
    "    print(\"\\n[INFO] Pipeline setup completed with augmentations and dataset.yaml creation.\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pipeline_train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import optuna\n",
    "from ultralytics import YOLO\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "\n",
    "EPOCAS = 4\n",
    "\n",
    "# === Configuración del Logger ===\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# === Detección del dispositivo ===\n",
    "def get_device():\n",
    "    \"\"\"\n",
    "    Detecta el dispositivo adecuado para la ejecución.\n",
    "\n",
    "    Returns:\n",
    "    - str: Dispositivo a usar (\"cpu\", \"0\", \"0,1\").\n",
    "    \"\"\"\n",
    "    if os.environ.get('COLAB_GPU') is not None:\n",
    "        return \"0\"  # Colab\n",
    "    elif os.path.exists(\"/kaggle\"):  # Kaggle\n",
    "        return \"0,1\"\n",
    "    else:\n",
    "        return \"cpu\"  # Local\n",
    "\n",
    "# === Callback personalizado para barra de progreso ===\n",
    "class ProgressBarCallback:\n",
    "    def __init__(self, total_epochs):\n",
    "        self.total_epochs = total_epochs\n",
    "        self.pbar = None\n",
    "\n",
    "    def on_train_start(self, trainer, **kwargs):\n",
    "        # Inicializar barra de progreso\n",
    "        self.pbar = tqdm(total=self.total_epochs, desc=\"Progreso del entrenamiento\", unit=\"época\")\n",
    "\n",
    "    def on_epoch_end(self, trainer, **kwargs):\n",
    "        # Actualizar barra de progreso al final de cada época\n",
    "        self.pbar.update(1)\n",
    "        self.pbar.set_postfix({\"Última época\": kwargs.get('epoch') + 1})\n",
    "\n",
    "    def on_train_end(self, trainer, **kwargs):\n",
    "        # Cerrar barra de progreso\n",
    "        self.pbar.close()\n",
    "\n",
    "# === Configuración de la Función Objetivo de Optuna ===\n",
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Función objetivo para Optuna que entrena el modelo YOLO utilizando hiperparámetros sugeridos.\n",
    "\n",
    "    Returns:\n",
    "    - mAP50 (float): Precisión media a IoU 0.5, métrica a optimizar.\n",
    "    \"\"\"\n",
    "    # Definir espacio de búsqueda para hiperparámetros\n",
    "    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-2)\n",
    "    batch_size = trial.suggest_int(\"batch_size\", 8, 32, step=8)\n",
    "    momentum = trial.suggest_uniform(\"momentum\", 0.8, 0.99)\n",
    "    imgsz = trial.suggest_categorical(\"imgsz\", [320, 480, 640, 800])  # Tamaños de imagen\n",
    "\n",
    "    # Inicializar modelo YOLO\n",
    "    model = YOLO(\"yolov8n.pt\")\n",
    "\n",
    "    # Configurar entrenamiento\n",
    "    project_name = \"optuna_yolo_training\"\n",
    "    dataset_yaml = os.path.join(os.getcwd(), \"working\", \"output\", \"dataset\", \"dataset.yaml\")\n",
    "    try:\n",
    "        results = model.train(\n",
    "            data=dataset_yaml,\n",
    "            epochs=EPOCAS,  # Épocas fijas para experimentos\n",
    "            batch=batch_size,\n",
    "            imgsz=imgsz,\n",
    "            lr0=learning_rate,\n",
    "            momentum=momentum,\n",
    "            project=project_name,\n",
    "            name=f\"trial_{trial.number}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n",
    "            device=get_device()\n",
    "        )\n",
    "\n",
    "        # Evaluar el modelo\n",
    "        metrics = model.val()\n",
    "        return metrics[\"mAP50\"]  # Devolver mAP50 como métrica objetivo\n",
    "    except Exception as e:\n",
    "        logging.error(f\"[ERROR] Error durante el entrenamiento en el trial {trial.number}: {e}\")\n",
    "        return float(\"nan\")\n",
    "\n",
    "# === Entrenamiento Regular (Sin Optuna) ===\n",
    "def train_model(dataset_yaml=None, pretrained_model=\"yolov8n.pt\", epochs=EPOCAS, batch_size=16, learning_rate=0.001, momentum=0.9, imgsz=640):\n",
    "    \"\"\"\n",
    "    Entrena el modelo YOLO con hiperparámetros definidos manualmente.\n",
    "\n",
    "    Parameters:\n",
    "    - dataset_yaml (str): Ruta al archivo dataset.yaml.\n",
    "    - pretrained_model (str): Modelo YOLO preentrenado.\n",
    "    - epochs (int): Número de épocas para el entrenamiento.\n",
    "    - batch_size (int): Tamaño del batch.\n",
    "    - learning_rate (float): Tasa de aprendizaje inicial.\n",
    "    - momentum (float): Momento para el optimizador.\n",
    "    - imgsz (int): Tamaño de las imágenes de entrada.\n",
    "    \"\"\"\n",
    "    dataset_yaml = dataset_yaml or os.path.join(os.getcwd(), \"working\", \"output\", \"dataset\", \"dataset.yaml\")\n",
    "\n",
    "    if not os.path.exists(dataset_yaml):\n",
    "        logging.error(f\"[ERROR] dataset.yaml no encontrado en {dataset_yaml}. Asegúrate de que el pipeline_setup.py lo haya generado.\")\n",
    "        return\n",
    "\n",
    "    logging.info(f\"[INFO] Usando dataset.yaml en: {dataset_yaml}\")\n",
    "\n",
    "    model = YOLO(pretrained_model)\n",
    "\n",
    "    output_dir = f\"regular_yolo_training/{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Registrar el callback de barra de progreso\n",
    "    progress_bar = ProgressBarCallback(total_epochs=epochs)\n",
    "    model.add_callback(\"on_train_start\", progress_bar.on_train_start)\n",
    "    model.add_callback(\"on_epoch_end\", progress_bar.on_epoch_end)\n",
    "    model.add_callback(\"on_train_end\", progress_bar.on_train_end)\n",
    "\n",
    "    try:\n",
    "        logging.info(\"[INFO] Iniciando entrenamiento regular...\")\n",
    "        model.train(\n",
    "            data=dataset_yaml,\n",
    "            epochs=epochs,\n",
    "            batch=batch_size,\n",
    "            imgsz=imgsz,\n",
    "            lr0=learning_rate,\n",
    "            momentum=momentum,\n",
    "            project=output_dir,\n",
    "            name=\"train\",\n",
    "            device=get_device()\n",
    "        )\n",
    "        logging.info(f\"[INFO] Entrenamiento completado. Resultados guardados en {output_dir}.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"[ERROR] Error durante el entrenamiento: {e}\")\n",
    "\n",
    "# === Integración de Optuna en el Pipeline ===\n",
    "def run_optuna_study(dataset_yaml=None, n_trials=20):\n",
    "    \"\"\"\n",
    "    Ejecuta un estudio de Optuna para optimizar los hiperparámetros de YOLO.\n",
    "\n",
    "    Parameters:\n",
    "    - dataset_yaml (str): Ruta al archivo dataset.yaml.\n",
    "    - n_trials (int): Número de pruebas a ejecutar.\n",
    "    \"\"\"\n",
    "    dataset_yaml = dataset_yaml or os.path.join(os.getcwd(), \"working\", \"output\", \"dataset\", \"dataset.yaml\")\n",
    "\n",
    "    if not os.path.exists(dataset_yaml):\n",
    "        logging.error(f\"[ERROR] dataset.yaml no encontrado en {dataset_yaml}. Asegúrate de que el pipeline_setup.py lo haya generado.\")\n",
    "        return\n",
    "\n",
    "    logging.info(f\"[INFO] Usando dataset.yaml en: {dataset_yaml}\")\n",
    "\n",
    "    logging.info(\"[INFO] Iniciando optimización con Optuna...\")\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "    # Mostrar resultados\n",
    "    logging.info(f\"[INFO] Mejor conjunto de hiperparámetros: {study.best_params}\")\n",
    "    logging.info(f\"[INFO] Mejor mAP50 obtenido: {study.best_value}\")\n",
    "\n",
    "    # Guardar resultados\n",
    "    study.trials_dataframe().to_csv(\"optuna_results.csv\")\n",
    "    optuna.visualization.plot_optimization_history(study).write_html(\"optuna_optimization_history.html\")\n",
    "\n",
    "\n",
    "# === Función Principal ===\n",
    "def main(optuna_mode=False):\n",
    "    \"\"\"\n",
    "    Ejecuta el entrenamiento con o sin Optuna.\n",
    "\n",
    "    Parameters:\n",
    "    - optuna_mode (bool): Si es True, utiliza Optuna para optimizar hiperparámetros.\n",
    "    \"\"\"\n",
    "    if \"google.colab\" in sys.modules:\n",
    "        dataset_yaml = os.path.join(os.getcwd(), \"working\", \"output\", \"dataset\", \"dataset.yaml\")\n",
    "        print(dataset_yaml)\n",
    "    elif os.path.exists(\"/kaggle\"):\n",
    "        dataset_yaml = os.path.join(os.getcwd(), \"output\", \"dataset\", \"dataset.yaml\")\n",
    "        print(dataset_yaml)\n",
    "    else:\n",
    "        dataset_yaml = os.path.join(os.getcwd(), \"output\", \"dataset\", \"dataset.yaml\")\n",
    "        print(dataset_yaml)\n",
    "\n",
    "\n",
    "    if optuna_mode:\n",
    "        run_optuna_study(dataset_yaml, n_trials=20)\n",
    "    else:\n",
    "        train_model(dataset_yaml, imgsz=640)  # Tamaño de imagen predeterminado\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(optuna_mode=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MiguelEnvHaB",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
